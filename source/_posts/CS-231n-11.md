---
title: CS 231n Lec9 笔记
date: 2024-11-22 19:31
tags:
    - 语义分割
    - 超采样
    - 目标识别
    - R-CNN
    - Fast R-CNN
    - Faster R-CNN
    - Mask R-CNN
    - One-Stage Object Detectors
    - 实例分割
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

## A General Glimpse: Computer Vision Tasks
计算机视觉工作的主要任务分为以下四种：
1. 分类(Classification)
2. 语义分割(Semantic Segmentation)
3. 目标检测(Object Detection)
4. 目标分割(Object Segmentation)

![](/assets/CS-231n-11/1.png)
此前的大部分模型都在完成分类工作，这节我们将介绍完成其它三类问题的一些可能方法。

## Semantic Segmentation
语义分割(Semantic Segmentation)（*总感觉这个翻译怪怪的*）的主要任务就是将整张图片按不同的类别划分成几个部分，如下图：
![](/assets/CS-231n-11/2.png)

从而一个十分自然的想法便是，在训练时，提供为所有像素点都打好标签的训练数据，测试时只需对每个像素点仅需分类。
但是显而易见的，这种方式存在一个问题，每一个像素点的分类应当是依赖于上下文（图片中即周遭像素乃至整张图片），而直接对每个像素点进行分类显然无法注意到这些特征，于是这种方式可以预见的会有很糟糕的表现。

于是问题转化到了如何为每个像素点提供上下文，又是一个朴素的想法：
测试时在分类每个像素时，采用滑动窗口，每次选取一个小区域来为其中心的像素点提供上下文从而进行中心像素点的分类。

但是这么做又存在一个问题，便是推理时十分低效，类似于卷积时，窗口在滑动的过程中会划过许多的重叠像素，并且没有复用重复的计算结果，这导致了计算的低效。

对于这个问题，我们可以先使用一个CNN导出原图的特征图，从而可以直接对导出的特征图进行分类（特征图中的每一个像素都对应了原图中的一片感受野）。
但是问题又双叒叕出现了，大部分CNN为了加深特征图的深度，都会在处理过程中对原图进行降采样，而我们应当需要特征图与原图具有相同的尺寸从而更便捷地进行语义分割。

但这个问题很好解决，我们只需使用没有池化层和大步长卷积层的CNN来保持原图尺寸即可，但这样太昂贵太慢了......

类似于之前的某个trick？我们可以在计算过程中进行降采样以加快运算速度，在输出前超采样回来即可。
![](/assets/CS-231n-11/3.png)

降采样很简单，我们只需采用池化层(Max-Pooling, Average-Pooling)或大步长卷积层即可。问题在于，超采样该怎么办，我们如何将损失的信息超回来。
以下是一些超采样的方法
### Unpooling
对于常见的最大池化
首先一个十分朴素的反池化方法就是将一个数变成四个数：
$$\begin{aligned}
\begin{bmatrix}1&2\\3&4\end{bmatrix} \rightarrow
\begin{bmatrix}
1&1&2&2\\
1&1&2&2\\
3&3&4&4\\
3&3&4&4
\end{bmatrix}
\end{aligned}$$
第二种可以根据池化操作的行为定义反池化操作的行为，例如记忆池化操作中最大数的索引，反池化时只将数填充到最大索引中：
$$\begin{aligned}
&\begin{bmatrix}
1&5&3&5\\
4&\color{red}6&\color{red}8&3\\
3&\color{red}6&4&6\\
1&5&\color{red}10&3
\end{bmatrix}
\overset{Max-Pool}{\rightarrow} 
\begin{bmatrix}
6&8\\
6&10\\
\end{bmatrix}\\
&\begin{bmatrix}
1&2\\
3&4\\
\end{bmatrix}
\overset{UnPool}{\rightarrow} 
\begin{bmatrix}
0&0&0&0\\
0&\color{red}1&\color{red}2&0\\
0&\color{red}3&0&0\\
0&0&\color{red}4&0
\end{bmatrix}
\end{aligned}$$
### Learnable Upsampling
从上面对最大池化操作的反转我们可以受到启发，那么对于大步长的卷积操作，是否也存在类似的超采样操作呢，事实上是存在的，这种操作被称为转置卷积(Transposed Convolution)，并且由于卷积并不是固定函数，卷积核可学习，那么转置卷积核也是可以学习的。
回忆在大步长卷积中我们对原矩阵做了什么：
![](/assets/CS-231n-11/4.png)

对转置卷积可以用矩阵乘法浅显地解释：
在卷积操作时，我们计算：
$$\textbf{y}_{a\times c} = \textbf{W}_{a\times b}\textbf{x}_{b\times c}$$
于是我们将卷积矩阵转置，结果与输出互换，就得到了转置卷积操作：
$$\textbf{x}_{b\times c} = \textbf{W}^T_{b\times a}\textbf{y}_{a\times c}$$
虽然实际上的操作比这个复杂很多，但原理就是这个。
![](/assets/CS-231n-11/5.png)

这里有一个一维卷积的例子，考虑使用卷积核$(x,y,z)$对矩阵$(a,b,c,d)$进行零填充为1，步长为2的卷积，那么卷积过程：
$$\begin{aligned}
\begin{bmatrix}
x&y&z&0&0&0\\
0&0&x&y&z&0
\end{bmatrix}
\begin{bmatrix}
0\\
a\\
b\\
c\\
d\\
0\\
\end{bmatrix}
=
\begin{bmatrix}
ay+bz\\
bx+cy+dz
\end{bmatrix}
\end{aligned}$$
转置卷积过程：
$$\begin{aligned}
\begin{bmatrix}
x&0\\
y&0\\
z&x\\
0&y\\
0&z\\
0&0
\end{bmatrix}
\begin{bmatrix}
a'\\b'
\end{bmatrix}
=
\begin{bmatrix}
a'x\\
a'y\\
a'z+b'y\\
b'y\\
b'z\\
0
\end{bmatrix}
\end{aligned}$$

从而我们知道了超采样的两种方法，从而完成了进行语义分割的全部步骤。
*注意：* 我们只需要将每个像素进行分类，不需要区分同一类别中的不同实例。

## Object Detection: Single Object
单目标识别是目标识别中比较简单的一类任务，大体上来说，我们可以将单目标识别的任务分为两大部分：分类以及定位(Classification & Localization)。需要注意的是，这辆个任务是同时进行的，不存在先后关系。

和之前学习的图像识别方法类似，在进行多目标识别时，我们也一般会使用预训练过的CNN提取特征图：
![](/assets/CS-231n-11/6.png)

如图，在提取完特征图之后，我们"兵分两路"，采用两个MLP，一个处理分类问题，采用Softmax输出置信度并计算CE损失函数，另一个处理定位问题，在经过多个FC层后输出四个定位目标检测框位置大小的数并计算L2损失（注意：定位是**回归问题**），再将两者损失求和得到最终损失并进行训练。

## Object Detection: Multiple Object
相较于单目标识别，多目标识别引入的最大问题就是对于一个图片，我们可能会输出对很多物体的检测结果，需要框定若干个目标检测框。

一个朴素的解决办法就是，从图像中截取很多很多个区域（可能重叠），并对每个区域单独应用一次CNN进行分类，判断这部分是背景还是某一类目标。显然这种方法太过朴素，我们可能需要尝试巨量的截取方式并对每一种使用CNN，这会导致完全不可接受的计算代价。

一种启发式的方法可以改善这种状况，称为选择性搜索(Selective Search)，这种方法会截取原图片中的若干片区域("blobby regions")，作为提议(proposal，也称Rol(Regions of interest))，表示这些区域可能含有目标。这种方式一般只会给出约2000个提议区域，选取过程相对来说是较快的。

应用这种方法，我们可以引入一种多目标检测的算法R-CNN：
### R-CNN
![](/assets/CS-231n-11/7.png)
在这种算法中，我们首先使用选择性搜索选出若干个Rol，然后对于每个Rol，首先通过一些变换将它转变为固定的大小，如$224\times 224$，之后将转换后的数据传入CNN得到特征图后进行分类以及定位（在图例中使用SVM进行分类并进行定位相关参数的回归）。
然而这种方式还是太慢了，因为需要对约2000个区域分别进行一次CNN的前向传播。从而我们可以沿用加速语义分割的方式：预处理图片并在导出的特征图上进行处理。

### Fast R-CNN
Fast R-CNN的想法很简单，就是先对整张图片进行一次特征提取，得到一张特征图，再在特征图上跑一遍区域建议算法，最后将所有建议的区域进行一次小的CNN二次处理，使用FC+softmax分类并且使用线性回归导出最终的定位信息。
![](/assets/CS-231n-11/8.png)
上文的介绍都很简略，接下来我们对其中的一些细节进行探究：
首先，我们如何将给出的建议区域全都映射成固定的大小？

可以注意到，这实际上也是一种降采样的过程，然而之前我们使用的降采样都是给定降采样的比例，而这里给出的是降采样的目标分辨率大小，这会引发一些问题，比如说缩放比例不是整数，因此划分的区域大小会不等，从而引发一些问题。
一种朴素的解决办法就是，直接大略地分块，不一定分成相同的大小，然而这样的分块方式会导致特征未对齐，可能会导致模型性能的大幅下降。
于是我们仍然需要将块划分成相同的大小，那这样的话每块的中心不一定会在像素中心，这时我们可以采用一种名为双线性插值的技术(bilinear interpolation)：
首先我们先计算出分割后每个格的中心坐标，则该点的数值由靠它最近的4个实际点的数据决定：
![](/assets/CS-231n-11/9.png)
$$f_{xy} = \sum_{i,j=1}^{2}f_{ij}\max(0,1-|x-x_i|)\max(0,1-|y-y_i|)$$

浅显地讲，我们可以认为一个实际点的数据对格点数据的影响按横纵坐标远近的乘积变化。


很好，在应用了Fast R-CNN技术之后，我们的模型效率有了质的改变，于是~~压力给到建议算法~~瓶颈就在于建议算法的效率。
![](/assets/CS-231n-11/10.png)

### Faster R-CNN
注意到，在实现Fast R-CNN算法时，我们直接使用CNN预处理出了特征图从而大幅加快了模型的效率，那么我们能否也让CNN给出建议从而进一步优化模型呢，答案是肯定的，我们可以在模型中加入RPN（Region Proposal Network）来直接使用预处理过的特征图给出建议而不用额外运用其它算法。

具体来说，就是在Fast R-CNN算法的基础上增加了箭头所指向的那一部分用来代替其它额外的区域建议算法：
![](/assets/CS-231n-11/11.png)

那么接下来我们讨论如何实现RPN：

具体来说，对于特征图中的每一个像素（由于CNN预处理过程中会降低分辨率，所以实际上特征图中的像素数量远低于原图），我们对若干个以该像素为中心的块(anchor box)（块的类型和大小是预先设定的）使用卷积网络进行二分类，判断该块区域是否含有物体，其中对于分类得分为正的块，我们运行一遍回归得到该块中物体的定位信息。
最后，按分类得分排序所有块，取最高的若干块作为建议区域。
我们来简略的计算一下输出参数的大小，假设导出的特征图为$H\times W$，对每个像素我们预设了$K$种不同的Anchor Box种类，于是对于分类问题我们需要输出总共$KHW$个得分，而对于位置信息，我们最多要输出$4KHW$条数据。
![](/assets/CS-231n-11/12.png)

在总体上看，对于整个Faster R-CNN网络，我们要优化以下四个损失：
1. RPN网络的二分类损失
2. RPN网络的回归损失
3. 最终得分向量的交叉熵损失
4. 最后得到box位置信息的回归损失

以上对各种多目标检测网络算法的介绍听上去十分美妙，然而它其实忽略了很多细节：
1. 略去了如何处理高度重叠区域的问题（实际上一般选取其中得分最高的）
2. 如何确定Anchor box的形状与大小
3. 如何给训练数据标注标签
4. 边界框回归部分的确定
5. ......

### One-Stage Object Detectors

再次回到整体架构，我们可以发现，无论如何优化，整个网络的运行都是需要经过两个阶段：
1. 需要在整张图片上运用一次：主干CNN网络，RPN
2. 需要在每个区域运用一次：分辨率转化，预测分类，预测目标框大小

但我们能否跳出这种范式呢，实际上是可以的，一种general的方法如下:

![](/assets/CS-231n-11/13.png)

对于一张图片，我们将其分为若干固定大小的小块，每个小块被称为grid，在每个小块的中心，类似于RPN，我们预先定义一些base box的形状，并以每个grid的中心，延伸出这些base box，接着对着这些所有的base box进行分类以及回归，对每个base box回归出五个输出：检测框大小四个参数以及一个是否为目标的置信度；对每个grid的中心输出一个分数向量，表示被分类为各类的分数，类似的总共需要输出$gridH\times gridW\times (5\times baseBoxNumber + C)$个输出

总体来说，单阶段目标检测器会在运行速度上表现更好但是精确度略逊一筹。

## Instance Segmentation
可以发现实际上这个任务就相当于在多目标识别之后，在每个识别框中进行一次每个像素只有两个类别（是否是检测框标签物体）的分割。于是很自然的，我们可以在Faster R-CNN的基础上进行一波微调，就可以得到可以用于实例分割的模型网络Mask R-CNN
### Mask R-CNN
![](/assets/CS-231n-11/14.png)
如图，在Faster R-CNN的基础上，我们只需要在最后加入一个Mask Prediction在每个区域中进行一个针对区域内每个像素的二分类即可