---
title: CS 231n Lec7 笔记
date: 2024-11-8 19:40
tags:
    - 循环神经网络
    - RNN
    - 嵌入层
    - BPTT
    - TBPTT
    - LSTM
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

本节将讲述另一神经网络的架构RNN(Recurrent Nerual Network)
## Review
回忆之前的神经网络，它们都遵循几乎同一个设计范式：
`Input->Hidden->Output`
然而这种设计范式有一个致命的限制，即它总是只能接受一个输入样本，并输出一个分类结果，进行一对一的处理。
然而有时候我们还需要处理如下这些需求：
![](/assets/CS-231n-9/1.png)
例如：
当我们需要为一张图片贴标签，或者生成一段话时，就需要One-To-Many
当我们需要将一个视频分类时，就需要Many-To-One
当我们需要为一个视频生成一段话时，就需要Many-To-Many
当我们需要从帧层面为视频分类时，就需要一对一的Many-To-Many(Fig-4)
## Recurrent Nerual Network
然而，前面我们也见识过，在大型神经网络中Hidden层的规模是相当大的，若是如上图为每一个输入都设计一个Hidden部分，那么计算资源的开销将会异常恐怖，于是我们引入一种全新的神经网络架构RNN：
![](/assets/CS-231n-9/2.png)
也许看图就能明白，RNN的核心思想就是将规模庞大的Hidden层作为一个有限状态机，它会随着输入改变状态从而达成多输入多输出的效果。
RNN的激励函数也很简单：
$$h_t = f_W(h_{t-1}, x_t)$$
其中$W$是一个固定的参数矩阵，用于激励函数，$h_t$指$t$时刻的状态$x_t$指$t$时刻的输入。
输出函数为：
$$y_t = f_{W_{hy}}(h_t)$$
将RNN的Hidden层展开，我们可以得到一张很浅显易懂的RNN工作原理图：
![](/assets/CS-231n-9/3.png)
*再次提醒注意：* RNN中的$W,W_{hh},W_{hx},W_{hy}$都是固定的参数，不会随着状态的变化而改变。
![](/assets/CS-231n-9/4.png)
从而，通过只选取一个输入或者指输入一次等操作，我们可以很轻易地在RNN中达成我们上面提到的另外三个需求。
### An example of vanilla RNN
一种朴素的RNN实现方式如下：
$$\begin{aligned}
& h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)\\
& y_t = W_{hy}h_t
\end{aligned}$$
值得一提的是，$x,h,y$是维度不同的一维向量，而权重则是在它们之间进行转换的矩阵，是固定值。

### An example: simple NLP
![](/assets/CS-231n-9/5.png)
这个例子展示了如何使用RNN在输入一个字母的情况下通过输入输出的连接预测出整个单词。
在这个例子中，输入为4维向量，状态为3维向量，输出为4维向量。于是我们也可以知道$W_{xh},W_{hh},W_{hy}$的大小分别为$4\times 3, 3\times 3, 3\times 4$。
但是我们从这个例子中可以发现在处理字母时，RNN的一个问题：
如果字母采用独热编码(One-hot coding)，那么输入向量就会极其稀疏（只有一个维度为1，其余为0），而其与$W_{xh}$的相乘也可以仅看作是从矩阵中提取出了一列，并未进行任何改变，于是我们在这种情况下一般会在输入和隐藏层之间引入：
### 嵌入层(Embedding Layer)
由于独热编码的性质，嵌入层的添加并不能改变第一次矩阵相乘只是相当于提取一列的行为，于是我们可以对列进行一些改变：
在嵌入层中，输入向量会与一个权重矩阵相乘，实际上仍是提取出一列，这个矩阵的维度一般是行数小于列数（即输入的维度被降低了），这是因为极其稀疏的高维向量实际上并不会蕴含很多信息，于是我们可以采用低维度稠密向量的输入来简化计算，减少开销。

### Backpropagation Through Time
BPTT是一种朴素的在RNN网络中进行反向传播的算法，在BPTT运行的过程中，我们首先对整个网络进行$1->T$时刻的前向传播，并缓存好中间计算结果。
之后从时刻$T->1$进行反向传播，需要注意的是由于隐藏状态$h_t$依赖于之前的隐藏状态，于是我们需要将$h_t$对应的梯度反向传播到之前所有的时刻。
另外由于RNN有多个输出，我们一般会对整个输出序列进行损失函数的评估，而对于每个输出$y_i$我们会计算出$\frac{\partial L}{\partial y_i}$以对每个输出进行反向传播。
![](/assets/CS-231n-9/6.png)

然而BPTT面临着一些很严重的问题：
1. 由于反向传播序列是一个连乘的序列，当计算的时间步$T$很大时，就很容易出现梯度爆炸和梯度消失的问题
2. 显然的，计算复杂度太高

### Truncated Backpropagation Through Time
TBPTT是一种对BPTT的改进反向传播算法，为了解决序列过长导致的梯度爆炸，消失，计算复杂度太大等问题，TBPTT将整个序列分组，并在每组组内进行反向传播而不跨组传播梯度。

### Applications
这里PPT中介绍了RNN在NLP中的应用，比如寻找具有特定属性的句子等等，要看可以去PPT。

## Long Short Term Memory
PPT中只对此进行了简要的介绍，这是一种对RNN的改进，可以极大地简化RNN中反向梯度的流动过程：
$$\begin{aligned}
&\begin{bmatrix}i\\f\\o\\g\end{bmatrix} =  \begin{bmatrix}\sigma\\\sigma\\\sigma\\\tanh\end{bmatrix}W\binom{h_{t-1}}{x_t}\\
&c_t = fc_{t-1} + ig\\
&y_t = o\tanh(c_t)
\end{aligned}$$

这节只是简要介绍，在之后的笔记中会记录推荐阅读材料中对RNN和LSTM的详细解释。