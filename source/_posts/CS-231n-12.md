---
title: CS 231n Lec10 笔记
date: 2024-11-24 20:07
tags:
    - 视频理解
    - Early/Late Fusion
    - 3D CNN
    - Optical Flow
    - RCN
    - Spatio-Temporal Self-Attetion
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

## Video Understanding: Example Tasks
在图像识别的任务中，我们在图片中检测物体并进行分类，分割等一系列任务。从而在视频理解中，我们的一大认识便是识别分类物体的运动即动作。但在一切的一切之前，有个巨大的问题：相较于图片，视频的体积过于庞大，在未经压缩的情况下，一般分辨率的视频可以轻松达到GB/min的大小。
一种显而易见的解决方案便是对食品进行压缩。即降采样，降帧率。
在训练时，我们提取一些关键帧进行训练，例如从21帧中选取7帧。
在测试时，我们对多个连续的帧序列进行预测（1-7, 2-8, 3-9帧等等），并取平均的预测结果。

## Single-Frame CNN
这是一个朴素的视频理解算法，不难注意到，视频其实就是图片加上时间尺度，于是可以训练一个普通的二维CNN网络为视频的每一帧单独分类，并在测试时使用帧之间平均预测的方式。
![](/assets/CS-231n-12/1.png)
虽然这种架构的性能一般不尽如人意，但是这为接下来的一些架构提供了很强的基础范式。

于是我们可以再这基础上先进行一些小小的改进
### Late Fusion
很容易注意到，朴素单帧CNN的一大问题在于视频每帧之间是割裂的，信息整合只是对每帧的得分进行简单的取平均，那么一个简单的改进方向便可以确定，便是在每帧得出分类结果之前对中间结果进行时间尺度上的信息整合：
Late Fusion便是在对特征图进行分类之前拼接帧特征进行整体得分计算的技术：
![](/assets/CS-231n-11/2.png)
另外，为了防止因为特征图数量过于庞大导致的最后一步输入参数量过于庞大，在将输出导入MLP之前，我们可以对特征图在时空尺度上进行平均池化。

不过，Late Fusion有一个致命的问题，便是对帧之间的微小变化并不敏感（池化以及特征提取会损失信息）

### Early Fusion
相较于在CNN提取特征之后整合时间尺度上的信息，直接对输入进行整合可能会在一定程度上改善上述问题：
![](/assets/CS-231n-11/3.png)
其实这种整合很好理解，就是将不同帧在通道维度上不断堆叠，拼接之后再传入CNN提取特征。
然而，对于以上所有模型，它们均只通过一层简单拼接整合时间尺度·的信息，这显然是不太足够的，那么接下来我们需要对CNN内部结构入手对整体进行改进。

## 3D-CNN
不难注意到，之前的所有问题成因都是时间尺度上的信息整合太快，都是通过一步操作整合全部信息，然而在CNN架构与baseline不变的情况下，这种问题是难以得到改善的。

回忆CNN的架构，卷积核在空间尺度上缓慢整合空间尺度上的信息，逐渐增加一个像素的感受野，那么一个自然的想法就是，我们能不能在时间尺度上也采取这种技术，其实实现也很简单，只需要将CNN的卷积核进行升维。

具体来说，在3D-CNN中我们采用$T'\times H'\times W'$的卷积核，在三维尺度上进行滑动，从而实现在时空尺度上同步的缓慢信息整合。
![](/assets/CS-231n-11/4.png)
### Early Fusion v.s. Late Fusion v.s. 3D-CNN
![](/assets/CS-231n-11/5.png)
如表格所示，这三者当中，Late Fusion只有在最后一步取全局平均值的时候才会在时间尺度的感受野上增长，并且一次增长到整个序列，类似的Early Fusion只有第一步。而3D-CNN是唯一每一步都在使时间尺度上的感受野缓慢增长的。

### 2D Conv(Early Fusion) v.s. 3D Conv
显而易见的，在二维卷积中，卷积核总是对整个帧序列进行卷积，这表示一个卷积核会整合所有帧的信息，但这也就代表，卷积核上的一个参数仅仅能对应一个帧序列中的信息，从而导致卷积核难以泛化不同时间的统一动作，为了识别不同时间的动作，我们需要学习更多的卷积核来泛化特征。
但在三维卷积中，这样的问题就不存在，因为卷积核也会在时间尺度上进行滑动，从而能有效提取不同时间统一动作的特征，这也被称作三维卷积时间平移的不变性(temporal shift-invariant)。

### An Example of 3D-CNN: C3D
如图，类似于2D-CNN中的VGG网络，C3D的架构如下图所示，也是由若干个三维卷积+池化的stack构成的：
![](/assets/CS-231n-11/6.png)
这种网络的问题在于计算量过大(3倍VGG网络的开销)。

## Recognizing Actions From Motion: Optical Flow
运动运动，动是重点，从而我们能不能通过识别视频的变化（即运动）来识别动作的类型，但是问题在于，我们该如何量化视频的运动呢？
我们需要引入一个新的知识：光流(Optical Flow)
光流是描述视频帧之间像素移动的图（即某一像素在下一帧会流向哪里）
![](/assets/CS-231n-11/7.png)
形式化的讲，若光流为$F$，衡量$l_t,l_{t+1}$的变化，那么我们有：
$$\begin{aligned}
& F_{x,y} = (dx, dy)
\\
&l_{t+1}(x+dx, y+dy) = l_t(x,y)
\end{aligned}$$
于是我们可以基于光流进行预测，开发出一种新的模型架构：双流网络(Two-Stream Network)。
![](/assets/CS-231n-11/8.png)
需要注意的是，由于两帧之间的光流需要分为横坐标方向和纵坐标方向，因此总共的光流图数目需要加倍。
关于如何处理出光流图，我们可以采用Early Fusion的方式计算。

## Modeling Long-Term Temporal Structure
需要注意，到目前为止，我们介绍的模型只能用于处理3-5秒的片段，否则计算开销是难以接受的，那么我们该如何处理长期依赖关系呢？
自然的，联想到视频也是一种序列型的数据，于是我们可以尝试联系RNN进行处理。
具体来说，我们将整个时间序列分成若干个小片段，对每一个小片段使用CNN（2/3D）提取特征，最后在将特征作为序列数据作为输入传入RNN进行预测（使用LSTM等）。

在这一种融合的架构中，我们尝试考虑内部的构造：
Inside CNN:特征图中的每一个值都是原图某一部分的函数
Inside RNN:每一个向量都是之前所有向量的函数
思考这些是为了尝试能不能讲两种架构融合在一起而不是生硬地拼接。
事实上是可以的，我们将融合而成的网络架构称为循环卷积网络(Recurrent Convolutional Network)
### Recurrent Convoutional Network
这种网络的整体架构如下：
![](/assets/CS-231n-11/9.png)
可以看出，每一个状态都由两个前驱状态直接相连：
1. 上一时间的同层状态
2. 同一时间步的上一层状态

具体来说该如何实现转移呢，我们可以先回顾一下CNN与RNN中的一些抽象范式：
CNN:
$$\mathrm{Conv2d}(X_{C\times H\times W}) = Y_{C'\times H'\times W'}$$
RNN:
$$\begin{aligned}
&h_{t+1} = f_W(h_t,x)
\\\text{Vanilla: }&h_{t+1} = \tanh(W_hh_t+W_xx_{t+1})
\end{aligned}$$
于是将这两种抽象范式进行融合，就得到了RCN中的状态转移方式：
$$h_{L,t} = tanh(W_h\mathrm{Conv2d}(h_{L,t-1}) + W_x\mathrm{Conv2d}(h_{L-1,t}))$$
虽然这样看上去是一种很完善的架构，但是事实上它存在着和朴素RNN相同的问题，便是由于无法并行计算，对长序列的处理运算过慢。
那么我们在RNN中是如何解决这样的问题的呢，没错，注意力又来了。
## Spatio-Temporal Self-Attetion(Nonlocal Block)
在时空自注意力的计算过程中，我们首先需要将原视频使用三维卷积处理成$C\times T\times H\times W$的特征，接着，类似于普通股自注意力计算中输入之后的全连接线性变化层，我们在特征图上再次进行$1\times 1\times 1$的三维卷积导出$C'\times T\times H\times W$的数据，分别处理三次，得到询问矩阵，键矩阵，值矩阵三组数据($\textbf{Q,K,V}$)，接下来便可以类似地计算出三维的自注意力:
$$\begin{aligned}
&\textbf{Att} = \mathrm{Softmax}(\textbf{Q}^T\textbf{K})
\\&\textbf{Y} = \textbf{V}\textbf{Att}
\end{aligned}$$
注意，在这里我们将它们都视作是$C'\times(THW)$的矩阵，表示有$THW$个输入向量以及对应的询问与输出（每个$1\times C'$的向量表示某个像素点某刻的序列输入信息）。
最后输出的$\textbf{Y}$矩阵也是一个$C'\times(THW)$的矩阵，我们继续通过$1\times 1\times 1$的卷积将它还原成$C\times T\times H\times W$的最终输出（还可以采用残差连接）。
![](/assets/CS-231n-11/10.png)

于是我们就完成了3D卷积网络中的自注意力模块的构建，可以向卷积网络中加入这种Nonlocal Block改善模型的性能。
![](/assets/CS-231n-11/11.png)

## Inflating 2D Networks to 3D
我们可以注意到对视频的分类理解可以被视作图像理解在维度上的扩充，从而我们有没有一种办法，把数量繁多的2D网络扩展到3D？其实这是可以的，关键就在于将2D架构中的所有卷积/池化操作都替换成对应的3D版本。
同时2D网络卷积核的初始化，仍然可以为3D使用，我们只需将每一个卷积核复制$T$次，并对所有数据都同除$T$即可达成一样的效果。
![](/assets/CS-231n-11/12.png)

## Vision Transformer for Video(ViViT)
## Temporal Action Localization
这两部分都是纯介绍内容，就不写了，可以去看PPT上的图做大致了解。