---
title: CS 231n Note6 笔记
date: 2024-10-23 20:50:54
tags:
    - 人工神经网络
    - 正则化方法
    - 数据预处理
    - 初始化权重
categories: 机器学习笔记
mathjax: true
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>


在上一部分内容中，我们已经知道了神经网络的构建，我们可以通过应用这一部分知识，得到一个能力很强的分类器。而在这一部分内容中，我们将讨论在一开始我们该干什么，也就是数据预处理，权重初始化以及损失函数的选择
## Data Preprocessing
在讲解一这一部分内容之前，我们现来约定一下符号，假定数据矩阵为$X$，$X$是一个$N\times D$的矩阵，表示训练数据有$N$条，$D$个数据维度。
**均值减法(Mean Subtraction)** 是一种极其常用并且可以与其他方法结合的预处理方法，这种方法很简单，就是对于每一维数据，对所有数据减去这一维数据的均值。这种方法可以使任意维的数据都是一0为中心的，体现在空间中就是整组数据以原点为中心。在python中的实现如下:
```python

X -= np.mean(X, axis = 0)
X -= np.mean(X) #对于图像处理，这也是一种常见的方法，它对于每一个像素减去所有像素的平均值
```
**归一化(Normalization)** 是一种对各维数据的尺度进行统一的方法，它避免了数据因为衡量标准的不一导致不同维数据间的大小差异很大的情况。这一版应用在数据尺度差异较大且不同维度数据对于学习的重要性相差不大的情况（在图像处理中，由于每一个像素都是[0, 255]间的实数，所以归一化一般来说不是很重要）。归一化有很多种方法，比较常用的有：
1. 对每一维数据除以该维数据的标准差
2. 对每一维数据将最大最小值缩放为-1，1

**主成分分析(PCA)和白化(Whitening)** 是另一种数据预处理的方法，注意在进行这两种方法之前需要首先对数据进行中心化，以便后续计算。
首先，我们需要计算数据的协方差矩阵
```python
X -= np.mean(X, axis = 0)
cov = np.dot(X.T, X) / X.shape[0] #计算得到协方差矩阵是一个D*D的矩阵
```
其中`cov[i, j]`表示第$i$维和第$j$维数据间的协方差，协方差的绝对值越大表示两维元素之间的相关性越大，正则为正相关，负则为负相关。
我们可以发现协方差矩阵是一个对称半正定的矩阵。
补：$X$与$Y$间的协方差$\sum_{i = 1}^{n}(x_i - avg(x))(y_i - avg(y))$
接下来我们可以对协方差矩阵进行SVD分解（对协方差方阵来说，这就是特征值分解）
```python
U, S, V = np.linalg.svd(cov)
```
补充一下，$U$是协方差矩阵$D$个特征向量为列组成的方阵，$S$是协方差矩阵$D$个特征值组成的向量。对协方差矩阵的特征值分解可以写成$cov = U\Sigma U^T$其中$\Sigma$是由特征值构成的对角矩阵。
接下来我们应用得到的矩阵对$X$进行一些处理，直观的说，就是将$X$投影到$U$上，也就是把$X$的坐标转换为以$U$的列向量为基的坐标。(这是由于$U$是实对称矩阵，有$D$个线性无关的特征向量)
```python
Xrot = np.dot(X, U)
```
原讲义中将这一步解释为$X$在某空间中的旋转，实际意思差不多。但有用的是经过这一步操作，我们对$X$中的不同维数据进行了去相关化处理，具体来说，$Xrot$的协方差矩阵已经是对角阵了，证明如下：
$$\begin{aligned}
&cov = X^TX = U\Sigma U^T\\
&Xrot = XU\\
&Xrot^TXrot = U^TX^TXU = U^TcovU = \Sigma
\end{aligned}$$
其实进一步，这个矩阵的协方差矩阵就是特征值矩阵。
实际上，这种方法还有另一种作用。
直观的，每一个特征向量蕴含的信息量(即对应向量上各数据的投影的差异大小)与对应的特征值成正比，于是，我们在去相关化时可以不使用所有的特征向量，转而采用较大的特征值所对应的几个特征向量，即可在不损失较多数据量的情况下，使数据的规模得到可观的减小，对应代码如下：
```python
Xrot_reduced = np.dot(X, U[:, :100]) #选择特征值最大的100个特征向量
```
值得注意的是`numpy`中自带的SVD分解得到的特征向量以及特征值就是按从大到小的顺序进行排序的，于是只需从前开始选取即可。

最后一个预处理的方法就是白化(Whitening)，简单来说，白化就是将去相关化后的矩阵的每一维除以这一维的基对应的特征开的根号。讲义中还提到，这种做法的几何解释使，如果输入的数据是一个多变量的高斯分布，那么处理后的数据将也是一个均值为0并且协方差矩阵不变的高斯分布。
```python
eps = 1e-5
Xwhite = Xrot / np.sqrt(S + eps)
```
*Warning:Exaggerating noise* 需要注意的是我们在特征值的后面加上了一个小值，这是为了防止出现除以0，但这样会夸张数据中的噪声，这是因为噪声一般信息量小，导致特征值小，在经过这样的除法后会导致数据被放大很多。为了解决这一个问题，我们可以增大`eps`的值。
![](/assets/CS-231n-5/1.png)
![](/assets/CS-231n-5/2.png)
实际上，在卷积神经网络(CNN)中，我们并不会应用PCA或是Whitening，然而，中心化，归一化数据仍然是很重要的。
此外，需要注意，我们只应在训练数据上进行数据预处理，测试数据不应进行改动。

## Weight Initialization
*Pitfall：全零初始化* 由于我们并不知道最终的结果是什么，一个合理的假设就是我们需要让权重数据尽可能是对称的（一半是正的，一半是负的）。因而就会有人想，我们能不能设置全零的初始化，然而这是完全不行的。这是因为这样的话会导致权重神经元的输出相等，从而导致神经网络具有很强的对称性。很强的对称性是一个好的神经网络应避免的。

**小随机数初始权重(Small ramdom numbers)** 虽然我们严厉禁止了0初始值的使用，但是我们还是很希望我们的初始权重能够尽可能地接近0。于是我们可以采用多变量高斯分布生成一个均值为0的随机初始化向量，同时为了使权重的初始值尽可能小，我们还可以在生成之后为向量乘上一个小量，代码如下：
```python
W = 0.01 * np.random.randn(D, H)
```
生成的向量在空间中指向一个完全随机的方向。
此外，使用均匀分布也似乎是一个不错的选择，但在实际表现上差别不大。
*Warning* 需要注意的是，并不是初始权重越小会获得更好的表现，太小的初始化权重会导致反向传播计算出的梯度值偏小引发诸多问题，这在深层神经网络中体现得尤为明显。

**校准方差(Calibrating the variances)** 随着权重数量的增加，会不可避免地出现一个问题，就是输出数据的方差变大，然而我们希望在初始权重的条件下，输出数据的方差应当与输入数据大致相同，于是我们可以考虑在生成权重后校准数据以达到这一点。为了搞清楚如何校准，我们可以先计算一下输出的方差：
$$\begin{aligned}
Var(out) &= Var(\sum_{i=1}^nw_ix_i)\\
&= \sum_{i=1}^nVar(w_ix_i)\\
&=\sum_{i=1}^n(E(x_i))^2Var(w_i) + (E(w_i))^2Var(x_i) +Var(x_i)Var(w_i)\\
&= (nVar(w))Var(x)
\end{aligned}$$
我们发现，要使输出数据的方差与输入方差相同，只需使$w$的方差为$\frac{1}{n}$。由于生成的随机权重服从方差为$1$的的高斯分布，只需对每一个权重除以$\frac{1}{\sqrt{n}}$即可。
除此之外，还有研究建议不同的方差调整方式，一篇论文建议将初始权重的方差设置为 $\frac{2}{n_{in} + n_{out}}$ ，其中分母的两个符号分别代表上一层的神经元数量和下一层的神经元数量。
另一片论文建议，对于采用 $ReLU$ 的神经元，调整后的方差应当是 $\frac{2}{n}$ 。

**稀疏初始化(Sparse Initialization)** 其实我们也是可以设置全零初始化的，但是在这种情况下我们要破坏神经网络的对称性，一种解决的办法就是不在全链接，对每个神经元，我们只连接固定数量的神经元(应当较小，比如10)

**初始化偏置向量(Initializing the bias)** 一般来说，由于偏置向量并不参与相乘过程，因此将其初始化设置为0是可能且有道理的。但同时，我们也可以将其设置为小常数，这可以在一开始就激活ReLU神经元，然而，实际上我们并不清楚这是否会改善网络的性能。

**批归一化(Batch Normalization)** 这种方法在卷积层之后，非线性层之前，插入一个单位高斯分布来强制激活神经元，这是一个相当有用且巧妙的方法，然而讲义中未详细介绍，稍后可以进行了解。

## Regularization
**L2正则化(L2 Regularization)** 是一种常用的正则化方法，它在损失函数中加上了一项L2正则化损失:$\frac{1}{2}\lambda w^2$，其中$\lambda$是一个控制正则化强度的超参数。为什么前面有一个$\frac{1}{2}$呢，这是因为这会在求导后被消掉。
在梯度下降中，这表示在每个权重都减去一个$\lambda\times w$值，因而会导致整个矩阵逐渐变小。这种方法会使权重矩阵向着分散且小的方向变化而非有几个"尖锐"的权重导致过拟合或被噪声大幅影响。
**L1正则化(L1 Regularization)** 也是一种常用的正则化方法，它在L2损失函数的基础上加了一项:$L1(w) = \lambda_1|w| +\frac{1}{2}\lambda_2w^2$。这表示在L2的基础上，每个权重还会向0方向移动$\lambda_1$单位。L1正则化会导致权重矩阵变得稀疏且趋于0，它会主动选择重要的特征使他们对应的权重较大，因而对噪声极度不敏感，与之相对L2会导致矩阵变得分散而不稀疏。
**最大范数约束(Max norm constriants)** 这种正则化方法会强制权重向量$\vec{w}$的范数小于一个特定的值（一般为3或者4），这种方法可能带来性能的提升（存疑），这个方法的好处主要体现在因为权重的总体大小被限制了，网络并不会"爆炸"（发生极大改变），从而较高的学习率并不会带来很严重的后果。

**Dropout** 是一个极其简单且有效的正则化方法它可以与前几种方法结合，简而言之，它在训练的过程中，将每一个神经元设置为概率被激活，并且在执行预测时将这个损失修正，达到避免噪声和过拟合的目的，代码如下：
```python
p = 0.5 # probability of keeping a unit active. higher = less dropout
def train_step(X):
""" X contains the data """
# forward pass for example 3-layer neural network
	H1 = np.maximum(0, np.dot(W1, X) + b1)
	U1 = np.random.rand(* H1.shape) < p # first dropout mask
	H1 * = U1 # drop!
	H2 = np.maximum(0, np.dot(W2, H1) + b2)
	U2 = np.random.rand(* H2.shape) < p # second dropout mask
	H2 * = U2 # drop!
	out = np.dot(W3, H2) + b3

# backward pass: compute gradients... (not shown)
# perform parameter update... (not shown)
def predict(X):
# ensembled forward pass
	H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
	H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
	out = np.dot(W3, H2) + b3
```
其中因为训练过程中部分神经元关闭，势必会造成最终输出得分函数的值大小以$p$为比例缩减，因而我们在预测时需乘$p$修正。
然而在较大的模型中，推理的性能是很重要的，所以我们一般采用下面的实现
```python
p = 0.5 # probability of keeping a unit active. higher = less dropout
def train_step(X):
""" X contains the data """
# forward pass for example 3-layer neural network
	H1 = np.maximum(0, np.dot(W1, X) + b1)
	U1 = (np.random.rand(* H1.shape) < p) / p # first dropout mask
	H1 * = U1 # drop!
	H2 = np.maximum(0, np.dot(W2, H1) + b2)
	U2 = (np.random.rand(* H2.shape) < p) / p # second dropout mask
	H2 * = U2 # drop!
	out = np.dot(W3, H2) + b3

# backward pass: compute gradients... (not shown)
# perform parameter update... (not shown)
def predict(X):
# ensembled forward pass
	H1 = np.maximum(0, np.dot(W1, X) + b1) # NOTE: scale the activations
	H2 = np.maximum(0, np.dot(W2, H1) + b2) # NOTE: scale the activations
	out = np.dot(W3, H2) + b3
```
这种实现直接在训练过程中将缩小的数值通过除以$p$恢复原来的尺度，这样我们就可以不用在推理时进行修正损失性能。
**Theme of noise in forward pass**
从上面的处理方法我们可以看出，对于噪声的处理我们一般采取两种方法：
1. 分析化处理（如dropout时乘p）
2. 数值化处理（如在训练过程中引入随机过程）

在卷积神经网络中我们也运用了一些其他方法来减小噪声的影响，在稍后我们会介绍。
**Bias regularization**
偏置向量因为不会参与乘法过程，因而并不必要采用正则化，但是对偏置向量采用正则化并不会大幅影响性能，所以在实践中还是会对其采取一些正则化
**Per-layer regularization**
这就是在神经网络中的不同层采用不同强度的正则化，然而这在实际上并不常用。
**In practice**
实际上，我们一般采用使用交叉验证的L2正则化，并采用$p=0.5$的dropout。

## Loss Function
上一节中我们讨论了正则化，它可以被看作是对于模型复杂化的惩罚。回想一下，而我们目标的第二部分就是尽可能最小化损失函数(准确来说是最小化对所有数据损失函数的均值)，于是我们可能要解决下面几个问题
**分类问题(Classification)**
在之前我们已经介绍过了SVM以及softmax分类器，在这里我们也可以使用这两个分类器对应的损失函数：
首先是hinge loss:
$$L_i = \sum_{j\neq y_i}max(0, f_j-f_i+1)$$
然而在实际应用中，有报告指出，在这个损失函数的基础上进行一次平方会获得更好的性能，即(squared hinge loss)：
$$L_i = \sum_{j\neq y_i}max(0, f_j-f_i+1)^2$$
此外还有Softmax分类器对应的交叉熵损失函数(cross-entropy loss)：
$$L_i = -\log(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}})$$
但在应用交叉熵损失函数的时候存在一个问题，就是当分类器需要处理的标签过多之时，计算该损失函数会极其消耗计算资源。为了解决这一问题，有一种名为Hierarchical Softmax的算法，以分类单词为例，它将所有单词都表示成树上的一个路径，我们训练分类器在每个节点上选择分支的正误。然而这种算法极其依赖树的结构并且实际上也依赖于实际问题。
**属性分类(Attribute Classification)**
在实际场景中，除了需要将数据分类的情况，还有需要给各个数据打上一个或多个标签的场景，在这种情况下，之前的分类器就不再能够直接使用了。
在这个情况下，一种常见的做法就是为每一个属性都构建一个单独的二分类器，一种可能的损失函数如下：
$$L_i = \sum_jmax(0, 1-y_{ij}f_j)$$
其中$y_{ij}$是一个指示器函数，当数据$i$具有属性$j$时为1，否则为-1。这时该函数的大意即为当具有标签时，得分低于1分会使得损失增加，若没有标签，则得分高于-1分会使得分增加。
另一种做法就是为每一个属性都独立地训练一个逻辑斯蒂回归分类器(Logistic Regression Classifier)，该函数类似于一个概率分布，它会计算出具有该属性的概率：
$$P(y=1 \mid x ; w, b)=\frac{1}{1+e^{-\left(w^{T} x+b\right)}}=\sigma\left(w^{T} x+b\right)$$
因而我们可以计算出对于该分类器的损失函数：
$$L_{i}=-\sum_{j} y_{i j} \log \left(\sigma\left(f_{j}\right)\right)+\left(1-y_{i j}\right) \log \left(1-\sigma\left(f_{j}\right)\right)$$
这个表达式看上去很复杂，但实际上就是一个在$(0,1)$单减的函数(当没有属性时)或是一个在$(0,1)$单增的函数(当数据具有该属性时)。此外，它的微分形式也很简单，如下：
$$\frac{\partial L_i}{\partial f_j} = \sigma(f_j) - y_{ij}$$
**回归问题(Regression)**
回归问题是一勒需要我们预测，拟合实值函数的问题，对于这类问题，我们一般通过将预测数值与实际答案相比较来计算出对应的损失函数。
L2标准平方(L2 norm)是一种常用的衡量预测差异的方式：
$$L_i = ||f - y_i||^2$$
此外我们还可以使用L1标准(L1 norm)：
$$L_i = \sum_j|f_j - y_{ij}|$$
我们用$\delta_{ij}$表示第$i$个样本的第$j$个维度上预测值与真实值的差异，那么我们就能很清楚地表示上述两式的梯度：
$$\begin{aligned}
\frac{\partial L2_i}{\partial f_j} &= 2\delta_{ij}\\
\frac{\partial L1_i}{\partial f_j} &= \text{sign}(\delta_{ij})
\end{aligned}$$
*注意：*
L2损失会比更稳定的分类器损失比如Softmax更难以优化，这是因为在回归问题中我们需要输出一个预测的值而非简单的分类。所以处理回归问题时，我们需要更加注意网络的构建（此时它的结构变得更加脆弱）。
此外，L2损失不稳定的另一大原因就是异常值会在它的身上引入较大的梯度。
因此一般来说回归是不必要的，我们更常采用的方法是将一段连续的取值离散化或划分成若干个区间并使用分类。
当不得不使用回归时，一定要小心，比如由于回归的不稳定性，在它身上使用`dropout`的方法就是不合适的。

**结构预测(Structured prediction)**
有时我们还会遇到另一种棘手的情况，就是当标签空间太大难以枚举时（比如需要预测树，图的结构）。在这种情况下我们一般就需要开发特定的求解器，这些求解器需要运用特定情境下给出的简化条件，不具有普适性，这部分内容超出了课程的范围，因此略过。