---
title: CS 231n Lec8 笔记
date: 2024-11-19 18:52
tags:
    - 注意力机制
    - Transformer
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

## Review of Encoder-Decoder Architecture
上一次笔记中我们提到了基于编码-解码的序列处理模型：
![](/assets/CS-231n-10/1.png)
我们将$\textbf{c}$称作是上下文向量(Context Vector)，其中编码器，解码器的激励函数分别为：
$$\begin{aligned}
& \textbf{h}_t = f_\textbf{W}(\textbf{x}_t,\textbf{h}_{t-1})\\
& \textbf{s}_t = g_\textbf{U}(\textbf{y}_{t-1},\textbf{s}_{t-1},\textbf{c})
\end{aligned}$$
可以注意到$\textbf{c}$是作为额外输入作用在解码器的状态转移上的。
*注意：* 在训练过程中，我们总是给解码器提供期望的输出$\textbf{y}_{t-1}$作为下一时刻的输入(Teacher Forcing)，而在推理时直接使用上一时刻的输出。

然而，这样的$\textbf{c}$的生成方式存在问题：
不管输入序列长度如何，它们蕴含的信息都被压缩在一个固定大小的$\textbf{c}$中，当序列较短时可能表现仍较好，但当序列长度较长时（如$\tau = 1000$）时$\textbf{c}$就很有可能丢失较早输入的信息。（理想状态下我们应当能从$\textbf{c}$中获取输入的全部信息）。

## Attention is All You Need
从而我们开始引入注意力机制：我们难以将所有的输入都压缩在一个固定的向量中，但我们可以从中挑选出一些较为重要的向量压缩（损失信息量大大减小）并作为额外输入，问题就在于对于每一时间步，重要的向量都不尽相同，从而需要引入一种方法来决定哪些输入是重要的。
具体来讲，在解码器中，对于每一时间步$t$，它都有一个专属的注意力向量$\textbf{a}_t$以及对应的上下文向量$\textbf{c}_t$，其中:
$$\textbf{c}_t = \textbf{a}_t^T\begin{bmatrix}\textbf{h}_1\\\textbf{h}_2\\...\\\textbf{h}_n\end{bmatrix}$$
这样我们就得到了一个输入信息的加权和，权重即为计算出的$\textbf{a}_t$。那么现在问题来到了如何确定这个注意力向量：
我们首先根据上一步解码器中的状态$\textbf{s}_{t-1}$计算出下一时间步可能用到的输入信息并按重要性赋权重，得到$\textbf{e}_t$，再利用sofrmax层对该向量进行标准化得到$\textbf{a}_t$，表示下一时刻应当注意到哪些$\textbf{h}$。

形式化地讲，我们有一个线性函数$f_{att}$：
$$\textbf{e}_t = f_{att}(\textbf{s}_{t-1},\textbf{H})$$
举一个例子，一个翻译RNN将"I love octopus"翻译为中文"我爱章鱼"，那么在解码器的第一个状态，即应当输出"我"时，得到的注意力向量就可能为{"I":$0.99$, "love":0.005, "octopus": 0.005}。
从而上下文向量不在来自于对整个输入序列的暴力压缩，而是来自于对重要信息的注意，最大程度地保留了需要的信息。
![](/assets/CS-231n-10/2.png)
另外，还需要注意的是，context vector对输入序列的注意是不关注顺序的，也就是说，在Decoder眼中$\textbf{H}$可以被视作一个无序集合。

## Image Captioning with Attention
在图生文技术中，我们一般先使用一个预训练过的CNN对图片进行特征提取，再将特征图导入RNN中生成文本。
在没有注意力机制时，仍然是同样的问题，由编码器生成的$\textbf{c}$难以概括图片的全部特征。

在引入了注意力机制后，类似地，我们可以这样改进RNN：
![](/assets/CS-231n-10/3.png)
如图所示：
$$\begin{aligned}
&\textbf{e}_t = f_{att}(\textbf{H}, \textbf{z})\\
&\textbf{a}_t = \mathrm{softmax}(\textbf{e}_t)\\
&\textbf{c}_t = \textbf{a}_t * \textbf{z}
\end{aligned}$$
*注意：* 前向传播的整个过程都是可微的，模型会自动优化注意力权重，即$f_{att}$，无需对其进行监督。

## General Attention Layer

观察上下文向量的生成过程，可以注意到，我们可以将$\textbf{h}\rightarrow\textbf{c}$的过程视作内容为$\textbf{h}$的询问，从而我们可以对整个注意力过程进行抽象，封装成一个注意力层。这样有以下几点优化以及好处：
1. 在询问$\textbf{Q}$已知时，我们可以高效地并行计算每一个询问向量对应的输出上下文向量。
2. 我们可以采用一些$f_{att}$的简单实现，如直接进行点乘，但当输入维度很高的时候，由于$\mathrm{softmax}$的存在，输出的$textbf{c}$往往具有很高的方差，因而信息熵很低，此时我们可以使用$\textbf{e}=\frac{\textbf{h}^T\textbf{x}}{dim_x}$来缓解这一现象。
3. 我们可以在输入序列$\textbf{x}$后插入全连接层增强注意力层的表达能力。

![](/assets/CS-231n-10/4.png)

## Self-Attention Layer
虽然General Attention Layer使得并行计算上下文向量变得轻而易举，然而我们还应该注意到，若是仍然使用解码器中的状态向量$\textbf{h}$作为查询，我们仍需要解决时序依赖的问题，否则并行计算仍然是不可能的。
于是，接下来我们应当开始取消注意力层对额外输入的查询向量的依赖，从而得到自注意力层(Self-Attetion Layer)。
然而查询向量还是需要的，从而一种很朴素的想法便是讲输入向量$\textbf{x}$直接作为查询向量，使用自己注意到自己。
![](/assets/CS-231n-10/5.png)

这样的想法是美好的，然而这又引入了一个新的问题，之前我们提到，注意力计算是对输入序列的顺序不敏感的，那么我们该如何处理有序序列呢。

解决方案是引入一个position encoding层，$\textbf{x}_i\rightarrow (\textbf{x}_i,\textbf{p}_i)$其中$\textbf{p}_i$就是$\textbf{x}_i$对应的位置向量。
这个位置向量的计算应当满足一些性质：
1. 对每一个时间步，应当输出一个独特的位置编码
2. 任意两时间步中间的距离在不同长度的句子中应当保持一致
3. 应当可以被简单地推广到更长的序列，值是有界的，但是输出应当是确定性的

以上性质有点抽象，为了满足这些性质，我们一般有两种做法：
1. 学习一个$T\times dim$的表，位置编码时查表即可
2. 使用满足期望性质的固定函数

让我们来看一个例子，也就是Transformer论文Attetion is All Your Need中使用的位置编码固定函数：
$$p(t) = \left(\sin\omega_1t,\cos\omega_1t,...,\sin\omega_\frac{d}{2}t,\cos\omega_\frac{d}{2}t\right)$$
其中
$$\omega_k = \frac{1}{10000^{\frac{2k}{d}}}$$
简单来说，该函数就是会生成二进制计数器序列。

## Masked Self-Attention Layer
之前的自注意力层对于每次询问都会使用所有的输入向量进行加权得到注意力向量，然而很显然的，有时我们并不希望开始的输出能够注意到整个序列，我们希望它们只能按时间顺序注意到前几个输入向量，于是，掩码自注意力层应运而生：
实际上，这种层的构建相当的简单，对于输入序列，我们会输出一个Alignment Score矩阵$\textbf{E}$，此时，我们只需将这个矩阵中的下半部分元素全部置$-\infty$，即可让输出的注意力矩阵呈现上三角的形状，即表示输出按时间顺序注意到输入，防止前面的输出窥探到未来的信息。
![](/assets/CS-231n-10/6.png)

## Multi-head Self-Attetion Layer
在之前的所有注意力层构建中，我们只通过一个"头"来计算注意力向量，只学习一组注意力层权重，因而，这种方式会限制注意力层对不同特征的泛化能力。于是，我们可以学习多组参数，最后将生成出的多组注意力向量拼接起来即可。
形式化地讲：
$$\begin{aligned}
&\text{Input:} X_{N\times D}\\
&Q_i = XW_{Q_i}, K_i = XW_{K_i}, V_i=XW_{V_i}\\
&Att_i = \mathrm{Softmax}(\frac{Q_i^TK_i}{\sqrt{D}})V_i\\
&\mathrm{Multi-Head}(Q,K,V) = \begin{bmatrix}Att_1|Att_2|......|Att_g\end{bmatrix}W_o
\end{aligned}$$
其中$W_o$是用于讲拼接矩阵映射会原维度的矩阵

## Transformer
以下是一个Transformer的总体架构，类似的，仍然是由编码器解码器构成：
![](/assets/CS-231n-10/7.png)
其中编码器解码器都是由多层的，成组的多头注意力层（部分不是自注意力）构成的：

![](/assets/CS-231n-10/8.png)

![](/assets/CS-231n-10/9.png)