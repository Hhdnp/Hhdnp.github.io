---
title: CS 231n Note5 笔记
date: 2024-10-21 19:06:16
tags:
    - 人工神经网络
    - 机器学习
    - 激活函数
    - 人工神经元
categories: 机器学习笔记
mathjax: true
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>
## Introduction
在之前的课程中，我们已经学习了计算得分函数，一些基础的分类器，正向传播，反向传播等等只是，接下来我们要将这些只是组合成人工神经网络。人工神经网络就是这些东西进行一些组合并加上一些非线性的激活函数所构成的，这些激活函数很重要，否则若只是单纯的一次次矩阵相乘，那么拟合出来的函数仍然只是线性函数。

## Modeling one neuron
人工神经网络的出现实际上就是收人脑内神经元运作机制的启发，下面我们来讲一讲人工神经元的结构。

### Biological motivation and connections
我们可以这样来理解人工神经网络与生物学的关系。
在人工神经网络中，神经元接受一个信号(输入$x$)，并与另一个神经元进行交互产生一个冲动的强度值($wx$)，其中另一个神经元给出的值$w$是一个可学习的值，而当这个冲动的强度值超出阈值时，突出便会释放信号传递兴奋(激活函数)。如图所示，这就是单个人工神经网络神经元的运作机理。
![](/assets/CS-231n-4/1.png)
下面是编程模拟这样一个神经元的代码：
```python

class Neuron(object):
    def forward:
        cell_body_sum = np.sum(inputs * self.weights) + self.bias
        firing_rate = 1.0 / (1.0 + np.exp(-cell_body_sum))
        return firing_rate
```
但实际上，这也只是一个粗略的模型，真实人脑的运作机理远比这复杂得多，如果有兴趣，可以去看原讲义，它在这里提供了一个相关资料的链接

### Single neuron as a linear classifier
直观地看，一个神经元必然会有偏好：它会喜欢某一类输入(会输出很大的兴奋，接近1)，也会讨厌某一类输入(几乎不产生兴奋，接近0)。
因此，我们也可以在单个神经元上构建我们先前学到的线性分类器：
**Binary Softmax classifier**
一个显而易见的想法就是在神经元上构建一个二分类Softmax分类器，他会输出一个又两种取值可能得标准概率分布$P(y_i = 1|x_i,w),P(y_i = 0|x_i,w)$，这很符合神经元输出的特点，此外，我们还可以很方便地在神经元上通过优化交叉熵损失函数(*cross-entropy loss function*)来调整合适的权重值。
**SVM classifier**
同理我们也可以在单个神经元上使用支持向量机来进行输出，此时应用的损失函数是*max-margin hinge loss function*
**Regularization interpretation**
既然说到了线性分类器，那么就不得不提到其中的正则化损失函数，类比到神经网络中，我们可以将正则化损失比作遗忘的过程，在优化正则化损失的过程中我们不断地缩小权重矩阵，使得模型更加简单并且具有更强的泛化能力

### Commonly used activation function
在神经网络的构建中，激活函数的选取十分重要，因为它是使得神经网络不是线性变换的关键，下面介绍几个常用的激活函数：
**Sigmoid**
sigmoid function:
$$\sigma(x) = \left( \frac{1}{1+e^x}\right)$$
sigmoid函数是个能很好模拟神经元行为的函数，因为他接受一个在$\mathbb{R}$内的输入并输出一个在$(0,1)$的值，这表现了神经元从完全不兴奋到完全兴奋的过程。然鹅，近些年来，sigmoid函数已经不在被频繁使用，因为sigmoid函数有以下缺点：
1. 当Sigmoid函数值很大的时候，它的图像趋于平缓，这就会导致反向传播计算出来的梯度很小，于是几乎不会有信号反向流入权重并改变它并影响在这之前的所有神经元。这种现象被称为神经元的饱和。
2. Sigmod函数输出的是一个正值，这会影响梯度下降的行为，因为如果正向传播输入的结果是全正的，那么反向传播过程中，权重的梯度就会要么全正要么全负。而这会引入一种不好的现象，名为Zig-zagging dynamics
*************
**Tanh**
这个就不多说了，它只是sigmoid函数的缩放，解决了第二个问题但是饱和问题仍然存在。
$$Tanh(x) = 2\sigma(x) - 1$$
*************
**ReLU**
ReLU function:
$$f(x) = max(x, 0)$$
ReLU函数是一个近些年来被广泛使用的激活函数，他又如下特点:
1. 它因为一些复杂的原因收敛得极快，一些研究指出，它的收敛比sigmoid函数快6倍
2. 相比于引入复杂指数运算的sigmoid函数，ReLU函数的构造相当简单，相应的实现也极其简单
3. 但是不幸的是，ReLU存在一种会导致神经元死亡率过高的问题，这是指如果正在反向传播的过程中，有一个较大的值流过时，有可能会导致任何输入都不会再激活这个神经元，于是我们称这样的神经元死亡了。这种问题我们一般通过合理地设置学习率来缓解。
*************
**Leaky ReLU**
这是ReLU的升级版，是一种用于解决ReLU死亡率过高的一种尝试：
$$f(x) = 
\begin{cases}
    \alpha x & \text{if } x < 0 \\
    x & \text{if } x \ge 0
\end{cases}$$
其中$\alpha$是一个很小的参数，就是为了防止在输出值小于0是，反向传播不会提供任何的梯度信息。
**************
**Maxout**
这是ReLU的广义版本：
$$f(x) = max(w_1^Tx_1+b_1,w_2Tx_2 + b_2)$$
可以看到不管是哪种ReLU都是这个函数的特例，这个函数是一个拥有所有有点的激活函数，唯一的缺点就是由于有两个线性值要计算，会使得参数量翻倍。

## Neural Network architectures
### Layer-wise organization
神经网络是一张无环图，它由许多由若干神经元构成的层组成，可以分为输入层，隐藏层(执行中间运算过程的层)，输出层，其中在隐藏层中，若相邻两层拥有相同数量的神经元并且两两相连(fully pairwise connected)。此外还有些要注意的地方
**Naming conventions**
1. 在说N层神经网络的时候，我们通常不把输入层包括在内
2. 神经网络有一些别名，比如人工神经网络("Artificial Neural Networks(ANN)")，多层感知机("Multi-Layer Perceptrons(MLP)")。
**Output Layer**
输出层通常是没有激活函数的，它只计算每一类对应的的分值并作出预测
**Sizing neural networks**
我们通常用神经元的数量以及参数量来衡量一个神经网络的复杂程度。
神经元数量是显而易见的，那么参数量怎么计算呢？
下面是一个计算神经网络参数量的例子：
![](/assets/CS-231n-4/2.png)
我们注意到在这个神经网络中有三次传递过程，于是需要三个矩阵，分别需要$3\times 4, 4\times 4 4\times 1$的矩阵，同时每一层还需要一个偏置向量，大小分别为$4\times 1, 4\times 1, 1\times 1$，将它们全部相加，就可以得到总共的参数量41。

## Example feedforward computation
Talk is cheap, the code is below:
```python

def feedforward(x : np.array()):
    f = lambda x : max(0, x)
    x = np.random.randn(3, 1)
    W1 = np.random.randn(4, 3)
    W2 = np.random.randn(4, 4)
    W3 = np.random.randn(4, 1)
    h1 = f(np.dot(W1, x) + b1)
    h2 = f(np.dot(W2, h1) + b2)
    out = np.dot(W3, h2) + b3
    return out
```

## Representation power
正如CS188讲义中所说的一样，有一层非线性层组成的神经网络的表示力就是无穷的，即它可以拟合任意实值函数。
但是在实际应用中，我们显然不可能只采用一层隐藏层，实际上，在一般的神经网络中，3层的效果会远好于2层或是1层，但继续增加层数产生的效果就不明显。
与之相对的，卷积神经网络(CNN)就是一种对深度很敏感的神经网络，在学习过程中一般会采用十层以上。

## Setting number of layers and their sizes
设置神经网络的层数以及每层的大小也是我们需要合适选择的超参数之一，下面这张图展示了不同大小的神经网络之间的效果差异：
![](/assets/CS-231n-4/3.png)
图中可以发现，太复杂的神经网络会出现过拟合的现象，即将数据中的噪声也纳入了考虑的范围之内，在这种情况下，反而更小的模型具有更好的泛化能力。
然而这并不意味着在应用过程中我们应当应用更小的模型。
更小的模型通常在损失函数上只有几个局部最小值，而且这些局部最小值与全局最小值相比是偏大的，因而通过梯度下降得到的所有解都是差不多的，然而大网络则具有更多的局部最小值，这通常意味着，我们可以通过随机化选择初始权重，逐渐达到更优的解。
于是为了防止过拟合的发生，我们通常并不采用减小模型的方式，我们可以转而通过控制正则化损失函数来避免过拟合的发生。