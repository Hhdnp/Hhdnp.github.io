---
title: CS 231n Note4 笔记
date: 2024-10-20 13:07:59
tags:
    - 梯度下降算法
    - 链式法则
    - 反向传播
    - 机器学习
categories: 机器学习笔记
mathjax: true
---
## Introduction
本节精讲的是反向传播算法(**backpropagation**)，这种算法用到了计算梯度中的链式法则(**chain rule**)
我们的目标就是，在给定损失函数$L$，数据$X$，权重矩阵和偏置矩阵$W,b$的情况下，高效地求出$\frac{\partial L}{\partial W}, \frac{\partial L}{\partial b}$。

## Simple expression and interpretation of the gradient
这节主要讲的是对梯度的一些解释，作为学过微积分II的大学生，就把这部分跳过了吧，唯一需要注意的就是，
对于最大值函数$f = max(x, y)$，我们将其作为一个分段函数进行求偏导数，即当$x>y$时$\frac{\partial f}{\partial x} = 1, \frac{\partial f}{\partial y} = 0$。
同时这又带来了一个问题，例如当$x=2,y=5$时，当且仅当x的变化幅度足够大(>3)时，这变化才能在梯度中体现出来，然而在计算梯度的时候，我们只会计算一个微小的邻域，这就导致了梯度在这种情况下的自变量的变化并不敏感。

## Compound expressions with chain rule
在上一节中，我们只介绍了一些简单函数的梯度，那么当函数变得复杂时我们该怎么做呢。
让我们先从一个如复杂的表达式开始$f(x) = (x + y)z$。
这个表达式实际上仍然足够简单，但我们可以通过链式法则将其分解为更简单的表达式从而求出梯度：
$f(x) = qz, q = x + y$，这样我们就将其分解为了最简单的表达式，然后应用链式法则：
$\frac{\partial f}{\partial z} = q = x+y, \frac{\partial f}{\partial x} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial x} = z\times 1$
这种链式法则的求梯度可以用一张类似于神经网络的图简单地表示
![](/assets/CS-231n-3/1.png)
我们先通过圆圈节点(门)正向传播计算出输出的值，即每条边上的值。之后通过反向传播，从输出的地方反向输入一个值1，再在每个门节点的地方计算出其输入对应的梯度(局部的梯度)，最终反馈到一开始的输入值。
此外需要注意的是，尽管我们一般只需要用到权重的梯度值，数据的梯度值也可以不费吹灰之力地一并求出，同时，数据的梯度值在进行可视化的时候也是有一定作用的。

## Intuitive understading of backpropagation
这种方法的步骤与原理已经在上面解释过了。
需要补充的只有，这种方法关注的就是求梯度值的局部过程，因为每个门仅需关注少数几个输入，不管它是一开始的输入值，还是中间结果，只需一步一步反向传播回去即可。

## Modularity: Sigmoid Function
好，接下来让我们真正开始接触一些较为复杂的函数吗，比如下面这个Sigmoid函数：
{% raw %}
$$f(w,x) = \frac{1}{e^{-(w_0x_0+w_1x_1+w_2)}}$$
{% endraw %}
在计算这个函数梯度值时，我们需要用到以下几个基础运算：
{% raw %}
$$\begin{array}{llll}f(x)=\frac{1}{x}  &\rightarrow & \frac{d f}{d x}=-1 / x^{2} \\ f_{c}(x)=c+x  & \rightarrow & \frac{d f}{d x}=1 \\ f(x)=e^{x}  &\rightarrow & \frac{d f}{d x}=e^{x} \\ f_{a}(x)=a x  & \rightarrow & \frac{d f}{d x}=a\end{array}$$
{% endraw %}
并且我们可以画出与上面类似的“门电路”图：
![](/assets/CS-231n-3/2.png)
这很直观，但是实际上我们使用的门可能有些太过基础了，比如说，最后计算sigmoid函数的四个门其实可以直接简化为一个sigmoid门而不增加太大计算量。这是因为sigmoid函数的梯度本身就可被简易地求得：
{% raw %}
$$\frac{\partial \frac{1}{1+e^{-x}} =\sigma(x)}{\partial x} = (1-\sigma(x))\sigma(x)$$
{% endraw %}
如此，我们就可以编写这样的简易函数来计算梯度：

```python
w = [w0, w1, w2]
x = [x0, x1]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + np.exp(-dot))

# backpropagation
ddot = (1 - f) * f
dx = [w[i] * ddot for i in range(2)]
dw = [x[i] * ddot for i in range(2)] + [1.0 * ddot]
```
需要注意的是，正向传播计算的中间结果也是需要被保存的，这是因为这些中间结果也需要被用来计算反向传播的梯度。

## Backprop in practice: Staged computation
下面是另一个更复杂的例子，在这一节中，没有新的知识，需要注意的就是如果一个输入再正向传播的过程中被多个门视作输入，那么在计算反向传播的时候，这个输入的梯度值需要将以它为输入的所有门返回的梯度值相加。
例子：
{% raw %}
$$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x + y)^2}$$
{% endraw %}
```python
x = ...
y = ...

def sigma(x);
    return 1.0 / (1 + np.exp(-x))
# forward pass
sigy = sigma(y)
sigx = sigma(x)
nume = x + sigy
xpy = x + y
xpysqr = spy ** 2
deno = sigx + xpysqr
invdeno = 1.0 / deno
f = nume * invdeno

#backpropagation
dnume = invdeno
dinvdeno = nume
ddeno = (-1.0 / (deno) ** 2) * dinvdeno
dsigx = ddeno
dxpysqr = ddeno
dxpy = (2 * xpy) * dxpysqr
dx = dxpy
dy = dxpy
dx += ((1 - sigx) * sigx) * dsigx
dy += ((1 - sigy) * sigy) * dsigy
dx += dnume
dy += dnume
```

## Patterns in backward flow
这节主要介绍了几个基本运算门的行为(正向传播和反向传播时)，这是浅显易懂的，唯一需要记录的一点就是，笔记中提到：
乘法门会有一项反直觉的行为，即当其两个输入相差很大时，反而会给极小的那个输入反馈一个极大的值，这可能会导致权重变化过大，难以收敛，因此在数据预处理时我们需要处理好数据以及权重初始值的相对关系。

## Gradients for Vectorized operations
这节主要介绍了矩阵运算的梯度计算，这是非常不直观且难以理解的(所以快去学矩阵求导啊喂)。
文中介绍了一个例子:
```python
# forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)

# back
dD = ... # suppose this is the gradients of matrix D
dW = dD.dot(X.T) # dD: (5, 3) X.T: (3, 10)
dX = W.T.dot(dD) # W.T: (10, 5) dD: (5, 3)
```
同时笔记中也说到，这部分确实是十分难以理解的，因此我们可以不用一开始就尝试直接去计算它，我们可以先从较小的例子开始计算并进行理解和推广，这样可以使得这部分计算变简单一些。
讲义里给出了一个有关矩阵求导的文档链接，有时间记得去学一下哦猫猫。