---
title: CS 231n Lec6 笔记
date: 2024-11-8 18:41
tags:
    - 卷积神经网络
    - CNN
    - 归一化
    - ResNet
    - Transfer Learning 
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

## 归一化
在神经网络中我们常常遇到目标函数难以优化的问题，这种问题一般是由以下两个原因导致的：
1. 输入数据不以0为中心（需要很大的偏置向量）
2. 输入中不同元素间尺度并不统一（权重矩阵中元素之间的大小差异会很大）

于是我们需要引入归一化来改善这些问题。
![](/assets/CS-231n-8/1.png)
### 批归一化 (Batch Normalization)
批归一化是对$C$维度数据进行归一化的操作，直观来看，我们沿着$C$轴，将数据切位若干大小为$H\times W\times N$的数据，并对它们进行零均值，单位方差的归一化：
$$\begin{aligned}
& \mu_j = \frac{1}{NHW}\sum_{i,k,l\in\lbrace{N,H,W\rbrace}}x_{i,j,k,l}\\
& \sigma_j^2 = \frac{1}{NHW}\sum_{i,k,l\in\lbrace{N,H,W\rbrace}}(x_{i,j,k,l - \mu_j})^2\\
& \hat{x_{i,k,j,l}} = \frac{x_{i,j,k,l} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}
\end{aligned}$$
其中参数$\epsilon$是为了防止除以0设置的一个很小的参数。
然而这种归一化方式又出现了另一问题，即为零均值，单位方差的约束太过严格，于是在实际应用过程中，我们对每一组引入两个可学习的参数$\gamma_j, \beta_j$，在进行单位归一化之后进行平移：$y_{i,j,k,l} = \gamma_j\hat{x_{i,j,k,l}} + \beta_j$。
*注意：* 若令$\gamma = \sigma, \beta = \mu$则会取消归一化操作。

另外，归一化是一个训练时和推理时行为不相同的操作，具体来说，在训练时，我们会维护一个`runningtime_avg, runningtime_var`以供推理时进行归一化使用而非使用测试数据计算得出的平均值以及方差。

在训练过程中我们也通常采用指数衰减的方法来更新这两个参数：
```python
# update
runningtime_avg = alpha * runningtime_avg + (1 - alpha) * current_avg
runningtime_var = alpha * runningtime_var + (1 - alpha) * current_var
```

在神经网络中采用归一化的优势是很明显的：
1. 可以使深度网络更易训练
2. 可以接受更高的学习率，收敛速度加快
3. 归一化实际上可以被视作一种正则化，因为它会拉近样本间的分布，使模型难以过拟合
然而由于归一化在测试和训练时的行为不一致，易于导致bug的出现。

另外还有诸如层归一化(Layer Normalization)，例归一化(Instance Normalization)，组归一化(Group Normalization)等归一化方式，大体与批归一化相同，只是分组的维度不同。

## Case Study
由于AlexNet与VGGNet都只是在CNN原本的层类别中改进架构与顺序，因此只简单介绍，重点介绍引入残差连接方式的ResNet。
### AlexNet
![](/assets/CS-231n-8/2.png)
### VGGNet
![](/assets/CS-231n-8/3.png)
## ResNet
可以看到，AlexNet到VGGNet基本只是在深度上进行了增加，但仍然获得了不小的性能提升(错误率$16.4\%\rightarrow 6.3\%$)，那么就会有一个自然而然的想法，将卷积神经网络的层数继续增加，能不能继续获得更加优秀的性能呢？
答案自然是肯定的，然而在实际训练的过程中，继续增加深度却会导致模型性能的下降，这是为什么呢。
这并不由过拟合导致，事实上，随着深度的不断增加，模型的表示能力飞速提升，于是我们可以作出猜测，是因为答案空间的增加导致模型难以优化从而使得模型性能下降。
这又引申出了一个新的问题，如何至少使得加深后的模型与之前模型表现一致？
一个很朴素的答案便是沿用之前的层，并且在新增的层中采用恒等映射的方式(identity mapping)。
以这个很朴素的想法为依据做延伸，一些计算机科学家想出了一种天才的解决方案：
在卷积层之后添加一个残差连接，从而让每一卷积块并不直接拟合目标函数而是拟合目标函数与实际输出的残差。
```
#Plain Solution
x->conv->relu->conv->H(x)
#Residual connection
x-+->conv->relu->conv->F(x)-+->H(x)=F(x)+x
  |                         |
  +-------------------------+
```
我们可以显然地发现，该层不再需要直接拟合目标函数，而是只需要拟合目标函数与输入的残差。
于是拟合目标就变为了捕捉期望输出与输入之间的残差并拟合。
这样的连接方式有很多的优势：
1. 缓解了深层网络中梯度消失的问题，因为每一个卷积块前后都有直接相连的路径。
2. 拟合残差显然比直接学习复杂的映射更加简单。
3. 在卷积层学习不充分时，这种残差连接方式也能有效保留原始信息。

### Full ResNet Architecture
![](/assets/CS-231n-8/4.png)
+ 以残差块的方式组建整个网络
+ 周期性的加倍卷积核的数量并且降采样一倍
+ 在最开始添加额外的卷积层（用于初步特征提取）
+ 无需使用额外的全连接层，仅需一个FC-1000输出分类得分。

*注意：* 对于很深的网络，注意要使用bottleneck层用于增加计算效率，如图：
![](/assets/CS-231n-8/5.png)
### Training ResNet in Practice
PPT在这里给出了一些训练时的超参数建议，这里就不再赘述，想要请看PPT。

从ResNet开始，卷积神经网络在ImageNet中的图片分类准确率($96.4\%$)就超过了人类($94.9\%$)。
于是部分研究方向转为了：
1. 模型训练以及推理的效率(MobileNet, ShuffleNet)
2. 自动化设计神经网络的架构

## Transfer Learning
当我们想要训练一个CNN的时候，我们常常会为自己没有巨量的数据以供训练而烦恼，于是这个时候我们可以采用Transfer Learning的方式。
即只在最后几层全连接层中进行训练而沿用预训练模型中已经训练好的卷积层部分。
![](/assets/CS-231n-8/6.png)