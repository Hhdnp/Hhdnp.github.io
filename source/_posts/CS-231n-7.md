---
title: CS 231n Note9 笔记
date: 2024-11-1 20:59
tags:
    - 卷积神经网络
    - CNN
    - 膨胀卷积
    - 池化
    - ResNet
    - Inception
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

由于Note8只是一些实践性的示例，于是跳过，接下来正式进入激动人心的CNN章节
## Architecture Overview
在学习CNN之前，需要说明的一点是，不同于之前我们学习的神经网络，CNN假设输入的数据全部为图像，从而以这个假设为依据，构建了一些很奇妙的特性，使得整个网络更加准确和高效。

回忆一般的神经网络，它们以一个数据向量作为输入，并且通过一系列隐藏层转换这个向量，最终得到一个得分向量从而将其归类。其中每一层由若干个神经元构成，并且层与层之间采用全连接的方式，层内的神经元互相独立，不连接。然而这种结构对于图像数据表现并不算很优秀。

于是我们把神经元“3D”化，即使其更符合输入数据原有尺寸的特点。在CNN中，每一层的神经元按三个维度排列（宽度$W$,高度$H$,深度$D$）。并且每一个神经元只按特定的规则与输入数据的一部分相连，不采用全连接的方式。直观来讲，CNN中的层将一个3D形状的输入通过一些可微函数转换成一个3D形状的输出。
![](/assets/CS-231n-7/1.png)
## ConvNet Layers
一个卷积神经网络（CNN）通常由三种主要的层结构构成：卷积层(Convolutional Layer)，池化层(Pooling Layer)，全连接层(Fully-Connected Layer)。例如，一个CNN可能结构如图：($INPUT\rightarrow CONV\rightarrow RELU\rightarrow POOL\rightarrow FC$)。

其中每层的功能简述如下：
+ 输入层：接收一个三维的输入矩阵（长$\times$宽$\times$三色数值）。

+ 卷积层： 每个神经元会与输入中的一部分相连并计算点乘，，最终一个卷积层会输出一个长高与输入一致或略少，深度为$filter$数量的三维矩阵。

+ RELU层：和全连接层类似，卷积层后的RELU也作为激活层存在，它对卷积层的输出执行$\max(0,x)$函数实现非线性。

+ 池化层：池化层的主要功能就是通过一些固定的函数为前面的输出执行降采样，例如接收一个$32\times 32 \times 12$的输入，输出$16\times 16\times 12$。

+ 全连接层：和之前神经网络中的全连接层类似，接受前面处理过的数据并计算出得分向量从而进行分类。

其中需要注意的是，一般来说，只有卷积层和全连接层中含有参数需要训练，RELU层和池化层仅为实现固定函数，其中不含有参数。
### Convolutional Layer

卷积层是CNN的关键。

一个卷积层是由若干个可学习的滤波器(filter)构成的。每个滤波器很小(远小于接收数据的大小)。

例如，一个典型的滤波器大小就是$5\times 5\times 3$(宽高各5像素，3是每个像素中的RGB值)，但它通过在宽度和高度上的滑动，对整个图像进行计算，每次只计算在输入数据上覆盖到的点乘，最终生成一张二维的图片，宽高略小于或等于原图大小，反映出滤波器在各个位置“看到的”特征。

在一层中会含有若干个这样的滤波器（比如典型的12个），然后将每个滤波器生成的二维数据按深度叠在一起，最终得到一个深度等于滤波器数量的三维输出数据。

抽象地讲，这种做法的好处在于，滤波器可以注意到一些全连接层无法注意到的特征，比如若输入图像的一部分与滤波器的方向高度一致，那么在输出图像上，这部分对应的点就会呈现出极高的数值，表示这种特征被滤波器观察到了。

**Local Connectivity**

在一些巨大的图像中采用全连接层是不现实的，于是我们让每个神经元只连接输入的一部分来生成输出，于是每个输出的数据不在由输入的全部决定，而是只由输入的一部分决定。我们将这种特性定量描述，即每一个输出的数据对应了多少的输入数据（与它们有关）称作这一层神经元的感受野($Receptive~Field$)。

对于只有一层的卷积网络来说，感受野就是滤波器长宽的大小，比如一层由$5\times 5\times 3$的滤波器构成的层感受野就是$5\times 5$，至于为什么与深度无关，因为深度是一个不可调的参数，它只能与输入的深度相同。

而对于多个卷积层来说就不是这样，比如经过三层的$3\times 3$神经元，那个输出中一个像素点就与输入中$7\times 7$的区域有关，感受野就是$7\times 7$。
对于感受野的大小，有一个计算公式，但是之后讲到相关参数的时候再提。

**Spatial Arrangement**

在上面我们已经看到，一个滤波器的各项参数会很大的影响输出的尺寸，于是我们接下来介绍控制输出大小的三个超参数。
1. *深度(Depth):* 即滤波器的数量，我们上文提到，每个滤波器会生成一张二维的图片，于是我们可以直观地看到输出数据的深度仅与滤波器的数量有关。

2. *步长(Stride):* 我们提到，滤波器通过在图像的宽度和高度上“滑动”来生成一张二维图片，于是很自然的滑动的步长就会影响输出图片的大小。

3. *零填充(Zero-padding):* 只通过之前的描述可能会有些疑惑，即当滤波器滑动到图片的边缘时，它便无法继续滑动，从而应当会使得输出的数据在宽和高上都略小于输入的数据。这时，我们就可以通过在边缘填充0数据来使滤波器继续滑动从而保持输入输出数据在大小上一致。

*符号规约：* 一般来说我们使用$W$表示输入尺寸（注意这里表示的是宽或高的尺寸，一般来说我们认为它们是相等的）。用$F$表示神经元感受野的大小（即尺寸）。用$S$表示步长，用$P$表示在边缘上零填充的宽度。

于是我们可以得到一个输出尺寸$W'$与上述变量之间的关系：
$$W' = \frac{W-F+2P}{S} + 1$$
这里面有两个注意点：
一个是为了确保输出尺寸与输入尺寸相等，我们可以简单地计算出我们需要的零填充数目为$\frac{F-1}{2}$，这是$S=1$时的状态，当$S$更大时保持输出尺寸不变就没有意义了，因为这会输出很多0。

第二个就是需要注意公式中分式必须能被整除，这是因为若不能被整除，则表示在这个步长下，神经元的连接就是不对称的（比如左边的局部可以从边缘开始，但最右端由于步长的限制就不行了）。

**Parameter Sharing**：虽然这部分很显然，但还是记一下：若我们像全连接层那样为每一个局部（即每一个输出像素点）都分配一个神经元，那么参数量就会变得极其巨大（$|param| = W'^2D$，于是我们在大部分情况下，都会让同一层的神经元共享参数（简单来说，就是在滑动的过程中不改变参数）。

另外，如果我们对每一层都共享参数，那么卷积层所做的操作，就可以被看作是神经元权重和输入数据的卷积，这也是名字的由来。

再另外，还是有部分情况我们不能使用参数共享，比如输入的图像具有特殊的结构（如中心化），我们可能需要在正面和侧面提取不同的特征（人脸的眼睛头发等）。这时，就可以不在全部参数共享，而是将图像分为不同部分，相同部分共享参数。

实际应用中，一个常见的设置就是$F=3,S=1,P=1$。下面是一个可视化演示：
![](/assets/CS-231n-7/2.png)
***************************
**Implementation as Matrix Multiplication**
在实际应用中，我们也会采用高效地向量化操作来加速卷积层的前向传播。

我们一般会将输入数据的局部（每次计算的）展开为列向量。比如输入$[277\times 227\times 3]$的图像，我们使用大小为$[11\times 11\times 3]$的滤波器，那么我们就会将输入矩阵转换为一系列长度为$11\times 11\times 3=363$的列向量。假设滤波器的步长为4，不使用零填充，那么输出大小$W' = 55$，于是我们知道输出的2维图片像素量为$55\times 55=3025$，因此我们应当将输入矩阵展开为$[363\times 3025]$的矩阵$X_{col}$，我们将这个操作成为`im2col`。

另外为了适配尺寸，我们还需要将滤波器转化为一个矩阵，假设我们有$96$个滤波器，那么我们需要将每个滤波器展开成行向量，最终得到一个$[96\times 363]$的矩阵$W_{row}$。

于是现在我们可以通过`np.dot(W_row, X_col)`方便地计算出输出矩阵，它的大小为$[96\times 3025]$，最后我们还需要将其`reshape`成本应有的形状$[55\times 55\times 96]$，于是就完成了一次前向传播。

这种方法优缺点都很明显，优点就是可以通过向量操作大幅加速计算过程，缺点就是由于卷积操作实际上在滑动的过程中会经历很多重复的像素点，`im2col`操作会极大地占用内存空间。但是，`im2col`操作还可以用于池化操作上，等下会提到。

**Backpropagation**
讲义中略过了，但我觉得这是挺重要的，于是我们来用一个小的例子演示一下。
```python
# backpropagation
Input = np.random.randn(32, 32)
filt = np.random.randn(3,3) #步长为1，采用1格零填充
X_col = np.pad(Input, pad_width=1, mode='constant', constant_values=0)
Out = .........

dfilt = np.zeros((3, 3))
dfilt[0, 0] = Input[0:30, 0:30] * out[1:31, 1:31]
dfilt[0, 1] = Input[0:30, 0:31] * out[1:31, 0:31]
dfilt[0, 2] = Input[0:30, 1:31] * out[1:31, 0:30]
......
```

**1×1 convolution**
这是经常被误解的一点，认为当filter大小为$1\times 1$时就是相当于矩阵数乘，但实际并非如此，需要注意到卷积操作有深度，因此如此的卷积实际上是对每个点进行对深度的点乘。

**Dilated convolutions**
膨胀卷积。在正常的卷积中，我们一般只根据滤波器对相邻的局部区域进行点乘，但是膨胀卷积提出了一种新的想法。即对不相邻的区域进行点乘，在膨胀卷积中，我们引入一个超参数$dilation$膨胀率，即局部区域中点之间的距离。
举个例子，假设我们有一个$3\times 3$的滤波器，若是在$dilation=1$的情况下，即正常卷积，那么计算为：
```python
# dilation = 1
out[x,y] = 0
for i  in range(3):
	for j in range(3):
		out[x,y] += fil[i,j] * inp[x+i,y+j]
```
而当$dilation=2$时，计算方式如下：
```python
# dilation = 2
out[x,y] = 0
for i in range(3):
	for j in range(3):
		out[x,y] = fil[i,j] * inp[x+i*2,y+j*2]
```
同时我们还要补充一点，就是膨胀卷积在滤波器大小相同的情况下，会极大的增加有效感受野。

在此我们补充多层卷积后一个输出像素点的感受野公式：
首先是标准卷积：
$$r_n = r_{n-1} + \left(k-1\right)\prod_{i=1}^{n-1}s_i$$
其中$r_i$是第$i$层后的感受野大小，$k$是滤波器大小，$s_i$是第$i$层的步长。
然后是膨胀卷积：
$$r_n = r_{n-1} + \left(k'-1\right)\prod_{i=1}^{n-1}s_i$$
这个公式形式上与标准卷积类似，但是需要注意，$k'= k + (k-1)(dilation-1)$表示膨胀卷积核的感受野大小。

### Pooling Layer

池化层是为了减少参数量以及防止过拟合所设置的，对卷积后的数据进行降采样操作的层。
它独立地处理每个深度下的2维数据矩阵，一般采用$\max$函数来缩小数据大小。最常见的形式是，类似于卷积层中的描述，采用$2\times 2$的滤波器和2的步长，对每次滑动应用一次对四个变量的$\max$函数，最后得到一个$\frac{1}{4}$大小的数据，丢弃了1$75\%$的激活数据。

**Backpropagation**
回忆之前讲过的对$\max$操作的求梯度运算，我们可以知道$\max$操作只对最大的值传递梯度，于是我们可以再前向传播的过程中记录下最大值的索引，以便在反向传播时高效地运算梯度。

**Getting rid of pooling**
这里提出了一种畅想，在之后的CNN中，我们也许可以不再采用池化层来降采样，转而使用重复的卷积层来减小数据的大小（通过加大步长等等）。实际上，这种操作在训练一些生成式模型时很有用，比如VAEs,GANs等等，它们使用了很少或者没使用池化层。

### Normalization Layer

这是一个神奇的层，它对接收到的数据进行归一化操作，用以模拟大脑的抑制作用，然而这种层已经逐渐不被使用，因为实践表明，它几乎不会提升模型的性能。
### Fully-Connected Layer

老熟人全连接层又来了，其实这层与前面讲过的全连接层并无差异，它只是为了将卷积好的数据转化为得分向量
### Converting Fully-Connected Layers to Convolutional Layers

实际上卷积层和全连接层是可以互相转换的。

卷积层向全连接层的转换比较复杂，我们将整个矩阵作为一个输入，并且使用对应卷积核区域的局部矩阵（即其余部分全为0）与它点乘，最后合成一个输出图像（这部分没看懂，但也不重要，大概意思就是这个）。

全连接层向卷积层的转换就较为简单，我们只需设置$K$（$K$为全连接层大小）个与输入大小相同的卷积核，便会输出一个$1\times 1\times K$的矩阵，与全连接层效果相同。

另外，将全连接层转换为卷积层也有别的好处，比如在输出大小为$224\times 224\times 3$时，我们通过一系列卷积层和五个池化层输出一个大小为$7 \times 7\times 512$的矩阵。此时我们将全连接层转换为一个$K=4096$的等价卷积层，我们同样可以得到一个$1\times 1\times 4096$的输出，并最终得到一个$1\times 1\times C$的得分向量。

并且如果我们加大输入尺寸，比如我们将输入矩阵大小提升为$384\times 384\times 3$。那么通过同上的卷积和池化，我们可以得到一个$12\times 12\times 512$的矩阵，此时将全连接层转化为卷积层的优势就显现了出来，我们可以继续对这个矩阵进行前向传播（全连接层不行），并最终得到一个$6\times 6\times C$的得分矩阵。

因此将全连接层转化为卷积层之后，我们在提升图像大小后不必再更改模型或对图像的不同部分总共计算36次（上例），我们可以直接得到这36个局部图像的得分，并且最后可以对每一个深度去平均作为最终的得分向量。

讲义中还提到一种在不更改模型减小缩小比例（上例为32）大小的方法，具体就不赘述了，核心思想就是错位应用两次模型。
## ConvNet Architectures

一般来说，在一个CNN中，有三（四）种层：卷积层(CONV)，池化层(POOL)，全连接层(FC)（激活层(RELU)）。接下来我们将会学习CNN中这几种层的排布。
### Layers Patterns

简而言之，一般化的CNN结构如下
$$INPUT\rightarrow[[CONV\rightarrow RELU]*N\rightarrow POOL(?)]*M\rightarrow[FC\rightarrow RELU]*K\rightarrow FC$$
通常来说，在构建卷积神经网络时，我们会取值$0\le N \le 3,M\ge 0,0\le K<3$

讲义中还有一些使用如此结构的解释，比如说，我们在池化层之前会循环使用几次卷积，激活层的组合，这通常是为了在降采样之前更好地提取特征。

*Prefer aa stack of  small filters*
我们可能会感到纠结，在感受野相同的情况下，究竟是选择多个小滤波器，还是选择一个大滤波器。其实这个问题是显然的。
首先，在感受野相同的情况下，例如一层$7\times 7$和三层$3\times 3$，他们都提供$7\times 7$ 的感受野，但是一层的卷积实际上仍然是线性的，然而三层的卷积就使得整个函数不再线性，从而具有更强的表达能力(more expressive)。
其次多层小滤波器的参数量往往更小，仍然以上面为例，三层小滤波器的参数量为$3\times D\times (3\times 3\times D) = 27D^2$，而一层大滤波器有着$D\times (7\times 7\times D) = 49D^2$参数，于是我们更应当选择多层小滤波器。

**Recent Departures**
坠新的研究表明，如上所述的线性排列的层结构事实上表现并非最佳。而采用残差连接方式的$ResNet$或是引入$Inception$层的$Inception-architecture$表现更好。

**In practice**
这部分没看懂，翻译过来就是不管如何，在绝大部分情况下，实践中我们只需要参考在ImageNet中表现最好的预训练模型并拿过来微调即可。
### Layer Sizing Patterns

这部分主要讲了一些注意事项：
1. 输入层接收的图像的宽高应当能被2多次整除以便于1池化
2. 卷积层应当使用较小的卷积核(如$3\times 3,5\times 5$)，并使用1为步长并可以使用$P=\frac{F-1}{2}$来维持数据大小不变。另外，若不得不使用大型卷积核，也应只在第一层中使用。
3. 在池化层中，我们一般采用$F=2, S=2$的最大值池化，另外一种不常见的池化方式是采用$F=3,S=2$的参数，这种池化方式需要使用0填充。此外，池化核大小不应超过3，因为这会导致损失过多的信息，使模型性能极大幅度下降。

以上方案是理想情况，我们总会使用零填充保持输入输出大小不变，这是很令人省心的。然而在实际情况下，我们还会遇到输出大小减少的情况，此时，我们就需要监视跟踪数据的大小变化防止出现意料之外的情况。

*为什么使用1为步幅*

小步幅在实际情况中效果较好，我们应当将降采样的任务交给池化层。

*为什么使用零填充*

保持边缘信息不被一次次缩小的输出丢失。

*Compromising based on memory constriants*
由于在神经网络的训练中，我们需要保存正向传播的中间值以便于计算反向传播的梯度。因此卷积层的中间结果可能会极其消耗内存。所以实际上，我们会在第一层对内存消耗进行妥协，采用更大的滤波器以及步长。

### Case Studies

一堆论文，是一些很高效的改进后CNN模型。
可以日后学习，讲义中给出了链接。
### Computational Considerations
这部分主要讲了消耗内存（显存）的~~三巨头~~三大来源：
1. 激活值以及其梯度的中间计算结果，这是需要用在反向传播中的。
2. 参数矩阵，由于我们可能还应用了比如RMSProp,Adam等方法更新参数，这部分占用的内存通常需要翻三倍。
3. 杂项内存，图像，预处理后图像等等。