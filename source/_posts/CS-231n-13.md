---
title: CS 231n Lec11 笔记
date: 2024-11-28 17:09
tags:
    - 模型可视化
    - Guided BackProp
    - Gradient Ascent
    - Adversarial Perturbations
    - Style Transfer
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

## Visualizing and Understanding: Challenges
随着大模型的入场，神经网络中的参数量迅速增长，但这也为给神经网络提供可视化和解释带来了困难，这是由于过多的参数，复杂的架构和转换，并不简洁的特征空间等等等等。
于是我们在本节中需要了解的内容便是，如何为复杂的神经网络提供可视化解释：神经网络的计算中到底发生了什么。

## Visualizing what models have learned
回忆我们在线性分类器中看到的，线性分类器所学习到的图像的特征是停留在像素层面的，类似于对所有的训练图像进行"杂糅"：
![](/assets/CS-231n-13/Pasted image 20241127201132.png)
这种学习方式存在泛化能力差等问题，那么在CNN中，卷积核可以学习到什么呢。

### Visualizing filters
![](/assets/CS-231n-13/Pasted image 20241127201249.png)
可以看到，卷积核学习到的特征更为"抽象"，由于我们一般在同一层中会运用多个不同的卷积核，每个卷积核都可以被视作是对一种特征的提取，可以提取更多更广泛的特征从而有更强的泛化能力。
上图所示的是第一层卷积的卷积核，因为每个卷积核都有三个通道，可以方便地通过RGB的方式将三个通道合成一张图片。同时我们也可以可视化更深层中的卷积核，然而因为输入通道数过大，我们不能像这样简单地合成彩色图片，因而不是很直观：
![](/assets/CS-231n-13/Pasted image 20241127201606.png)
### Visualizing final layer features

在CNN最后的全链接层中，经过卷积的特征图片都被映射成一个较大维度的特征向量，因此我们可以在特征向量的空间中，利用最近邻的方法来直观理解。
回忆在像素空间的最近邻算法，它在大部分情况下都不具有较高的准确率，因为有些完全不同的图片只是看上去较为类似，但是在特征提取之后的特征空间中最近邻就会有很好的表现：
![](/assets/CS-231n-13/Pasted image 20241127202114.png)
这样看可能不是很直观，因为高维的特征空间对可视化来说仍然是抽象的，我们可以通过一些方式，将特征空间降到2维，从而很好地可视化：
![](/assets/CS-231n-13/Pasted image 20241127202250.png)
可以看到，在经过CNN特征提取之后，特征空间中的同类数据点都是很相近的。
关于特征降维，我们可以使用简单的方法，如主成分分析(PCA)，也可以使用较为复杂的方法(t-SNE)，关于这种方法，可能在补充中会介绍。

### Visualizing activations
对激活值的图进行可视化也是一种常见的方法，这是由于只有和卷积核很相似的图像区域才能输出较高的激活值，这会在激活值图中呈现出一个"亮点"。如下图，是一个拥有128个卷积核的卷积层输出的激活值图：
![](/assets/CS-231n-13/Pasted image 20241127202643.png)
通过可视化，我们可以找到对最终输出得分贡献较大的图像区域，从而找出具有决定性作用的图像区域（可以用于图像分割？）：
![](/assets/CS-231n-13/Pasted image 20241127202804.png)

## Understanding input pixels
### Identifying important pixels
首先，我们可以讨论一下哪些图片较为重要，这里的重要与否肯定是针对某一个卷积核而言的，因为重要程度的量化指标是通过比较某一特征的提取情况（即某一卷积核的激活情况）。
一种常见的做法便是在一个神经网络中挑选某一层的某一个卷积核，在网络中前向传播很多图片，并记录这些图片在这一卷积核输出的激活值，最后按激活值大小排序，便可以看出哪些图片可以最大地激活该卷积核（即具有某项特征）：
![](/assets/CS-231n-13/Pasted image 20241127203615.png)

对于判断图片的哪些像素较为重要，有一个很自然的想法便是，我们可以将一张图片的某一部分覆盖，观察CNN输出的置信度变化，从而可以间接判断图片的哪些部分较为重要：
![](/assets/CS-231n-13/Pasted image 20241127203850.png)
从而可以获得图片像素重要程度的热力图：
![](/assets/CS-231n-13/Pasted image 20241127204458.png)
### Saliency via backprop
既然我们可以通过记录正向传播中的激活值，置信度变化等可视化，那么对于反向传播，是否也有类似的方法？显然是有的：
由于方向传播的梯度值正相关于该像素值对于最终输出的贡献，因此我们也可以从输入数据传播到的梯度值可视化图片的哪些区域较为重要，类似的，我们也可以通过获取的梯度值图来进行图像分割：
![](/assets/CS-231n-13/Pasted image 20241127204853.png)
同时，通过这种方法我们也可以获知CNN学习的一些奇怪现象，比如对于一个分类狼和狗的CNN分类器，通过观察图片的哪些部分较为重要，我们发现：分类器只学到了观察背景里有没有雪。。。
![](/assets/CS-231n-13/Pasted image 20241127205242.png)


### Guided backprop to generate images
类似的，不止可以通过观察原图的梯度结果，我们也可以通过观察中间层的梯度结果来可视化学习到的特征，并且，实际上，我们可以通过一个小小的trick来使得中间层梯度值输出的图片更好，我们将这种方法称为Guided Backprop：
对于一般的反向传播，我们只传播正向传播过程中大于0的激活值，在Guided Backprop中，我们还额外地不传播梯度值小于0的梯度值：
![](/assets/CS-231n-13/Pasted image 20241127205719.png)
### Gradient ascent to visualize features
既然我们可以通过观察输入数据的梯度值可视化，那么转变一下想法，我们能否对输入数据进行反向传播优化从而生成符合网络特征的图片？答案是肯定的，我们将这种方法称作是Gradient ascent。
形式化地讲，在Gradient Ascent中，我们的目标是寻找一个输入数据$I^*$，使得：
$$I^*=\mathrm{argmax}_I(f(I) + R(I))$$
其中$f(I)$是我们需要最大化的卷积核激活值（可以是多个求和/平均），$R(I)$是我们为了使生成的图片更加自然添加的正则化函数。
算法的过程与梯度下降类似，损失函数可以定义为目标函数（即需要最大化的值）的负数，我们将输出的图片初始化为全0，并且每次反向传播只更新图片的值。

关于其中的正则化函数，一种简单的实现就是采用简单的L2正则化函数$-\lambda||I||_2^2$，此外我们还可以采用一些更好用的正则化方式，例如：
1. 在使用L2正则化的同时，周期性地对图像进行投影，使其满足一定约束（如像素值在一定区间内）
2. 采用高斯模糊（补充）
3. 设置小值像素到0
4. 设置小梯度像素为0

![](/assets/CS-231n-13/Pasted image 20241128101436.png)

当然我们也可以设置中间层的激活值为目标函数：

![](/assets/CS-231n-13/Pasted image 20241128101541.png)

为了更高效地生成图片并且提高图片的质量，我们也可以不在像素空间中进行优化，而是使用一个额外的神经网络用于生成图片，将目标转为优化该网络中的隐藏空间（补充）：
![](/assets/CS-231n-13/Pasted image 20241128102258.png)

## Adversarial Perturbations（对抗性扰动）
既然我们可以从一张空白的图片开始，最大化它在某种类别上的得分，那么我们能不能通过对某一张任意的图片添加类似的扰动，使其能被分成给定的一类，而不是原图的结果，同时我们也很好奇对某一张图片进行如此操作后，它在像素空间上的视觉变化是否会很大。
实际上，这种方法的操作很类似于Gradient Ascent，我们从一张任意的图片和一个任意选定的类别开始，不断优化图片使得图片在选定类别上的得分最高，达到欺骗神经网络的效果。
事实上，经过这些操作的图片在视觉上一般来说并不会出现很大的变化：
![](/assets/CS-231n-13/Pasted image 20241128144354.png)
同时，我们也可以泛化这种扰动Difference（可补充）， 使得大部分图片在添加上述扰动之后会使得神经网络对其的分类发生变化：
![](/assets/CS-231n-13/Pasted image 20241128144449.png)
## Style Transfer
### Feature Inversion（特征反演）
在Gradient Ascent中，我们的目标一般来说是最大化输入在某个类别上的得分，而在Feature Inversion中，我们将目标改为最小化输入经过CNN处理后得到的特征向量与给定特征向量之间的距离，形式化地讲，我们要找到：
$$\begin{aligned}
\textbf{x}^* &= \mathrm{argmin}_{\textbf{x}\in\mathbb{R}^{H\times X\times C}}\ell(\Phi(\textbf{x}) - \Phi_0) + \lambda\mathcal{R}(\textbf{x})\\
&\ell(\Phi(\textbf{x}) - \Phi_0) = ||\textbf{x} - \Phi_0||^2\\
& \mathcal{R}_{V^\beta}(\textbf{x}) = \sum_{i,j}((x_{i+1,j}-x_{i,j})^2+(x_{i,j+1} - x_{i,j})^2)^{\frac{\beta}{2}}
\end{aligned}$$
其中$\Phi_0$是给定的特征向量，$\ell$计算输入与期望输出特征向量之间的距离，$\mathcal{R}$是总体的正则化函数，上面的示例中加入了强度为$\beta$的平滑正则化（即希望图片中相邻像素间的差值不应太大，从而使得得到的图片相对自然）。

当然，由于每一个卷积层都会输出特征，我们可以选定任意一个卷积层来反演特征，如下图所示，可以发现，对越深的层进行反演则输出的图片更加的抽象扭曲，这是因为更深的层提取了更加广泛泛化的特征。
![](/assets/CS-231n-13/Pasted image 20241128161833.png)

### Deep Dream: Amplifying Existing Features
这部分好抽象，没看懂，贴一张ppt
![](/assets/CS-231n-13/Pasted image 20241128162152.png)

### Texture Synthesis
Texture Synthesis（纹理合成）是一类类似于超分的任务（？），在这个任务中，我们需要接受一个纹理图片，并生成一张较大的相同纹理的图片：
![](/assets/CS-231n-13/Pasted image 20241128162924.png)

一种显然的方法就是采用类似ps中仿制图章的做法，每次从相邻区域复制并拼接，但这种方式在复制一些并不那么对称的纹理时就会显得比较奇怪，于是我们可以考虑使用神经网络进行生成。

在介绍具体的架构之前，我们先来介绍一个名叫Gram Matrix的东西，回忆，CNN中某一卷积层输出的是一个$C\times H\times W$的矩阵，这意味着每一个像素都对应着一个$C$维向量。我们现将矩阵reshap成一个由$H\times W$个$C$维列向量组成的矩阵$F=[c_1,c_2,...,c_n]$，于是我们定义Gram Matrix为：
$$\textbf{G}_{C\times C} = \textbf{FF}^T = \sum_{i = 1}^{HW}c_ic_i^T$$
这保证了不同大小的图片在同一个CNN网络同一层的输入中总是能输出相同大小的Gram Matrix

计算的方法也与之前类似，采用随机生成噪声图片之后进行梯度下降优化图片的方式，但这次我们需要优化的目标函数为Gram Matrix之间的距离。具体来说，我们先在预训练的CNN网络中计算给定纹理小图片在CNN各层的Gram Matrix，再以此为目标对随机生成的噪声图片在同一个CNN中进行优化，损失函数为：
$$
\begin{aligned}
&E_l = \frac{1}{4H_l^2W_l^2}\sum_{i,j}(\hat{\textbf{G}}_{i,j} - \textbf{G}_{i,j})^2\\
&\mathcal{L}(\vec{x}, \hat{\vec{x}}) = \sum_{l}w_lE_l
\end{aligned}
$$
从损失函数的定义我们可以看出，我们可以同时最小化多层中的Gram Matrix距离，并对其做加权平均。

实际上，从更深的层合成纹理一般可以获得更好的效果：
![](/assets/CS-231n-13/Pasted image 20241128165300.png)


### Neural Style Transfer
在前几节中，我们看到，特征反演（Feature Inversion）尝试复原原图在像素层面上的特征，而纹理合成（Texture Synthesis）尝试模仿原图的风格，并将其复制。那么我们将这两种技术结合到一起，就变成了一项很有意思的技术：Style Tranfer。输入一张内容图片和风格图片，Style Tranfer可以生成一张保留内容图片特征并且使用风格图片风格的新图片。
具体而言，我们只需要融合Feature Inversion和Texture Synthesis中的损失函数并进行优化即可：
![](/assets/CS-231n-13/Pasted image 20241128165823.png)

我们还可以对多个风格图的Gram Matrix进行加权平均来得到混合风格的图片：
![](/assets/CS-231n-13/Pasted image 20241128165936.png)

然而上述做法存在问题，每次生成图片我们都需要对图片进行多次优化一降低损失函数，类似于模型训练，这种做法的时空开销是很大的，于是我们想，能不能通过训练一个专门的神经网络，使得只要通过一次前向传播就能输出结果图片？

事实上这是可行的，类似于我们对图片进行训练，这次我们对这个专门的前馈网络进行训练：
![](/assets/CS-231n-13/Pasted image 20241128170346.png)
训练时这个网络接受内容图片并进行前向传播生成结果图片，结果图片传入计算损失函数的预训练CNN模型得到梯度并反向传播回前馈网络。
从中我们可以发现，这种网络只能训练并且转换一个单独的风格。同时我们需要使用多个内容图进行训练否则易于出现过拟合的现象。

**一些小的trick：**
对于style transfer网络，我们可以使用例归一化的方法提升模型的性能：
![](/assets/CS-231n-13/Pasted image 20241128170825.png)
同时我们还可以通过这一种归一化技术来使用一个网络训练多种不同的风格，即我们可以为每一种风格单独训练一个例归一化参数$\mu,\sigma,\beta,\gamma$只需要通过调整这些参数即可在同一网络中实现输出不同风格的图片。