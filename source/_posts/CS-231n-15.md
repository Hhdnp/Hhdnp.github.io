---
title: CS 231n Lec13 笔记
date: 2024-12-05 21:27
tags:
    - 生成式模型
    - Generative Models
    - Pixel CNN/RNN
    - Variational Autoencoders
    - Generative Adversarial Networks
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>


## Supervised v.s. Unsupervised
对于有监督学习，我们的目的是通过让模型学习一些数据$(x,y)$使得模型能够学习到其中隐藏的映射关系$x\rightarrow y$从而进行分类，回归亦或是目标检测等更加细化的任务

在无监督学习中，我们给模型学习的数据仅有输入$x$而不再有期望的输出$y$，而目标也简化为让目标发现数据$x$中隐藏的规律/结构，典型任务有聚类(Clustering)，降维(Dimensional Reduction)，密度估计(Density Estimation)等等。

在自监督学习中，我们仍然只有输入$x$可以学习，但可以通过伪任务生成伪标签。我们的目的是使模型通过与任务的训练，具有良好的特征提取能力。
![](/assets/CS-231n-15/Pasted image 20241203200707.png)

## Generative Models
广泛地来说，生成式模型(Generative Models)的任务是让模型学习给定的训练数据data后，可以生成符合训练数据（与训练数据具有类似特征）的sample。具体来说：
$$\begin{aligned}
&data\sim p_{data}(x)\\
\text{aim:}&p_{model}(x)\rightarrow p_{data}(x)\\
\text{output:}&\sim p_{model}(x)
\end{aligned}$$
简而言之，我们希望训练过后的数据输出能够符合训练数据的分布。
关于生成式模型的分类，主要是关于模型如何学习训练数据的分布，大体上来说可以分成两类：
1. Explicit Density Estimation：模型需要明确定义并求解$p_{model}(x)$
2. Implicit Density Estimation：在无需定义求解概率的情况下学习采样

同时生成式模型也有广泛的应用：超分，上色，学习特征等等。
生成式模型的详细分类可以参见下图：
![](/assets/CS-231n-15/Pasted image 20241203201328.png)
在这节课中我们将主要讨论其中的三个：
PixelCNN
VAEs
GANs

## PixelCNN & PixelRNN
### Fully Visible Belief Network(FVBN)
首先由于这是一个Explicit GM，我们首先需要为模型定义一个概率分布：
$$p(x) = p(x_1,x_2,x_3,...x_n)$$
这是一个关于图像中各个像素点的联合概率分布。
于是我们应当通过训练数据估计出$p(x)$的分布，然而，我们并没有参数可供估计，于是有一个很直观的想法，便是将联合概率分布通过链式法则分解为一系列条件概率之积后估计这些条件概率的分布：
$$p(x) = \prod_{i=1}^{n}p(x_i|x_1,...,x_{i-1})$$
然而这样的估计太过复杂（条件概率难以表示）。从而我们考虑使用神经网络表示这个联合概率分布。

### Pixel RNN
观察条件概率的形式，很容易联想到类似的RNN隐藏状态间的依赖关系，于是我们只需要将RNN这一张DAG一一种类似于图片中像素的方式连接起来即可：
![](/assets/CS-231n-15/Pasted image 20241203202037.png)
然而就如同RNN的通病，这样的计算太过缓慢。

### Pixel CNN
在这种方法里，我们仍然从角落开始生成，但我们会使用CNN计算像素间的依赖关系：
![](/assets/CS-231n-15/Pasted image 20241203202150.png)
这样可以通过并行化显著加快训练的速度，然而生成仍然是相当缓慢的，因为需要逐像素生成。
针对Pixek RNN/CNN还有一些可能的改进措施：
![](/assets/CS-231n-15/Pasted image 20241203202256.png)

## Variational Autoencoders(VAEs)
在上一种方式中，我们直接对输入数据的分布进行建模优化：
$$p_{\theta}(x) = \prod_{i=1}^np_{\theta}(x_i|x_1,...,x_{i-1})$$
而在VAE中，我们放弃了直接对$x$间的依赖关系进行捕捉与建模优化，而是引入一个隐藏状态$z$，通过假设$x$依赖于$z$，估计出$x$的分布：
$$p_{\theta}(x) = \int p_{\theta}(z)p_\theta(x|z)\mathrm{d}z$$
这种方式有一个显著的好处，像素间在计算时不再具有依赖的关系，可以通过并行化大幅加速训练以及计算的过程。但隐藏状态$z$以及积分$\int$的引入为优化带来了很大的困难。
那具体来说我们应该如何解决这一问题呢？
首先我们需要通过训练数据学习隐藏空间$z$，并通过$z$采样。
从而我们自然而然地想到了编码器-解码器的架构。
![](/assets/CS-231n-15/Pasted image 20241203202914.png)
其中，一般来说$z$的维度低于$x$，为的是使特征$z$能够具有更强的概括能力(提高其中的信息熵)。因此这实际上也是一种用于降维特征表示(lower-dimensional feature representation)的无监督学习方法。
关于优化过程，显然地，我们的目标是减少数据在编码解码过程中的信息损失，于是我们只需要直观地比较$x$与$\hat{x}$之间的差异，典型的我们可以选取均方误差函数：
$$\mathcal{L} = ||x - \hat{x}||_2^2$$
从而至少我们在学习完成之后得到了一个可以用于特征降维的Encoder。
我们可以将这个Encoder用于特征提取，例如（在微调之后）用于预测，分类等典型的下游任务。
但这偏离了我们的初始目的：生成新图片。但这仍然是很困难的，因为我们不知道$z$的取值空间。
### Probability Spin on Autoencoder
首先我们假设训是从隐藏空间$z$中采样$x\sim p_{\theta^*}(x|z^{(i)})$，而$z$又从一个先验分布中采样$z\sim p_{\theta^*}(z)$。
*Remember：* $x$是输入的图像，$z$是隐藏状态，用于采样$x$
于是我们需要在给定数据$x$的前提下估计真实参数$\theta^*$，那又该如何建立表示概率模型呢？我们可以选择一个简单且make sense的分布比如正太分布。
同样，条件概率$p(x|z)$很复杂，我们采用神经网络进行表示。
那该如何训练呢？
$$p_{\theta}(x) = \int p_{\theta}(x)p_{\theta}(x|z)\mathrm{d}z$$
棘手的问题是积分，那么我们可以：
1. 使用Monte Carlo估计：
$$\log p(x)\approx \log\frac1k\sum_{i=1}^k p(x|z^{(i)})$$
但这种方式方差太大。

2. 使用后验分布:
$$p_{\theta}(z|x) = \frac{p_\theta(x|z)p_\theta(z)}{p_\theta(x)}$$
我们可以转而训练$q_{\phi}(z|x)$用于估计真实分布$p_\theta(z|x)$，这是易于训练的，因为$x$的范围是给定的。
然后我们就有：
$$\begin{aligned}
\log p_{\theta}\left(x^{(i)}\right) & =\mathbf{E}_{z \sim q_{\phi}\left(z \mid x^{(i)}\right)}\left[\log p_{\theta}\left(x^{(i)}\right)\right] \quad\left(p_{\theta}\left(x^{(i)}\right) \text { Does not depend on } z\right) \\
& =\mathbf{E}_{z}\left[\log \frac{p_{\theta}\left(x^{(i)} \mid z\right) p_{\theta}(z)}{p_{\theta}\left(z \mid x^{(i)}\right)}\right] \quad \text { (Bayes' Rule) } \\
& =\mathbf{E}_{z}\left[\log \frac{p_{\theta}\left(x^{(i)} \mid z\right) p_{\theta}(z)}{p_{\theta}\left(z \mid x^{(i)}\right)} \frac{q_{\phi}\left(z \mid x^{(i)}\right)}{q_{\phi}\left(z \mid x^{(i)}\right)}\right] \quad \text { (Multiply by constant) } \\
& =\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} \mid z\right)\right]-\mathbf{E}_{z}\left[\log \frac{q_{\phi}\left(z \mid x^{(i)}\right)}{p_{\theta}(z)}\right]+\mathbf{E}_{z}\left[\log \frac{q_{\phi}\left(z \mid x^{(i)}\right)}{p_{\theta}\left(z \mid x^{(i)}\right)}\right] \quad \text { (Logarithms) } \\
& =\mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} \mid z\right)\right]-D_{K L}\left(q_{\phi}\left(z \mid x^{(i)}\right) \| p_{\theta}(z)\right)+D_{K L}\left(q_{\phi}\left(z \mid x^{(i)}\right) \| p_{\theta}\left(z \mid x^{(i)}\right)\right)
\end{aligned}$$
其中：
$$\begin{aligned}
D_{KL}(p||q) = \mathbb{E}_x\left(\log\frac{p(x)}{q(x)}\right)
\end{aligned}$$
是KL散度，用于衡量两种概率分布的相似程度。

上式由一个期望和两个KL散度构成，其中第一部分由Decoder网络给出，第二部分可由$z$的先验分布和Encoder网络给出，第三部分的计算很棘手，但由于KL散度是一个非负的量，从而可以有：
$$\begin{aligned}\log p_{\theta}\left(x^{(i)}\right) \ge \mathbf{E}_{z}\left[\log p_{\theta}\left(x^{(i)} \mid z\right) \right]-D_{K L}\left(q_{\phi}\left(z \mid x^{(i)}\right) \| p_{\theta}(z)\right)
\end{aligned}$$
从而我们给出了似然估计的下界并且这是可以通过对Encoder，Decoder以及$z$的参数优化的，以假设$z$服从正太分布为例：
![](/assets/CS-231n-15/Pasted image 20241203210048.png)

优化完成之后我们就可以生成数据了，这一部分不在需要Encoder：
![](/assets/CS-231n-15/Pasted image 20241203210119.png)

并且在尝试生成了一些数据之后我们发现一个很好玩的现象，$z$的不同维变化很像是改变图片的不同的特征，如：
![](/assets/CS-231n-15/Pasted image 20241203210256.png)
![](/assets/CS-231n-15/Pasted image 20241203210325.png)

### Pros and Cons
优势：
1. 有坚实的理论基础
2. 隐藏空间$z$有很强的可解释性
3. Encoder学习到的$x\rightarrow z$也可以用于特征表示供下游任务

不足：
1. 优化的下界而非概率本身，不够精确，生成图片的质量，分辨率相较于PixelRNN/CNN不佳
2. 与GANs相比由于采用正太分布，生成的sample较为模糊

可供研究的方向：
1. VAEs目前大部分以对角正太分布为$z$的分布，灵活性较差，是否可以采用其他分布？
2. 能否学习解耦表示，即让$z$的每个维度可控地学习，表示不同的特征，并且可解释。如正则化VAE（$\beta$-VAE）或新架构。
3. 能否通过改进解码器（如PixelCNN）提高生成sample的清晰度？或是能否融合GANs和VAEs保留VAEs的概率框架与GANs的生成高质量。
## Generative Adversarial Networks(GANs)
之前介绍的生成式模型，在PixelRNN/CNN中，我们需要优化
$$p(x) = \prod_{i=1}^n p(x_i|x_1,...,x_{i-1})$$
而在VAE中，我们需要优化
$$p(x) = \int p(z)p(x|z)\mathrm{d}z$$
但是由于这些概率模型太过复杂，我们无法直接对它们进行优化，而是需要转而对它们它们的下界进行优化。
那么有没有一种方法能够对模型进行直接的优化呢。
既然建立概率模型的方法不行，那么我们可以考虑不建立概率模型而直接训练模型采样能力的方法，GAN就是其中一种。


首先，一种显而易见的想法便是类似于VAE中我们先从一个简单的分布（正态分布等等）中采样噪声$z$，在通过生成网络将$z$映射到我们想要的采样中。
![](/assets/CS-231n-15/Pasted image 20241205204934.png)
但最大的问题就是我们不知道对于$z$，我们希望输出的采样是什么，这也就意味着我们不能运用VAE中重建图像的方式来对生成网络进行优化。
于是，天才般的科学家想出了竞争的办法：我们引入一个辨别网络(Discriminator Network)用于判断输入图像的真伪，并且同时优化这两个网络。
![](/assets/CS-231n-15/Pasted image 20241205205458.png)
于是我们训练的目标就有两个：
对于辨别器来说，我们需要最大化判断正确的成功率，对于生成器来说，我们需要尽可能的不让辨别器判断正确，从而我们的目标就类似于一个Minimax Game：
$$\min_{\theta_g}\max_{\theta_d}\left[\mathbb{E}_{x\sim p_{data}}\log D_{\theta_d}(x) + \mathbb{E}_{z\sim p(z)}\log\left(1-D_{\theta_{d}}\left(G_{\theta_g}\left(z\right)\right)\right)\right]$$
其中$D_{\theta}(\cdot)$ 是以$\theta_d$为参数的辨别器网络，$G_{\theta_g}$是以$\theta_g$为参数的生成器网络。
简单来说，辨别器希望能够对所有真实图片都输出接近1的预测概率，对所有生成图片都输出接近0的预测概率。
生成器只能控制第二部分，他希望能够使辨别器对自己生成的图片也输出接近1的预测概率。
形式化地说，我们需要以以下两个函数分别为损失函数训练辨别器和生成器：
$$\begin{aligned}
-&\left[\mathbb{E}_{x \sim p_{\text {data }}} \log D_{\theta_{d}}(x)+\mathbb{E}_{z \sim p(z)} \log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)\right]\\
&\left[\mathbb{E}_{z \sim p(z)} \log \left(1-D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)\right]
\end{aligned}$$
然而实际上这样的训练效果并不好，观察图像就可以发现，在训练的初期，生成器的损失仍然很高时，它对应的梯度是很小的，这意味着训练将会很缓慢，而只有当训练效果很好时，即生成器已经可以生成相对"real"的图片时，梯度才开始变得陡峭。
![](/assets/CS-231n-15/Pasted image 20241205211739.png)
从而我们需要使用一个小trick，我们将生成器的损失函数改为:
$$-\left[\mathbb{E}_{z \sim p(z)} \log \left(D_{\theta_{d}}\left(G_{\theta_{g}}(z)\right)\right)\right]$$
这样我们就可以将梯度的大小倒过来了，可以提升训练效果。
![](/assets/CS-231n-15/Pasted image 20241205212140.png)

在训练之后，我们将生成器网络单独拿出来，就可以生成图片了。
PPT后还有一些改进算法，相关介绍的资源，在这里就不写了，详见PPT。
![](/assets/CS-231n-15/Pasted image 20241205212658.png)