---
title: CS188 Note 21笔记
date: 2024/10/11 20:057
tags:
    - 机器学习
    - Logistic回归
    - 梯度下降算法
    - 人工神经网络
    - 反向传播算法
categories: 机器学习笔记
mathjax: true
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>
## Optimization
在上一节讲义中，我们学习了如何结合微分，通过最小二乘法，进行对目标函数的拟合。然而，目标函数不总是存在的，于是，在这种情况下，我们可以使用梯度下降（上升）(gradient descent/ascent)的方法来优化我们的权重向量使其符合我们的要求。
算法流程如下：
```python
while w not converge:
	w = w +(or -) alpha * gradient(f(w), w) # 这是f对每个w求偏微分得到的向量
```
其中`alpha`是学习率，它表示权重向量向梯度下降的方向移动的速率，学习率是一个超参数，它的选取相当重要，太高的学习率会导致权重向量难以收敛，同时太低的权重向量会导致收敛缓慢，降低程序运行速率。其中一种常用的方法就是在一开始设置较大的`alpha`并在之后逐渐下降。
同时每次采样的数据点数量也是一个超参数，在随机梯度下降（stochastic gradient descent）中，我们每次只采样一个数据点，然而一个数据点可能含有较大的噪声，这会导致收敛较为缓慢。于是我们可以小批量分次采样，每次处理若干个数据点，这种方法称作Mini-batch gradient descent。
此外，讲义以最小二乘拟合线性函数进行了梯度下降算法的举例。

## Logistic Regression
Logistic函数是一种用于分类问题的函数，它一般不用做拟合：
$$h_w(\overrightarrow{x}) = \frac{1}{1+e^{-\overrightarrow{w}^T\overrightarrow{x}}}$$
由于函数的取值范围是0-1，该函数可以被用于衡量某一数据点被分类为某个标签的概率。这在分类中相当有用，例如，如果我们计算出一个数据点的Logistic函数值大于0.5，那么我们将它归类于标签1，否则归类为标签0。
### Logistic 函数的损失函数
证明可以见讲义，这里只给出结论：
$$\frac{\partial L}{\partial \overrightarrow{w}} = -(y - h_w(\overrightarrow{x}))\overrightarrow{x}$$

## Multi-class Logistic Reression
在多分类问题中，我们使用softmax函数来衡量数据点被分类为各个标签的概率，softmax函数的定义如下：
$$P(y=i \mid \mathbf{f}(\mathbf{x}) ; \mathbf{w})=\frac{e^{\mathbf{w}_{i}^{T} \mathbf{f}(\mathbf{x})}}{\sum_{k=1}^{K} e^{e_{k}^{T} \mathbf{f}(\mathbf{x})}}
$$
可以看出，softmax函数仍然是基于Logistic函数的，它可以被简易地描述为，数据点i被分为某一类别的概率占i被分为各个类别的概率之和。
因此，显然有由softmax函数生成的概率分布是合法的，我们进而应当使用最大似然估计来优化我们的权重向量，似然函数可以定义为：
$$\ell\left(\mathbf{w}_{1}, \ldots, \mathbf{w}_{K}\right)=\prod_{i=1}^{n} P\left(y_{i} \mid \mathbf{f}\left(\mathbf{x}_{i}\right) ; \mathbf{w}\right)$$
假设共有$K$个可能的标签，$w_k$是第$k$个类别对应的权重函数。
似然函数可以进一步简化表示为：
$$\ell\left(\mathbf{w}_{1}, \ldots, \mathbf{w}_{K}\right)=\prod_{i=1}^{n} \prod_{k=1}^{K}\left(\frac{e^{\mathbf{w}_{k}^{T} \mathbf{f}\left(\mathbf{x}_{i}\right)}}{\sum_{\ell=1}^{K} e^{\mathbf{w}_{\ell}^{T} \mathbf{f}\left(\mathbf{x}_{i}\right)}}\right)^{t_{i, k}}$$
表达式看上去变得复杂，但是我们可以理解为，对每个数据点仅保留预期类别的概率，其余在乘积式中均设为1。
类似的，我们还可以将似然函数通过对数运算转换为和：
$$\nabla_{\mathbf{w}_{j}} \log \ell(\mathbf{w})=\sum_{i=1}^{n} \nabla_{\mathbf{w}_{j}} \sum_{k=1}^{K} t_{i, k} \log \left(\frac{e^{\mathbf{w}_{k}^{T} \mathbf{f}\left(\mathbf{x}_{i}\right)}}{\sum_{\ell=1}^{K} e^{\mathbf{w}_{\ell}^{T}} \mathbf{f}\left(\mathbf{x}_{i}\right)}\right)=\sum_{i=1}^{n}\left(t_{i, j}-\frac{e^{\mathbf{w}_{j}^{T} \mathbf{f}\left(\mathbf{x}_{i}\right)}}{\sum_{\ell=1}^{K} e^{\mathbf{w}_{\ell}^{T}} \mathbf{f}\left(\mathbf{x}_{i}\right)}\right) \mathbf{f}\left(\mathbf{x}_{i}\right)$$
## Neural Networks: Motivation
### Non-linear Seperators
这章表达的意思很简单，如下两张图，我们可以通过适当的映射（升维）来将非线性的分类问题转换为可以是线性的。
![alt text](/assets/CS-188-2/CS-188-2-1.png)
![alt text](/assets/CS-188-2/CS-188-2-2.png)
### Multi-layer Perceptron
我们可以如图，通过对输入的数据点进行多次线性函数的组合（有大于0判断，所以结果结果不一定线性），拟合出需要的函数。
![alt text](/assets/CS-188-2/CS-188-2-3.png)
这里还介绍了一个定理：有足够神经元的两层的神经网络已经可以用来以任意精度拟合估计任意的连续实函数。
### Measuring Accuracy
我们利用下面的函数来评估一个模型的准确度
$$l^{a c c}(\boldsymbol{w})=\frac{1}{n} \sum_{i=1}^{n}\left(\operatorname{sgn}\left(\boldsymbol{w} \cdot \mathbf{f}\left(\mathbf{x}_{i}\right)\right)==y_{i}\right)$$
即预测准确的数据点数占总数的比值。
在多分类问题中，我们用softmax函数来衡量准确度，其中我们需要最大化每个数据点被归类正确的概率之积：
$$\log \ell(\boldsymbol{w})=\log \prod_{i=1}^{n} P\left(y_{i} \mid x_{i} ; \boldsymbol{w}\right)=\sum_{i=1}^{n} \log P\left(y_{i} \mid \mathbf{f}\left(\mathbf{x}_{i}\right) ; \mathbf{w}\right)$$
（换成对数求和更好算）
### Multi-layer Feedforward Neural Networks
为了实现更好的预测，我们需要在神经网络中加上一些非线性函数，如
sigmoid函数：
$$\frac{1}{1+e^{-x}}$$
ReLU函数：
$$f(x)=\left\{\begin{array}{ll}0 & \text { if } x<0 \\ x & \text { if } x \geq 0\end{array}\right.$$
在神经网络中，我们需要恰当地选择每层使用的函数，这是个相当困难的事情。
### Loss Functions and Multivariate Optimization
关于如何优化，仍然是梯度下降，只有一点点不同，要回顾的话回去看讲义吧，讲义也不长。

## Neural Networks: Backpropagation
这部分是机器学习的精髓，讲义以及配图已经相当浅显易懂了，需要回顾还是建议去回看讲义，主要内容就是偏导数求导的链式法则。