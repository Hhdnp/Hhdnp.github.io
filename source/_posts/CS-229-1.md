---
title: CS 229 Note1 线性模型
date: 2024-12-09 20:13
tags:
    - Linear Regression
    - Logistic Regression
    - General Linear Models
    - Softmax Regression
    - LMS algorithm
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>




首先先让我们对符号做出一些约定：

| 符号                                            | 含义                               |
| ----------------------------------------------- | ---------------------------------- |
| $x^{(i)}$                                       | features(特征)                     |
| $y^{(i)}$                                       | target(目标输出)                   |
| $(x^{(i)}, y^{(i)})$                            | a training example                 |
| $\lbrace(x^{(i)}, y^{(i)}) ; i=1,2,...n\rbrace$ | training set                       |
| $\mathcal{X}$                                   | 特征的取值范围                     |
| $\mathcal{Y}$                                   | 输出的取值范围                     |
| $h:\mathcal{X}\rightarrow\mathcal{Y}$           | hypothesis(假设的输入到输出的函数) |
在有监督学习中，我们将target取值连续的预测问题称为Regression Problem(回归问题)，取值离散的预测问题称为Classification Problem(分类问题)。

# Part I: Linear Regression
线性回归的问题简单来说就是使用一个线性函数:
$$h_\theta(x) = \sum_{i=0}^n\theta_ix_i = \theta^Tx$$
来拟合已有的用于学习的数据点，在介绍什么是很好的拟合，怎么样很好的拟合之前，我们先进行一些小的修正。
上述的$h_\theta$函数严格意义上来说是一个Affine Function(仿射函数)而不是一个Linear Function(线性函数)，但在机器学习中我们还是将其称为线性函数。
另外由于这样定义的Hypothesis会有一个问题，便是始终会过原点，我们约定只有$i=1,2,3...,n$时$x_i$才是需要输入的特征，而$x_0$是一个约定恒为1的参数，用于提供偏置(intercept term)。

那么我们该如何选择，学习参数$\theta$？显然的，我们需要使得$\theta$对应的线性函数能够很好地拟合给出的训练数据，为了合适地定义怎样为很好的拟合，我们定义一个**cost function：**
$$J(\theta) = \frac12\sum_{i=1}^n\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^2$$
这就是**ordinary least squares**回归模型，很好的拟合就是需要尽可能减少这个函数的值，这是一个相当自然的想法，因为很好的估计就是应当能够使得经过假设函数处理后输出的预测值与实际值向接近。
但实际上，这个cost function的背后有很深刻的统计学原理，稍后我们会介绍，并且这还是一个更加广泛模型的特例。

## LMS algorithm
在我们给定了一个定义相当自然的需要最小化的cost function后，我们需要解决的问题便转换成了如何改变$\theta$使得$J(\theta)$可以最小化？
直观上来说，我们可以不断地改变$\theta$，类似于爬山法，将$J(\theta)$不断减小，我们便可以推测$\theta$最后会收敛在$J(\theta)$的最小值点（可能会有疑问，万一这是局部最小值，但实际上$J(\theta)$这个函数是一个凸二次函数，局部最小值就是全局最小值）。
对这种方式的形式化实现就是经典的梯度下降算法：
$$\theta_j:=\theta_j-\alpha\frac{\partial J(\theta)}{\partial \theta_j}$$
这一更新对于$j=0,1,2,...,n$是同步进行的，并且 $\alpha$是**learning rate**(学习率)。这是一个非常自然的算法因为在$\theta$的空间中，这样更新的方向是$J$下降最快的方向。
接下来计算更新公式中的偏导数（不同于原讲义，直接向量化求导）：
$$\begin{aligned}
\mathrm{tr}\left(J\left(\theta\right)\right) &= \frac12\mathrm{tr}\left(\mathrm{d}\left(\theta^Tx^{(i)}-y^{(i)}\right)^2\right)\\
&=\mathrm{tr}\left(\left(\theta^Tx^{(i)}-y^{(i)}\right)\mathrm{d}\left(\theta^Tx^{(i)}-y^{(i)}\right)\right)\\
&=\mathrm{tr}\left(\left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}\mathrm{d}\theta^T\right)\\
\frac{\partial J}{\partial \theta} &= \left(\theta^Tx^{(i)}-y^{(i)}\right)x^{(i)}
\end{aligned}$$
这就是经典的**LMS update rule**。这种更新方式也是相当自然的，比如每一个参数被更新的幅度是正相关于它本身所对应的误差项**error term**$\left(\theta^Tx^{(i)}-y^{(i)}\right)$的。
同时，若是在每一步中对整个Training Set进行学习并且更新，那么这种更新方法被称作是**batch gradient descent**。
以下是一个线性回归的可视化学习过程以及学习的结果：
![](/assets/CS-229-1/Pasted image 20241213191431.png)
![](/assets/CS-229-1/Pasted image 20241213191446.png)
当然，与批量的学习相对，我们也可以对训练集中的training example随即逐个地进行训练，这种训练方式被称为**stochastic gradient descent**。这种训练方式是为了避免当训练集的大小过大的时候批量化更新的每次更新会变得极其昂贵。而逐个的训练方式在这种情况下能快速地训练并且达到接近全局最小值的地方，但可能并不会收敛到全局最小值而是在其周围进行徘徊。

## The normal equations
虽然梯度下降已经是一种相当有效的优化参数$\theta$的方法，但我们在这一节中还会介绍另一种优化的方法，而这种优化的算法无需使用循环多次优化。而是借助二次凸函数的性质，直接求一阶偏导为$\textbf{0}$的参数$\theta$。但在进行这些计算之前，我们先来对矩阵进行微分中需要使用并且约定的一些符号。
### Matrix derivatives
其实就是对于一个函数$f:\mathbb{R}^{n\times d}\rightarrow \mathbb{R}$，我们称其对于输入矩阵中每一个参数的偏导数仍然构成一个矩阵，记作：
$$\begin{aligned}
\nabla_\theta f(A)=
\begin{bmatrix}
\frac{\partial f}{\partial A_{11}}&\frac{\partial f}{\partial A_{12}}&...&\frac{\partial f}{\partial A_{1d}}\\
\frac{\partial f}{\partial A_{21}}&...&...&\frac{\partial f}{\partial A_{2d}}\\
...&...&...&...\\
\frac{\partial f}{\partial A_{n1}}&...&...&\frac{\partial f}{\partial A_{nd}}

\end{bmatrix}
\end{aligned}$$
### Least squares revisited
为了一次性使得最小二乘估计对于所有的训练样本都生效，我们先将所有训练样本拼接在一起:
$$X = \begin{bmatrix}\left(x^{(1)}\right)^T\\\left(x^{(2)}\right)^T\\...\\\left(x^{(n)}\right)^T\end{bmatrix}$$
$$\vec{y} = \begin{bmatrix}y^{(1)}\\y^{(2)}\\...\\y^{(n)}\end{bmatrix}$$
我们对$J(\theta)$函数进行求梯度可以得到：
$$\begin{aligned}
J(\theta) &= \frac12\left(X\theta - y\right)^T\left(X\theta-y\right)\\
\mathrm{tr}(\mathrm{d}J) &= \frac12\mathrm{tr}\left(\mathrm{d}z^Tz\right)\\
&=\frac12\mathrm{tr}\left(2z^T\mathrm{d}z\right)\\
&=\frac12\mathrm{tr}\left(2\left(X\theta-y\right)^T\mathrm{d}\left(X\theta-y\right)\right)\\
&=\frac12\mathrm{tr}\left(2(X\theta-y)^TX\mathrm{d}\theta\right)\\
\frac{\partial J}{\partial \theta}  &= X^T(X\theta-y)
\end{aligned}$$

令其等于$\textbf{0}$就可以得到所谓的**normal equations**:
$$X^TX\theta = X^Ty$$
从而我们就可以得到$J$的全局最小值点：
$$\theta = \left(X^TX\right)^{-1}X^Ty$$
### Probabilistic interpretation
这一节中将介绍很自然的$J(\theta)$函数到底是怎么来的。
首先我们假设目标的输出值是由一个确定值加上一个误差项构成的：
$$y^{(i)} = \theta^Tx^{(i)} + \epsilon_i$$
其中误差项$\epsilon_i$可能是输入数据完全无法确定的误差，也可能是随机的噪声项，但总而言之，这个误差项是无法避免的。
进一步，我们假设这个误差项均从一个正态分布$\mathcal{N}(0,\sigma^2)$中独立采样，于是我们就可以得到在给定输入的情况下目标输出正好是$y^{(i)}$的概率：
$$p\left(y^{(i)} \mid x^{(i)} ; \theta\right)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)$$
也就是说，输出服从均值为$\theta^{T} x^{(i)}$，方差为$\sigma^2$的正态分布。
从而，我们所有训练集中的目标输出都有了一个在给定输入和假设参数下的输出概率，自然的，我们想要最大化这些概率，于是我们对$\theta$进行极大似然估计。
$$\begin{aligned}
L(\theta) & =\prod_{i=1}^{n} p\left(y^{(i)} \mid x^{(i)} ; \theta\right) \\
& =\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right)
\end{aligned}$$
改写成对数似然：
$$\begin{aligned} \ell(\theta) & =\log L(\theta) \\ & =\log \prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \\ & =\sum_{i=1}^{n} \log \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}}{2 \sigma^{2}}\right) \\ & =n \log \frac{1}{\sqrt{2 \pi} \sigma}-\frac{1}{\sigma^{2}} \cdot \frac{1}{2} \sum_{i=1}^{n}\left(y^{(i)}-\theta^{T} x^{(i)}\right)^{2}\end{aligned}$$
去除掉无关的常数项，我们惊喜地发现，在如上的这些假设下，最大化似然函数实际上就等价于最小化我们自然定义得出的$J(\theta)$函数。
同时值得一提的是，也可能在别的一些假设下，也能很自然地导出最小二乘估计的结果。
同时也可以注意到，我们假设中对误差项定义的方差$\sigma$似乎对最终的结果并没有影响，但实际上$\sigma$未知或是已知对于我们对参数的估计实际上不会产生任何影响，我们稍后会看到。

### Locally weighted linear regression
讲义中借由使用高次函数作为hypothesis进行拟合的例子，讲解了过拟合和欠拟合，这告诉我们进行好的假设是很重要的（稍后会正式谈到这些概念）。
在之前的线性回归算法中，对于一个预测的询问$x$，我们是这样进行预测的：
1. 拟合参数$\theta$来最小化$J$函数
2. 计算$\theta^Tx$

而在本节介绍的Locally weighted linear regression算法中，对于一个询问x我们是这样预测的：
1. 拟合参数用以最小化$\sum w^{(i)}\left(y^{(i)} - \theta^T x^{(i)}\right)^2$
2. 输出$\theta^T x$

其中$w$是根据样本的重要性进行的权重赋值，一种可能得赋值方法：
$$w^{(i)} = \exp\left(-\frac{(x^{(i)}-x)^2}{2\tau^2}\right)$$
这实际上可以被理解为是一种离线的询问算法，我们在训练时就已经知道的要预测的询问，于是，我们对询问附近的训练数据进行着重关照。
并且其中参数$\tau$表示随着$x^{(i)}$远离$x$，相应权重的下降速度，它也被称作是**bandwidth parameter**。
同时这种算法与之前的线性回归算法有一个很大的不同，便是该算法在预测时还需要保存训练集的数据，而unweighted算法并需要，而是直接将它们都化作了参数。我们将这种算法归类为**parametric** learning algorithm而将正常的线性回归算法归类为**non-parametric** learning algorithm。形式化地讲，这两种算法分别需要$O(nd)$与$O(d)$的存储空间。

# Part II: Classification and logistic regression
在本节中我们将要讨论分类问题，与回归问题类似，但是我们目标输出的$y$值在分类问题中一般来说只有很小一部分的离散取值范围。
首先我们先来介绍**binary classification problem**二分类问题，在这种问题中目标输出只有两种可能取值$0$或$1$，同时，我们讲$1$称为**positive class**正类而将$0$称为**negative class**负类。
对于给定的输入数据$x^{(i)}$，我们称其对应的期望输出$y^{(i)}$为该训练样本的**label**标签。
## Logistic Regression
很显然的，虽然$y$的取值是离散的，但我们还是可以沿用解决回归问题的方法来解决分类问题，但是很显然，这样做是很不明智的，并且模型的预测效果也很容易变得很差。
直观上来看，对于$h_\theta(x)$来说，它的值域超出$[0,1]$的部分都应该是无意义的，为了不让这种无意义的取值范围发生，我们精心挑选了一个函数（喜），让我们看一看我们新的假设长什么样子吧：
$$h_\theta(x) = g(\theta^Tx) =  \frac{1}{1+\exp(-\theta^Tx)}$$
这个函数被称为**logistic function**或是**sigmoid function**，它的图像如下图所示：
![](/assets/CS-229-1/Pasted image 20241214204552.png)
可以看到，随着$x$由$-\infty$增长到$+\infty$，函数的值从$0$缓慢增长到$1$。另外，同样的，在$z=\theta^Tx$中，我们还是保留$x_0=0$的偏置项。
但是看到这里可能有个疑问，既然这个函数由于其$\mathbb{R}\rightarrow \left(0,1\right)$并且缓慢单增的特性得以被采用，那么其他具有同样性质的函数例如$\tanh(x)$等等函数能不能也像这样被使用呢。但实际上logistic函数是极其自然的，稍后我们会提到这一点。
惯例，我们先对函数进行求导：
$$\begin{aligned} g^{\prime}(z) & =\frac{d}{d z} \frac{1}{1+e^{-z}} \\ & =\frac{1}{\left(1+e^{-z}\right)^{2}}\left(e^{-z}\right) \\ & =\frac{1}{\left(1+e^{-z}\right)} \cdot\left(1-\frac{1}{\left(1+e^{-z}\right)}\right) \\ & =g(z)(1-g(z)) .\end{aligned}$$
可以看到导函数的形式也是相当简洁的。
好的，那么现在我们给定了假设函数，但实际上假设函数的值域也是一段连续的取值，我们如何用它来处理取值离散的分类问题，并且优化出合适的$\theta$呢？
其实我们可以通过一个简单的假设来避免这一问题，我们假设假设函数得出的值代表了一个分布，即：
$$\begin{aligned}
&P(y=1|x;\theta) = h_\theta(x)\\
&P(y=0|x,\theta) = 1 - h_\theta(x)
\end{aligned}$$
其实我们可以简化这个式子，将它们写进一个式子里：
$$p(y|x;\theta) = \left(h_\theta\left(x\right)\right)^y\left(1-h_\theta\left(x\right)\right)^{1-y}$$
于是类似于我们在线性回归里做的，我们也可以使用极大似然估计来挑选出合适的参数$\theta$。得到的似然函数是：
$$\begin{aligned}
L(\theta) & = \prod_{i=1}^n p(y^{(i)}|x^{(i)};\theta)\\
&=\prod_{i=1}^n\left(h_\theta\left(x\right)\right)^y\left(1-h_\theta\left(x\right)\right)^{1-y}
\end{aligned}$$
对数似然是：
$$\ell(\theta) = \sum_{i=1}^ny^{(i)}\log h\left(x^{(i)}\right) + \left(1-y^{(i)}\right)\left(1-\log h\left(x^{(i)}\right)\right)$$
于是接下来的目标仍然是类似的通过梯度下降来逐步逼近合适的参数$\theta$值，于是导一下先：
$$\begin{aligned} \frac{\partial}{\partial \theta} \ell(\theta) & =\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) \frac{\partial}{\partial \theta} g\left(\theta^{T} x\right) \\ & =\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) g\left(\theta^{T} x\right)\left(1-g\left(\theta^{T} x\right)\right) \frac{\partial}{\partial \theta} \theta^{T} x \\ & =\left(y\left(1-g\left(\theta^{T} x\right)\right)-(1-y) g\left(\theta^{T} x\right)\right) x \\ & =\left(y-h_{\theta}(x)\right) x\end{aligned}$$
于是我们的更新法则是：
$$\theta:=\theta + \alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)}$$
可以发现，这个更新的方法和线性回归中对于参数的更新方法几乎是一致的，只是假设函数由线性函数转换成了Logistic函数。这是巧合吗，还是由更深层次的原因？待会会谈到。

## Digression: The preceptron learning algorithm
在使用Logistic函数作为假设函数的分类问题解决中，我们通过将输出的函数值理解为被分为某一类的概率，巧妙地规避了连续函数映射到离散取值上的问题，那么有没有办法能不能直接让函数输出离散的分类结果呢？事实上很显然是存在的，我们定义**Threshold function**：
$$g(x) = 
\begin{cases}
    1 & \text{if } x\ge0 \\
    0 & \text{if } x<0
\end{cases}$$
如果我们像之前一样定义$h_\theta(x) = g(\theta^Tx)$，但函数$g$使用如上的定义，那么我们会得到如下的更新法则（实际没有变化）：
$$\theta:=\theta + \alpha\left(y^{(i)}-h_\theta(x^{(i)})\right)x^{(i)}$$
我们将这个称为**perceptron learning algorithm**。
这个算法的预测很难与有意义的概率建立联系，梯度下降也不想MLE一样自然，但我们稍后还是会提到这种算法。

## Another algorithm formaximizing $\ell(\theta)$
本节中我们介绍另一种迭代参数$\theta$来最大化对数似然的方法，在介绍这种方法之前，我们先引入牛顿法求函数零点的知识牛顿法也是一种迭代的算法，它的更新如下：
$$\theta:=\theta-\frac{f(\theta)}{f'(\theta)}$$
解释如图，所求的新$\theta$其实就是原$\theta$对应函数上的点作切线与$f(\theta)=0$的交点。
![](/assets/CS-229-1/Pasted image 20241214214013.png)
而我们在寻找最优的参数$\theta$时实际上也就是寻找使对数似然$\ell(\theta)$最大的$\theta$，也就是$\ell'(\theta)$的零点，于是可以使用牛顿法求解，得到的更新法则如下：
$$\theta:=\theta-\frac{\ell'(\theta)}{\ell''(\theta)}$$
最后在我们实际的Logistic回归假设中，$\theta$应该是一个向量，于是我们需要对更新法则进行一些改变：
$$\theta:=\theta-H^{-1}\nabla_\theta\ell(\theta)$$
其中$H$时**Hessian**矩阵，他是一个由函数二阶偏导数组成的矩阵：
$$H_{ij} = \cfrac{\partial^2\ell(\theta)}{\partial\theta_i\partial\theta_j}$$
实际上，这种牛顿法的更新法则得到了更快的收敛速度，相较于梯度下降只需要迭代更少的次数即可收敛到很接近全局最优解的附近。然而牛顿法的每次更新都需要进行$O(nd^2)$次计算，这相对于梯度下降的$O(nd)$次运算来说是相当昂贵的，但只要当$d$不过大，我们还是可以使用牛顿法来进行更快的收敛。同时，当牛顿法被用于最大化Logistic回归的对数似然函数的时候，我们也将这种方法称作**Fisher Scoring**。

# Part III: Generalized Linear Models
在前两节中，我们已经讨论了一种回归模型和一种分类模型。在分类模型中，我们假设的是$y\sim \mathcal{N}(\mu, \sigma^2)$，而在分类问题中，我们假设的是$y\sim \text{Bernoulli}(\phi)$。那么有没有一种方法，使得更广泛的一些概率分布也能构建成上述的模型呢，这就是本节所要叙述的内容：Generalized Linear Models(GLMs)。
## The exponential family
为了建立相关的模型，我们现需要介绍一个很广泛的概率分布族：Exponential family distributions指数族分布，我们在本节中都将围绕这一分布族进行讨论。
指数族分布指的是能被写成如下形式的分布：
$$p(y;\eta) = b(y)\exp\left(\eta^TT\left(y\right) - a\left(\eta\right)\right)$$
其中$\eta$被称为是**natural parameter(canonical parameter)**，$T(y)$被称为**sufficient statistic**，$a(\eta)$被称为是**log partition function**，实际上$\exp\left(-a\left(\eta\right)\right)$只是一个用于标准化分布的常数。
从而，每一组固定的$a,b,T(\cdot)$都定义了一个以$\eta$为参数的分布集合。
那么接下来我们将要说明Bernoulli分布以及Gaussian分布均为指数族分布，首先来看Bernoulli分布，我们知道Bernoulli分布的分布函数是:
$$p(y =1;\phi) = \phi; p(y=0;\phi) = 1-\phi$$
我们将其改写成指数族分布的形式：
$$\begin{aligned} p(y ; \phi) & =\phi^{y}(1-\phi)^{1-y} \\ & =\exp (y \log \phi+(1-y) \log (1-\phi)) \\ & =\exp \left(\left(\log \left(\frac{\phi}{1-\phi}\right)\right) y+\log (1-\phi)\right)\end{aligned}$$
于是我们就得到了Bernoulli是满足一下条件的指数族分布：
$$\begin{aligned}
& \eta = \log\frac{\phi}{1-\phi}\\
& \phi = \frac{1}{1+e^{-\eta}}\\
& T(y) = y\\
& a(\eta) = -\log(1-\phi) = \log(e^\eta+1)\\
& b(y) = 1
\end{aligned}$$
从中我们可以发现Sigmoid函数就是Bernoulli分布中概率到natural parameter映射的逆映射。
接下来我们考虑正态分布：
$$\begin{aligned} p(y ; \mu) & =\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2}(y-\mu)^{2}\right) \\ & =\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{1}{2} y^{2}\right) \cdot \exp \left(\mu y-\frac{1}{2} \mu^{2}\right)\end{aligned}$$
需要注意的是这里我们为了简便，假设了方差$\sigma^2=1$，而如果我们将$\sigma^2$保留成一个变量，那么我们也能得到类似的结果，只是$\eta$将会变成一个二维的向量，并且最后用于标准化的函数也将发生改变。
于是我们得到了正态函数对应指数分布族的参数：
$$\begin{aligned} \eta & =\mu \\ T(y) & =y \\ a(\eta) & =\mu^{2} / 2 \\ & =\eta^{2} / 2 \\ b(y) & =(1 / \sqrt{2 \pi}) \exp \left(-y^{2} / 2\right)\end{aligned}$$
事实上，绝大部分常见的概率分布都是属于指数族分布的，而接下来我们将介绍一种为这些分布建立对应模型的通用方法。

## Constructing GLMs
从广义上来讲，我们考虑回归或者分类问题，其中我们需要通过将其作为一个关于输入数据$x$的函数，预测随机变量（假设）$y$的值。同时，为了将其转换成一个GLM问题，我们需要假设$y$服从某种分布，具体做法如下：
1. 首先假设$y|x;\theta \sim \text{ExponentialFamily}(\eta)$，即给定输入$x$以及参数$\theta$，我们可以认为$y$服从以$\eta$为参数的某种指数族分布。
2. 在给定$x$的情况下，我们的任务是预测$T(y)$的值，但在大部分情形下$T(\cdot)$都是一个恒等映射，于是我们可以认为是要预测$y$的值。这也意味着，我们需要确保我们的假设函数$h(\cdot)$满足$h_\theta(x) = \mathbb{E}\left[y|x\right]$，例如在Logistic模型中，我们有
$$h_\theta(x) = 0\cdot p(y=0|x,\theta) + 1\cdot p(y=1|x,\theta) = \mathbb{E}\left[y|x\right]$$
3. 我们假设自然参数$\eta=\theta^Tx$

虽然第三个假设看上去并没有前两个这么自然，但这三个假设作用在一起我们就可以得到一个非常优雅的构建线性模型的方法，我们稍后会看到。

###  Ordinary Least Square
以线性回归模型为例运用如上方法构建模型。
像先前一样，我们假设在给定$x$的情况下有$y\sim\mathcal{N}(\mu,\sigma^2)$，另外根据假设我们有$\mu=\eta$，从而我们可以得到：
$$\begin{aligned}
h_\theta(x) &= \mathbb{E}\left[y|x;\theta\right]\\
&=\mu\\
&=\eta\\
&=\theta^Tx
\end{aligned}$$
下面还有对于各个等号的一些说明，但我觉得这是很显然的，于是略过这一部分。

### Logistic Regression
不多说，直接上公式：
$$\begin{aligned} h_{\theta}(x) & =E[y \mid x ; \theta] \\ & =\phi \\ & =1 /\left(1+e^{-\eta}\right) \\ & =1 /\left(1+e^{-\theta^{T} x}\right)\end{aligned}$$
这也是我们在Logistic回归中的假设函数的由来。
另外，我们也可以注意到$\mathbb{E}\left[T(y)|\eta\right]$其实可以表示为关于$\eta$的一个函数$g(\eta) = \mathbb{E}\left[T(y)|\eta\right]$，我们称这个函数是**canonical response function**典型响应函数，同时我们称这个函数的反函数$g^{-1}$为**canonical link function**典型链接函数。于是线性回归的响应函数是恒等映射，而Logistic回归的响应函数就是Sigmoid函数。

### Softmax Regression
这里我们继续考虑分类问题，但与Logistic回归不同，这里我们考虑得稍稍复杂一些，我们考虑多分类问题，也就是说$y$有$k$种可能的取值，即$y\in\lbrace1,2,...,k\rbrace$。在这种情况下，我们就要引入一个新的分布Multinomial distribution。
首先我们介绍一下这种分布，类似于Logistic回归，但由于要表示$k$个不同的类别，所以我们需要引入$k$个参数来表示被分类为各个类别的概率：$\phi_{1-k}$ ，同时我们也应该注意到，由于这些参数应当满足$\sum_{i=1}^k\phi_i=1$的条件，因此我们应当要减少一个参数使得各个参数之间保持独立，即使$\phi_k=1-\sum_{i=1}^{k-1}\phi_i$。
同时为了使这种分布满足指数族分布的形式，我们定义$T:\lbrace1,2,...,k\rbrace\rightarrow \mathbb{R}^{k-1}$函数如下：
$$T(1) = \begin{bmatrix}1\\0\\0\\...\\0\end{bmatrix}_{(k-1)\times 1},T(2) = \begin{bmatrix}0\\1\\0\\...\\0\end{bmatrix}_{(k-1)\times 1},...,T(k-1) = \begin{bmatrix}0\\0\\0\\...\\1\end{bmatrix}_{(k-1)\times 1}$$
可以注意到，在这种模型中$T(\cdot)$函数就不再是恒等映射，$y$是一个标量，但$T(y)$则是一个$(k-1)\times 1$的向量。
同时为了接下来表示的简便，我们引入指示器函数:$1\lbrace\cdot\rbrace$，若括号中的表达式为真，则函数值为1，否则为0。
接下来我们就可以展示，Multinomial distribution是一个指数族分布：
$$\begin{aligned} p(y ; \phi)= & \phi_{1}^{1\{y=1\}} \phi_{2}^{1\{y=2\}} \cdots \phi_{k}^{1\{y=k\}} \\ = & \phi_{1}^{1\{y=1\}} \phi_{2}^{1\{y=2\}} \cdots \phi_{k}^{1-\sum_{i=1}^{k-1} 1\{y=i\}} \\ = & \phi_{1}^{(T(y))_{1}} \phi_{2}^{(T(y))_{2}} \cdots \phi_{k}^{1-\sum_{i=1}^{k-1}(T(y))_{i}} \\ = & \exp \left((T(y))_{1} \log \left(\phi_{1}\right)+(T(y))_{2} \log \left(\phi_{2}\right)+\right.  \left.\cdots+\left(1-\sum_{i=1}^{k-1}(T(y))_{i}\right) \log \left(\phi_{k}\right)\right) \\ = & \exp \left((T(y))_{1} \log \left(\phi_{1} / \phi_{k}\right)+(T(y))_{2} \log \left(\phi_{2} / \phi_{k}\right)+\right.  \left.\cdots+(T(y))_{k-1} \log \left(\phi_{k-1} / \phi_{k}\right)+\log \left(\phi_{k}\right)\right) \\ = & b(y) \exp \left(\eta^{T} T(y)-a(\eta)\right)\end{aligned}$$
其中：
$$\begin{aligned}
\eta &= \begin{bmatrix}\log\cfrac{\phi_1}{\phi_k}\\\log\cfrac{\phi_2}{\phi_k}\\ \log \cfrac{\phi_3}{\phi_k}\\...\\\log\cfrac{\phi_{k-1}}{\phi_k}\end{bmatrix}\\
a(\eta) &= -\log(\phi_k)\\
b(y)&=1
\end{aligned}$$
我们可以发现，链接函数是：
$$\eta_i = \log\cfrac{\phi_i}{\phi_k}$$
接下来我们的任务就是找到响应函数作为假设函数：
$$\begin{aligned}
e^{\eta_i} &=\cfrac{\phi_i}{\phi_k}\\
\phi_k\sum_{i=1}^{k}e^{\eta_i} &= \sum_{i=1}^{k}\phi_i\\
\phi_k &= \cfrac{1}{\sum_{i=1}^{k}e^{\eta_i}}
\end{aligned}$$
从而我们将得到的$\phi_k$带回原式：
$$\phi_i=\cfrac{e^{\eta_i}}{\sum_{i=1}^{k}e^{\eta_i}}$$
这个将自然参数$\eta$映射到被解释为概率的参数$\phi$的函数被称为**softmax function**。
从而我们可以得到我们心心念念的假设函数：
$$\begin{aligned} h_{\theta}(x) & =\mathrm{E}[T(y) \mid x ; \theta] \\ & =\mathrm{E}\left[\left.\begin{array}{c}1\{y=1\} \\ 1\{y=2\} \\ \vdots \\ 1\{y=k-1\}\end{array} \right]\,\right. \\ & =\left[\begin{array}{c} \phi_{1} \\ \phi_{2} \\ \vdots \\ \phi_{k-1}\end{array}\right] \\ & =\left[\begin{array}{c}\frac{\exp \left(\theta_{1}^{T} x\right)}{\sum_{j=1}^{k} \exp \left(\theta_{x}^{T} x\right)} \\ \frac{\exp \left(\theta_{x}^{T}\right)}{\sum_{j=1}^{k} \exp \left(\theta_{j}^{T} x\right)} \\ \vdots \\ \frac{\exp \left(\theta_{k-1}^{T} x\right)}{\sum_{j=1}^{k} \exp \left(\theta_{j}^{T} x\right)}\end{array}\right]\end{aligned}$$
同时值得一提的是，虽然这个向量只是$k-1$维的，但是显然剩下那个参数是可以被轻易求出来的。
最后，按照惯例，有需要求似然函数了，如下：
$$\begin{aligned} \ell(\theta) & =\sum_{i=1}^{n} \log p\left(y^{(i)} \mid x^{(i)} ; \theta\right) \\ & =\sum_{i=1}^{n} \log \prod_{l=1}^{k}\left(\frac{e^{\theta_{l}^{T} x^{(i)}}}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x^{(i)}}}\right)^{1\left\{y^{(i)}=l\right\}}\end{aligned}$$
之后我们就可以使用梯度下降或者牛顿法来最大化似然函数求得合适的$\theta$了。