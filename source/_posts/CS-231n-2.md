---
title: CS 231n Note3 笔记
date: 2024-10-16 19:42
tags:
    - 机器学习
    - 支持向量机
    - 梯度下降算法
categories: 机器学习笔记
mathjax: true
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>


## Introduction
在上一节课中我们介绍了支持向量机(SVM)以及Softmax分类器对应的损失函数，显然的是，能够使损失函数得出更小损失的矩阵$W$是更好的。
于是，在本节课中，我们将介绍一些可以用于优化(Optimize)权重矩阵$W$的方法
## Visualizing the Loss Function
在介绍优化方法之前，我们先对损失函数的图像进行一些可视化分析，然而由于在分类器模型中，数据通常具有极高的维度(比如在CIFAR-10数据集中，这是一个30730(数据维度\*类别数量)维的函数)。这使得函数难以可视化。
于是我们可以使用1/2维切片的方法对这个函数在1/2维上的一小部分进行分析。
首先我们随机选取一个函数点$W$，并且随机选取一/两个方向$W_1,W_2$，于是我们可以在这个高维空间选取出一个二维的平面，只要通过遍历$W+aW_1+bW_2,\forall a,b$，在支持向量机损失函数中得到的某一个平面如图，红色表示较高的损失函数值：
![](/assets/CS-231n-2/1.png)
可以注意到这个函数可以被看作是类似于碗状的。
观察损失函数的表达式：
$$\begin{array}{l}L_{0}=\max \left(0, w_{1}^{T} x_{0}-w_{0}^{T} x_{0}+1\right)+\max \left(0, w_{2}^{T} x_{0}-w_{0}^{T} x_{0}+1\right) \\ L_{1}=\max \left(0, w_{0}^{T} x_{1}-w_{1}^{T} x_{1}+1\right)+\max \left(0, w_{2}^{T} x_{1}-w_{1}^{T} x_{1}+1\right) \\ L_{2}=\max \left(0, w_{0}^{T} x_{2}-w_{2}^{T} x_{2}+1\right)+\max \left(0, w_{1}^{T} x_{2}-w_{2}^{T} x_{2}+1\right)\end{array}$$
可以发现，损失函数是多个线性函数把负数部分截断后求和，如下图，是一个一维的可视化：
![](/assets/CS-231n-2/2.png)
我们可以推测，如上构建出来的函数实际上就是一个凸函数。因而，对于凸函数，有很多高效的方法进行优化（见凸优化）。但是对于复杂的人工神经网络，它的函数图像就不是简单的凸函数，而是复杂的多峰函数了。
此外，还有一个问题我们需要解决，在SVM损失函数中，由于`max`函数的存在，在函数图像中会存在许多的拐点，这会导致这些点的梯度是不存在的，于是我们对这些点求出次梯度来替代。
************
补充：次梯度
对于函数$f$在$x_0$点，次梯度实际上是定义为一个集合，是对$x_0$的去心邻域的任意$x$中满足$f(x)-f(x_0) \ge c^T(x-x_0)$的$c$构成的集合。写成矩阵形式即为$$\begin{bmatrix}c \\-1\end{bmatrix}^T(\begin{bmatrix}f(x)\\x\end{bmatrix} - \begin{bmatrix}f(x_0)\\x_0\end{bmatrix})\le 0$$
对于一元函数，我们称次梯度为次导数，次导数实际上是一个闭区间，并且有趣的是，左右端点分别等于函数在该点的左右倒数。
![](/assets/CS-231n-2/3.png)
次梯度有如下性质：
+ 次梯度映射是单调算子，即$(u-v)^T(y-x)\ge 0$

+ 内点处的次梯度始终存在，并且是有界的闭凸集

+ 若$f$在$x*$处取得最小值，那么$0$必然在该点的次梯度中

**************
## Optimization
### Strategy #1: A first very bad idea solution: Random search
随机搜索，即随机生成很多$W$并计算对应的损失函数，从中选取最优的$W$，事实上由于$W$的取值范围极其巨大，如此生成的$W$很难具有较好的效果讲义中运用该种算法在CIFAR-10数据集中最后只达到了略高于纯蒙的准确率($13.3\%$)
***********
**Core idea: iterative refinement(迭代优化)**
在整个优化的过程中，有一个想法贯穿始终，我们总是先随机生成一个$W$矩阵并在此基础上不断进行优化(即Local Search的思想)
### Strategy #2: Random Local Search
此算法即最朴素的Local Search算法，先随机生成一个$W$，在随机生成一个方向$W'$,每次将当前的$W$向$W'$的方向移动一定幅度，如果得到的损失函数值减少了，那么将$W$更新为$W+\alpha W'$。这种算法达到了$21.4\%$的准确率
### Strategy #3: Following the Gradient
我们知道，在一个函数中，它的梯度对应的方向即函数值上升最快的方向，与之相对的，反方向即函数值下降最快的方向，于是在优化$W$的过程中，我们也可以运用这一知识，每次迭代将$W$向梯度反方向移动，从而达到优化的效果

## Computing the gradient
在计算机计算梯度时，我们主要有两种方法：
+ 数值梯度计算

+ 解析梯度计算

### Computing the gradient numerically with finite differences(有限差分法)
在实际梯度的计算中，我们会运用到无穷小量，于是我们可以认为模拟一个小量$h=1e-5$来计算实际梯度的近似值。
代码见讲义
********
**Practical Consideration**
实际上在计算梯度的时候运用中心差分公式可以得到更精确的结果(Centered difference formula)
******
**Update in negative gradient direction**
如上所述，每次迭代朝梯度反方向移动，即可快速使损失函数值快速下降，收敛。
*********
**Effect of step size**
实际上，为了实现梯度下降算法，我们需要设置$W$在每次迭代中向梯度反方向移动的幅度，这也是一个超参数并需要合适选取，太大的步长会出现overstep现象，跳跃太大以至于无法收敛，太小的步长会导致收敛缓慢，降低效率。
*************
**A problem of effiiency**
由于在数值梯度计算过程中，我们需要在高维数据中逐维度计算差分，这在现代模型上千万的参数量下会导致极低的效率：$O(|X|)$。
### Computing the gradient analytically with Calculus
由于损失函数是简单的分段线性函数，于是我们可以解析地推导出函数的梯度表达式。然而相较于数值计算的方法，解析计算还是显得较为复杂且容易出错。于是我们可以进行*梯度检查(gradient check)*，即将解析计算出的结果与数值计算的结果比较。
观察损失函数的表达式：
$$L_{i}=\sum_{j \neq y_{i}}\left[\max \left(0, w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta\right)\right]$$
我们可以发现对$w_{y_i}$求导后只会剩下一项$x_i$，于是只要统计处所有能使$w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta$的$w_j$数量并乘上$-x_i$即为求导结果：
$${\nabla}_{w_{y_{i}}} L_{i}=-\left(\sum_{j \neq y_{i}} 1\left(w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta>0\right)\right) x_{i}$$
其中$1()$是一个指示器函数，当括号内的内容成立时值为1否则为0。
同时，当$j\neq y_i$时，我们也可以简单地求出导数：
$$\nabla_{w_{j}} L_{i}=1\left(w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta>0\right) x_{i}$$
于是我们可以得到整个的梯度方向
$$\nabla_W L_i = \begin{bmatrix}\nabla_{w_1}L_i\\\nabla_{w_2}L_i\\...\\\nabla_{w_K}L_i\end{bmatrix}$$
对所有需要训练的数据点都计算L并求和去平均值即可得到需要的下降方向。
## Gradient Descent
```python

# Vanilla Gradient Descent
while True:
	weights_grad = evaluate_gradient(loss_fun, data, weights)
	weights + = -step_size * weights_grad # perform parameter update
```
这个简单的循环就是所有人工神经网络的核心，尽管还有一些别的优化方法，比如LBFGS，但梯度下降算法还是神经网络最成熟的算法。
********
**Mini-batch gradient descent**
在实际训练过程中，我们不一定需要对所有训练数据计算梯度，我们可以从中每次小批量地选取一些数据计算，这样可以大大地提高效率，并且因为样本间是具有相关性的，该方法仍可较好地拟合。
*********
**Stochastic Gradient Descent**
即为批量大小为1的小批量计算，又被称为在线梯度下降(online gradient descent)，然而由于每次只选取1个样本，可能导致波动较大，并且由于在现代计算机中，向量化的计算更加迅速，所以*Mini-batch*更加常用，同时由于计算机的特性，通常选取2的幂来作为批量大小。