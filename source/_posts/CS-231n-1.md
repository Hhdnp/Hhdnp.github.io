---
title: CS 231n Note2 笔记
date: 2024-10-15 19:56:03
tags:
    - 机器学习
    - 线性回归
    - 支持向量机
    - 损失函数
    - CIFAR-10数据集
    - Softmax分类器
    - cross-entropy损失函数
categories: 机器学习笔记
mathjax: true
---
## Linear Classification
在上一节中，我们使用kNN的方式来分类图像，但是显而易见的，这种方式有两个缺点：
+ 无论是训练时还是推理是，都需要储存所有的训练图像，这将极大地小号训练的储存资源

+ 推理时需将待分类图片与测试集一一比较，这是很低效的操作，将大大增加推理时间

于是我们引入一种通用的方法，用于改善上面提到的这些缺点。
在这个方法中，有两个主要的组成部分：
+ 评分函数(score function): 用于计算测试图像在每个类别上的得分

+ 损失函数(loss function): 用于衡量预测分数与实际标签之间的差异

## Parameterized mapping from image to label scores
首先我们来明确一些符号，图像对应的向量$x_i\in \mathbb{R}^D$，其中每个测试图像都有一个对应的正确标签$y_i$，其中$i\in\lbrace 1,2,...,N\rbrace,y\in\lbrace 1,2...,K\rbrace$。于是我们可以定义损失函数$f:\mathbb{R}^D\rightarrow \mathbb{R}^K$。
线性分类器(Linear Classifier)，我们用一个自然的线性映射定义它的评分函数:
$$f(x_i,W,b) = Wx_i+b$$
其中$W$是一个$K\times D$的矩阵，$b$被称为偏置向量(bias vector)，用于防止对应的分类平面始终只能过原点
## Interpreting a linear classifier
我们可以这样通俗地解释线性分类器，分类器对每个标签在某个特定的位置的特定颜色具有偏好，比如说在船的图片中就会有很多蓝色的像素点，于是这些像素点对应的权重便会相应地变高，从而提高该类图片的评分函数。让我们用可视化来使这解释更清晰，虽然CIFAR-10中的数据点事3072维的，但我们还是在2维的图像上表示它以便观察：
![alt text](/assets/CS-231n-1/1.png)
其中箭头的方向便是评分函数增加的方向。
实际上，这种方法还是与kNN类似的，只是把kNN中所需要计算的距离替换成与训练出的向量的内积，并且用一个被泛化过的向量来代替训练集中的图片。
然而，这个泛化过的向量实际上只是对训练集中图片的融合，如下图所示，可以看到，每类对应的图片中保留并融合了许多训练集图片中的特征，同时又由于训练集可能带有一些偏向性，导致训练出的融合图像带有一部分并不广泛的特征，比如汽车是红色的。
![alt text](/assets/CS-231n-1/2.png)
**bias trick**:就是给训练集数据升维从而避免最后还要加上偏置向量。
**Image data preprocessing**:即将训练集中的数据全部映射到$[-1,1]$，这么做的作用后面会讲
## Loss function
损失函数是一个与矩阵$W$有关的函数，简单来说当训练出的的$W$与训练数据越接近(即预测越准确)，损失函数就越小。
## Multiclass Support Vector Machine Loss
在多分类支持向量机损失函数中，我们有两个超参数$\Delta, \lambda$，损失函数定义为：
$$L=\frac{1}{N} \sum_{i} \sum_{j \neq y_{i}}\left[\max \left(0, f\left(x_{i} ; W\right)_{j}-f\left(x_{i} ; W\right)_{y_{i}}+\Delta\right)\right]+\lambda \sum_{k} \sum_{l} W_{k, l}^{2}$$
多分类向量机的损失函数由两部分构成：
+ 数据损失：即前半部分，在支持向量机中，我们最希望能够让不正确标签的得分低于正确标签的得分至少$\Delta$，于是我们将所有不正确标签与其对应正确标签的得分分别作差，若满足要求则设为0，否则求和，并取平均值，便得到了数据损失。

+ 正则化损失：在支持向量机中，我们希望分类器的各个权重能够趋向于分散均匀。于是我们加入正则化损失来尽量使$W$中的值均匀，其中参数$\lambda$即该项损失的权重。
讲义中给出了计算数据损失的python代码，分为纯循环实现，半向量化实现和全向量化实现，其中最后一种效率最高，但被留做了习题，这里只记录最后一种：

```python
def L(x, y, W):
    delta = 1.0
    score = X.dot(W)
    correct_score = score[y, np.arange(score.shape[1])]
    margin = np.maximum(0, score - correct_score + delta)
    margin[y, np.arange(score.shape[1])] = 0
    loss = np.sum(margin) / X.shape[1]
    return loss
```

## Practical Consideration
本节主要讨论了超参数的选取，主要结论只有一个，就是$\Delta$与$\lambda$的倒数成反比。
## Softmax classifier
在cs188的学习中，我们已经知道了Logistic二分类，即通过Logistic函数是否大于0.5来判断分类。在这里，我们将该种分类方法提升到多分类即为Softmax分类器。
在SVM分类器中，我们通过学习拟合一个矩阵$W$，用于计算数据在每个类别中的得分通过比较正确分类与其余分类得分的差值计算损失函数
而在Softmax分类中，我们不采用这种直接比较得分的方式，我们通过计算出每一个分类的Logistic函数值，生成一个标准概率分布，从而得到每个分类的置信度，Softmax分类器中的损失函数被定义为；
$$L_i = -\log(\frac{e^{f_{y_i}}}{\sum_{j}e^{f_{j}}}) = -f_{y_{i}} + \log \sum_j e^{f_{j}}$$
我们将这个函数称为 **cross-entropy loss**
符号不在说明，应该很清晰易懂，即计算出每个标签对应向量与数据点的内积，求和并化为标准概率分布，对正确标签的概率取负倒数。
并且，对所有训练数据来说，总的损失函数就是对每个数据点求得的损失函数取平均值。
函数$f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}$被称为**softmax function**，它接收一个一个包含各标签得分的向量，并计算出一个标准概率分布
**信息论视角(Information Theory view)**: 我们定义一个正确概率分布$p$与一个预测出来的估计分布(q)之间的*cross-entropy*差异为：
$$H(p,q) = -\sum_x p(x)\log q(x)$$
注意到，因为在Softmax分类时，真实的概率分布只有当标签为正确标签是概率为1，其余均为0，于是我们只要把这个条件带进$H(p,q)$即可发现表达式就是上文提到的Softmax分类器对应的损失函数。
因此在训练Softmax分类器时，我们的目标实际上也就是缩小两个概率分布之间的差异。
**概率上的解释(Probabilistic interpretation)**: 正如上文所说，由Softmax函数生成的各标签的值就是一个标准概率分布，因此我们可以使用*Maximum Likelihood Estimation*或者*MAximum a posteriori estimation*来优化$W$的选取，但是由于这些内容超出了本课程的范围，因此并没有讲授。
**实际：精度问题(Practical issues: Numeric stability)**: 由于Softmax函数在计算时涉及到了指数运算，因此可以想象会有很大的数字出现，然而对大数做除法会出现*Numerically Unstable*的情况(应该就是精度问题)。因此我们可以对函数进行一些小的调整来避免这种问题，如下：
$$\frac{e^{f_{y_i}}}{\sum_{j}e^{f_{j}}} = \frac{Ce^{f_{y_i}}}{C\sum_{j}e^{f_{j}}} = \frac{e^{f_{y_i}+\log C}}{\sum_{j}e^{f_{j} + \log C}}$$
因此我们可以在所有指数上同时加上一个常数$\log C$而不改变整体的值。于是在实际操作过程中，我们加上指数中最大的值即使$\log C = -max_j f_j$从而使所有指数都是一个非正数。
## SVM v.s. Softmax
![alt text](/assets/CS-231n-1/3.png)
上图很好地展现了两者之间的关系，两者都需要用到矩阵$W$计算各标签的得分，但使用标签的方式不同
**Softmax classifier Provides probabilities for each class**: 在softmax函数的计算过程中，每个类别都被赋予了一个置信度，这个置信度是由得分函数进行指数函数得到的，由于指数函数并不线性，因此我们再次引入一个超参数$\lambda$在计算概率之前，我们将得分向量数乘这个超参数，$\lambda$的改变会极大地改变概率分布，具体来说，越大的$\lambda$会导致越均匀的分布。
**In practice, SVM and Softmax are usually comparable**: SVM通常来说相较于softmax是“乐观的”，比如说在SVM看来[100,99,99]与[100,-100,-100]在第一个为正确类别且$\Delta=1$时是一样的，都可以使损失函数达到0，但softmax显然是不行的，更进一步讲，“悲观的”softmax分类器几乎不可能使损失函数降到0。