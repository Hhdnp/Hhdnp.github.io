---
title: CS 231n Lec7.5 之DLbook RNN部分
date: 2024-11-16 18:35
tags:
    - RNN
    - RNN变种
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

## Unfolding Computational Graph
回顾有限状态机中的一个重要函数，激励函数：
$$s^{(t)} = f(s^{(t-1)},\theta)$$
我们总能将未来时刻的状态$s^{(t)}$通过参数$\theta$以及过去时刻的状态$s^{(t-1)}$表示出来。
RNN就是借鉴了这一点来大幅缩减了需要输入或者输出序列信息的机器学习模型，它的隐藏层也是由状态构成，激励函数为：
$$s^{(t)} = f(s^{(t-1)},x^{(t)},\theta)$$
可以注意到的是：
1. $s^{(t)}$包含了所有过去输入序列$\textbf{x}$的信息
2. $s^{(t)}$同时也是对过去输入序列有损耗的概括
3. 形式化地讲，RNN是一个任意长序列$x^{(t)}......x^{(1)}$到隐藏状态$h^{(t)}$的映射。

![](/assets/CS-231n-9.5/1.png)
于是我们可以将激励函数从初始状态开始展开，得到：
$$h^{(t)} = g^{(t)}(\vec{x})$$
然而我们还是习惯使用激励函数的迭代表示，因为这样函数的输入大小固定，且函数不会随着时间而变化。

## Recurrent Nerual Networks
利用以上知识，根据参数的不同，函数连接方式的不同，我们可以将RNN划分为以下三类：
![](/assets/CS-231n-9.5/2.png)
只有隐藏状态之间存在前向反馈连接
![](/assets/CS-231n-9.5/3.png)
前一时刻的输入与下一时刻的隐藏状态连接形成前向反馈
![](/assets/CS-231n-9.5/4.png)
只在序列的最后进行输出并且计算损失。

接下来我们进行RNN中前向传播方程的推导。
以第一种结构为例，除了指定神经网络结构外，我们还需为隐藏状态的传播指定激活函数，接下来以激活函数$\tanh$与损失函数$\mathrm{softmax}$为例：
$$\begin{aligned}
&\textbf{a}^{(t)} = \textbf{b} + \textbf{W}\textbf{h}^{(t-1)} + \textbf{U}\textbf{x}^{(t)}\\
&\textbf{h}^{(t)} = \tanh(\textbf{a}^{(t)})\\
&\textbf{o}^{(t)} = \textbf{c} + \textbf{V}\textbf{h}^{(t)}\\
&\hat{\textbf{y}^{(t)}} = \mathrm{softmax}(\textbf{o}^{(t)})
\end{aligned}$$
对于损失函数，我们也可以自然地得到，他是所有时间步中的损失之和：
$$L = -\sum_{t=1}^{\tau}\log \mathrm{p}_{model}(y^{(t)}| x^{(1)}.....x^{(\tau)})$$
其中的概率即为我们预测出的softmax概率。
然而这样子我们必须按照时间步一步步计算，因为前驱的计算会影响后续的结果。
进行一次前向传播时空复杂度均为$\Theta(\tau)$。

### Teacher Forcing and Networks with Output Recurrence
本节的内容关于第二种模型结构展开，这种RNN结构并没有像第一种结构那么强的表示能力，因为它只能从上一时间步中的输出结果捕捉前面序列的信息，这是较为困难的，但是这种结构有一种可以加速训练的方法，我们称之为导师驱动过程(Teacher forcing)。

通过观察结构图我们可以发现，某一时刻的输出$\textbf{o}$来源于该时刻的隐藏状态$\textbf{h}$而$\textbf{h}$仅与输入与上一时刻输出直接相连。
在导师驱动过程中，我们使用一个很强的假设，即上一时刻的输出值必定是理想值，于是我们训练所需要达成的最大似然估计的目标为：
$$\begin{aligned}&\log \mathrm{p}(y^{(t-1)},y^{(t)}|x^{(1)},...,x^{(t)}) \\=& \log\mathrm{p}(y^{(t-1)}|x^{(1)},...,x^{(t)}) + \log\mathrm{p}(y^{(t-1)},y^{(t)}|x^{(1)},...,x^{(t)},y^{(t-1)})\end{aligned}$$
于是我们需要最大化在给定输入序列和理想上一时间步输入的条件下，输出当前时间步理想输出的概率。
这么做的好处就是我们不必在计算某一时间步时等待前面的计算结果，可以采用高效的并行计算完成前向传播和反向传播过程。这避免了计算代价高昂的BPTT，然而当一个状态与之前时间步的某个时间状态直接相连的时候，BPTT仍然是不可避免的。

同时，这种训练方式由很大的缺陷，因为我们假设了上一步的输出必然是完美的，当实际运行时，前一时间步的反馈是来自自身，出现了不同于理想输出的反馈，那么模型的表现就会变得很糟糕。
这样的缺陷可以通过一些训练方式进行改善，比如使用自由训练，在训练过程中不再一味使用理想输出反馈，而是随机使用理想或者自身输出进行反馈。

### Computing The Gradient
我们从损失函数说起，前面说到，总的损失函数是每一时间步损失函数之和，于是我们有：
$$\frac{\partial L}{\partial L^{(t)}} = 1$$
首先计算最后一个时间步中隐藏状态的梯度值

$$\begin{aligned}
&\nabla_{\textbf{h}^{(\tau)}}L = \textbf{V}^T\nabla_{\textbf{o}^{(\tau)}}L
\end{aligned}$$
接下来以某一时间步为例计算隐藏状态的梯度值：
$$\begin{aligned}
&\nabla_{\textbf{o}^{(t)}}L = \hat{\textbf{y}^{(t)}} - (\textbf{y}^{(t)} == 1)\\
&\nabla_{\textbf{h}^{(t)}}L = \textbf{V}^T\nabla_{\textbf{o}^{(t)}}L + \textbf{W}^T\mathrm{diag}(1-(\textbf{h}{(t+1)})^2)\nabla_{\textbf{h}^{(t+1)}}L\\
&
\end{aligned}$$
于是我们可以得到剩余参数的梯度值：
$$\begin{aligned}
\nabla_{\boldsymbol{c}} L & =\sum_{t}\left(\frac{\partial \boldsymbol{o}^{(t)}}{\partial \boldsymbol{c}}\right)^{\top} \nabla_{\boldsymbol{o}^{(t)}} L=\sum_{t} \nabla_{\boldsymbol{o}^{(t)}} L\\
\nabla_{\boldsymbol{b}} L & =\sum_{t}\left(\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{b}^{(t)}}\right)^{\top} \nabla_{\boldsymbol{h}^{(t)}} L=\sum_{t} \operatorname{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right) \nabla_{\boldsymbol{h}^{(t)}} L \\
\nabla_{\boldsymbol{V}} L & =\sum_{t} \sum_{i}\left(\frac{\partial L}{\partial o_{i}^{(t)}}\right) \nabla_{\boldsymbol{V}^{(t)}} o_{i}^{(t)}=\sum_{t}\left(\nabla_{\boldsymbol{o}^{(t)}} L\right) \boldsymbol{h}^{(t)^{\top}}\\
\nabla_{\boldsymbol{W}} L & =\sum_{t} \sum_{i}\left(\frac{\partial L}{\partial h_{i}^{(t)}}\right) \nabla_{\boldsymbol{W}^{(t)}} h_{i}^{(t)} \\
& =\sum_{t} \operatorname{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right)\left(\nabla_{\boldsymbol{h}^{(t)}} L\right) \boldsymbol{h}^{(t-1)^{\top}} \\
\nabla_{\boldsymbol{U}} L & =\sum_{t} \sum_{i}\left(\frac{\partial L}{\partial h_{i}^{(t)}}\right) \nabla_{\boldsymbol{U}^{(t)}} h_{i}^{(t)} \\
& =\sum_{t} \operatorname{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right)\left(\nabla_{\boldsymbol{h}^{(t)}} L\right) \boldsymbol{x}^{(t)^{\top}}
\end{aligned}$$
### Directed Graph Model
回忆我们训练的目标是最大化似然函数
$$\log \Pr(y^{(t)}|x^{(t)},...,x^{(1)})$$
或者当输出与隐藏状态之间存在连接的时候我们需要最大化
$$\log \Pr(y^{(t)}|x^{(t)},...,x^{(1)},y^{(t-1)}, ...,y^{(1)})$$
总体上讲我们就是需要最大化输出整个完整序列的概率，此时我们考虑一个简单的情况，即整个RNN展开序列没有额外输入：
$$\Pr(\textbf{Y}) = \Pr(y^{(1)},...,y^{(\tau)}) = \prod_{t=1}^\tau \Pr(y^{(t)}|y^{(t-1)},...,y^{(1)})$$
*注意*，这种模型是与Markov模型不尽相同的，因为Markov链中有一个很强的假设：$y^{(t)}$与$y^{(1)},...y^{(t-2)}$在$y^{(t-1)}$给定的情况下均条件独立，这在RNN中是不可能成立的，因为一个输出必然要根据之前整个序列进行推断。
同时，这也表明，当我们无法只用$y^{(t-1)}$来获取需要用于推断$y^{(t)}$的全部信息例如$y^{(i)}$时，RNN就体现出了它独特的作用。

然而，回忆我们在CS188中学到的，若是采用表格表示法来记录$y^{(t)}$与之前序列的关系，我们就需要$O(k^{\tau})$空间，然而RNN使用了有限状态机的模型巧妙地避免了如此巨大的空间消耗，具体来说，在RNN中我们可以这样来获取任意$y^{(i)}$的信息：
$$y^{(i)}\rightarrow h^{(i+1)}\rightarrow...\rightarrow h^{(t)}\rightarrow y^{(t)}$$
这也可以被视作是一种参数共享，然而这个参数共享是有一个前提的，就是给定$t$时刻的模型各参数，$t+1$时刻的模型各参数是平稳的(stationary)，形式化地讲，就是$t+1$时刻的模型各参数是与$t$本身独立的。
我们可以通过一个小trick来验证平稳性是否成立：
在每一时刻添加一个额外的input，即时刻$t$，并在观察到参数与时刻独立的情况下，我们可以尽可能多地进行参数共享。

接着我们需要完善我们的RNN模型，具体来说，类似于算法证明，我们要让我们的模型能够停下来(terminate)。
书中提到了三种方法：
1. 训练时添加一个在序列末端的特殊符号(Finish token)
2. 额外引入一个二元的输出(Bernoulli output)决定序列是否要继续生成
3. 用RNN直接预测序列长度$\tau$，但注意此时需要将每一时刻输出的$\tau$重新作为下一时刻的输入，确保模型知道是否靠近末尾。

方法三基于以下等式：
$$P\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(\tau)}\right)=P(\tau) P\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(\tau)} \mid \tau\right)$$
### Modeling Sequences Conditioned on Context with RNNs
之前我们讨论的都是RNN在没有向量$\textbf{x}$作为输入情况下的行为，本节讨论输入序列$\textbf{x}$定长的情况。
当引入输入$\textbf{x}$后，我们需要决定如何将它们融合进RNN模型中，常见的有三种做法：
1. 在每一时间步做一额外输入，如下图，我们通过引入一个新的矩阵$\textbf{R}$来将输入$\textbf{x}$转化成一个对隐藏状态的新偏置。
![](/assets/CS-231n-9.5/5.png)

2. 将输入$\textbf{x}$作为RNN的初始状态$\textbf{h}^{(0)}$。
3. 结合以上两种方法

然而在实际应用过程中，除了如上图所示的只有一个输入的情况，我们还可能会遇到输入一个序列$\textbf{x}^{(0)}, \textbf{x}^{(1)},...$的情况，这种情况就如一开始所介绍的第一种模型。
它的目标是最大化
$$\Pr(y^{(1)}, ..., y^{(\tau)}|x^{(1)},...,x^{(\tau)})$$
然而在一开始介绍的模型中，做了一个很强的假设，即$y$之间的独立性假设，于是上式可以写成：
$$\prod_{i=1}^{\tau}Pr(y^{(i)}|x^{(1)},...,x^{(\tau)})$$
然而这样会降低模型的表达能力（？），于是我们可以通过添加从上一时刻输出指向下一时刻隐藏单元的边来移除这一假设，如下图所示：
![](/assets/CS-231n-9.5/6.png)

**以下内容仅做介绍，我没看懂**
## Bidirectional RNN
可以注意到，之前讨论的RNN都是基于时间顺序从过去到将来进行前向传播的，于是显然的有
$$\forall i >0,y^{(t)}\perp\!\!\!\perp y^{(t+i)}$$
这就导致在序列中靠后的输入输出无法为靠前的输出提供信息，但是在实际应用过程中，这种关系是需要被使用的（语音识别，手写识别等），于是我们就可以采用双向RNN的形式：
![](/assets/CS-231n-9.5/7.png)
但是这种RNN形式的传播过程肉眼可见的复杂，书中也没有提及，于是略过。
书中还提到，这种双向的传播方式还可以十分自然地扩展到二维的形式（真的吗）。

## Encoder-Decoder Sequence-to-Sequence Architecture
我们继续对RNN的适用范围进行扩展，之前我们都讨论的是输入输出序列等长的情形，在这一节中会介绍一种允许输入输出序列不等长的模型：
我们将输入称作上下文(Context)$\textbf{c}$，我们使$\textbf{c}$成为一个概括输入$\textbf{X} = (x^{(1)},...x^{(\tau)})$的向量，于是我们就可以得到一种朴素而又实用的想法：将输入序列RNN中隐藏状态的最终值作为上下文向量$\textbf{c}$，再将其作为输出序列RNN的初始隐藏状态进行输出，于是我们将整个输入输出的过程分成了两部分：概括输入上下文以及通过概括结果生成输出，得到了类似于编码器，解码器的结构：
![](/assets/CS-231n-9.5/8.png)
这个想法一开始听上去十分地简洁且巧妙，然而实际使用过程中会存在一些问题，会在下一节中提到。

## Deep Recurrent Networks
从整体架构来讲，RNN可以概括为以下三步：
1. 通过矩阵$\textbf{U}$将input传播到hidden
2. 通过矩阵$\textbf{W}$将Hidden传播到Hidden的后继
3. 通过矩阵$\textbf{V}$将Hidden传播到Output

在之前介绍的种种模型中，这三步都是浅层的变换（仅由一层线性变换和一层非线性激活函数构成），于是就会自然地产生一个想法，能不能加深这三步变换，即在其中引入一些新的层，如下图所示
![](/assets/CS-231n-9.5/9.png)

## Recursive Nerual Networks
在之前介绍的RNN中，我们采用链式结构按时间顺序从前向后传递信息，那么我们能不能采用其它的结构来达成相同的效果呢（这里应该是针对多输入一输出的模型）。仍然是很自然的，我们可以想到借助树的结构来达成，如下图所示：
![](/assets/CS-231n-9.5/10.png)
这种方式的显著优势就是将链状结构的$O(\tau)$传播深度迅速降低到了$O(\log\tau)$深度，然而这么做会引发一个问题，我们该如何确定树的结构，以下是两种常见的做法：
1. 采用不依赖情景就具有较好结构的树，比如各种平衡树
2. 针对场景设计专门的树结构，比如在处理自然语言时就可以采用语法分析树

## The Challenge of Long-Term Dependencies
在深度RNN的构建时，一个最要命的问题就是当传播序列过长时，会出现梯度消失（常见）以及梯度爆炸（不常见）的问题，这种现象的介绍即为：
在忽略传播非线性的情况下，我们可以得到隐藏状态间的转移：
$$h^{(t)} =\textbf{W}^th^{(0)} = \textbf{Q}^T\Lambda^t\textbf{Q}h^{(0)}$$
于是$\Lambda$中小于1的特征值就会迅速趋于0，大于1的特征值就会飞速增长发生爆炸。
这种现象可以通过在不同时刻使用调整过方差的不同权重矩阵来缓解。
然而，在RNN中梯度消失的问题仍然是不可避免的，因为需要表示参数的长期以来关系，梯度的幅值必然会变得很小，下面几节开始讨论如何解决这些问题

**下面完全看不懂了，看上去书上也是仅做介绍，于是略过**