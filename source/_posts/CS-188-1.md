---
title: CS188 Note 17笔记
date: 2024/10/9 21:03
tags: 
    - 机器学习
    - 简易贝叶斯分类
    - 最大似然估计
    - 线性回归
    - 拉普拉斯近似 
categories: 机器学习笔记
mathjax: true
---
## 机器学习

分类
+ supervised learning
	训练集中有input以及预期的output输出，训练的目的是使模型能够预测不在训练集中的数据。
+ unsupervised learning
	训练集中没有output数据，目的是识别数据的特征并进行分类等工作
在本课程中我们讨论的话题仅限于supervised learning

几种数据集
+ training data
	训练集，需要使用这些数据进行拟合
+ validation data
	用于在训练的过程中对训练的暂时结果进行评估
+ test set
	用于在训练结束后对训练结果进行评估
## Naive Bayes
Naive Bayes是一种机器学习分类算法，它主要思想是通过训练数据拟合概率数据$P(Y|x_1,x_2,...,x_n)$，其中$Y$是输出，$x_i$是从输入中提取出的特征。
然而若是存储这所有的概率表，则需要$\Theta(2^n)$的空间资源，这显然是不可接受的，于是我们采用$Bayes' Net$来存储概率，在此，该算法做了一些假设
+ $x_i,x_j,i\neq j$间相互独立。
这是个非常强的假设，然而这可以极大的简化$Bayes' Net$的结构，如下图：

![image file label](/assets/CS-188-1/CS-188-1-1.png)
这样，概率表的存储需求就减少为$\Theta(n)$，同时，概率的计算也变得简便，我们预测时需计算：
$$\begin{aligned}
prediction(\overrightarrow{f}) &= \underset{y}{\operatorname{argmax}}P(Y = y|\overrightarrow{F} = \overrightarrow{f})\\
&=\underset{y}{\operatorname{argmax}}P(Y = y,\overrightarrow{F} = \overrightarrow{f})\\
&=\underset{y}{\operatorname{argmax}}P(Y=y)\prod_{i = 1}^{N}P(F_i = f_i|Y=y)
\end{aligned}$$
那么现在问题又来了，我们该如何计算这个概率表呢？
## Parameter Estimation

我们假设，对每个参数，概率函数可以表示为$P_\theta(x_i)$。于是我们可以将所有概率表示为一个和$\Theta$相关的函数。接下来我们将使用最大似然估计来确定对每个参数$\theta$的值。
在此之前，我们仍然需要做一些假设：
+ 每次采样，采样遵从的分布都是不变的
+ 特征间相互独立
+ 在获得数据之前，$\theta$可以被看作是一个等概率分布的值。
在计算之前，我们需要先假定似然函数，在此我们假设每个参数的概率函数$P_\theta(x_i) = \theta$，于是我们可以得到似然函数：
$$\mathscr{L}(\theta)=\prod_{i=1}^{N} P_{\theta}\left(x_{i}\right)$$
为了使最终得到的似然函数满足最大似然估计，根据微积分知识，我们需要使似然函数对$\theta$的偏导数为0，即：
$$\frac{\partial}{\partial \theta}\mathscr{L}(\theta) = 0$$
## Maximum Likelihood in Naive Bayes
对每个特征进行一次最大似然估计即可，讲义上有例子，这里只给出结论：
$$\theta_i = \text{训练集中所有数据中}f_i\text{的均值}$$
## Smoothing
最大似然估计看上去非常合理，但是事实上存在一些漏洞，比如说，若一个特征$f_i$在训练集中每个都出现，则会导致$P(F_i=f_i|Y=y)=0$从而导致计算出的$Y=y$概率均为0，从而始终不会被分类到$y$中，这被称为$overfitting$现象。
于是在进行最大似然估计得到的概率：$P_{MLE}=\frac{count(x)}{N}$后，我们对概率进行$Laplace~ estimate(LAP)$处理：
$$P_{LAP,k}=\frac{count(x)+k}{N+k|X|}$$
其中$|X|$是该特征的取值数量，这就相当于使估计的概率以$k$为强度向均值靠近。

## Linear Regression
见讲义，其中有笔记的补充矩阵求导知识点。
知识点有
+ 线性回归的概念
+ 损失函数，如何最小化损失函数
+ 二分类的一种方法（向量），边界
+ 如何得到权重向量，边界
+ 过原点分界面的改进（升维）
+ 多分类问题