---
title: CS 231n Lec14 笔记
date: 2024-12-08 20:26
tags:
    - 强化学习
    - Reinforcement Learning
    - Value Iteration
    - Deep Q-Learning
    - Policy Gradients
categories: 机器学习笔记
mathjax: true
---

<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']],

			displayMath: [['$$', '$$']]

            }
        });
    </script>
</head>

今日主题：强化学习
## Reinforcement Learning
简单来说，在强化学习中，我们需要让Agent在Environment中进行一系列行动(Take Action)并且从中获得奖励(Rewards)。而我们学习的目标是最大化获得的奖励。
详细来说，在每一个时刻$t$:
环境给Agent提供当前的状态信息$s_t$，但这状态信息可能是不完全/有噪声的。
Agent根据$s_t$决定要采取的行动$a_t$。
最后环境根据当前状态以及Agent采取的行动给出奖励$r_t$，并转换到下一状态$s_{t+1}$
以下是几个例子：
![](/assets/Pasted image 20241208165050.png)
![](/assets/Pasted image 20241208165111.png)
![](/assets/Pasted image 20241208165129.png)
![](/assets/Pasted image 20241208165151.png)

## Reinforcement Learning vs. Supervised Learning
从以下两张概括叙述两种学习方式的图来看，这两者应该是高度相似的：
![](/assets/Pasted image 20241208165317.png)
![](/assets/Pasted image 20241208165331.png)
但实际上这两者是完全不同的：
1. RL中奖励与环境的变化可能是完全随机的
2. RL中的$r_t$可能不仅与$a_t$相关，还与其它因素相关
3. RL中的环境是未知的，不能反向传播（即无法直接计算$\frac{\partial r_t}{\partial a_t}$）
4. $s_{t+1}$是与$a_t$高度相关的

## Markov Decision Process(MDP)
我们可以形式化地描述一个强化学习问题，用一个五元组表示：
$$\begin{aligned}
&(S,A,R,P,\gamma)\\
&S:\text{可能的状态集合}\\
&A:\text{可能行动的集合}\\
&R:\text{表示每一个}(state,action)\text{对应的奖励的概率分布}\\
&P:\text{表示每一个}(state,action)\text{对应的下一状态的概率分布}\\
&\gamma:\text{奖励的衰减率}
\end{aligned}$$
**Recall:** MDP的性质，我们应该知道，在状态转移的过程中，在给定上一个状态的前提下，当前状态的分布与其余任意状态独立。
然后我们让Agent以一个给定的策略(实际上是一个可用行动的概率分布)进行模拟。
于是我们的目标也就变为了找到一个策略$\pi^*$使得$\sum_t\gamma^tr_t$最大。
Agent进行行动的逻辑也可由此进行形式化的描述：
- Initialize：在$t=0$时刻，环境采样初始状态$s_0\sim p(s_0)$
- 之后不断循环如下过程：
- Agent选择行动$a_t\sim \pi(a|s_t)$
- 环境给出奖励$r_t\sim R(r|s_t,a_t)$，采样下一状态$s_{t+1}\sim P(s|s_t, a_t)$
- Agent接收到$r_t,s_{t+1}$

如下是一个简易的Example: Grid World
![](/assets/Pasted image 20241208170814.png)
而实际上这个例子中的$\pi^*$是易于寻找的，我们只需寻找最短路即可。

## Finding Optimal Policies
我们的目标是最大化$\sum_t\gamma^t r_t$但问题在于在RL问题中随机部分太过多且庞杂，难以精确计算。
于是我们将目标转换为最大化期望收益：
$$\pi^* = \mathrm{argmax}_\pi\mathbb{E}(\sum_t\gamma^tr_t|\pi)$$
### Value Function and Q Function
根据策略$\pi$进行一次完整模拟会生成一次样本轨迹(sample trajection)：
$$\tau = (s_0,a_0,r_0,...,s_{T-1},a_{T-1},r_{T-1},s_{T})$$
我们为了制定策略，需要对一个状态的好坏进行量化。于是我们定义一个这样的函数Value Function表示在$s$为初状态，以$\pi$为策略进行行动的期望收益：
$$V^\pi(s) = \mathbb{E}\left[\sum_t\gamma^tr_t|s_0=s,\pi\right]$$
从而自然地，我们也可以定义一个对$(state,action)$对的优劣进行衡量的函数Q Function：
$$Q^\pi(s,a) = \mathbb{E}\left[\sum_t\gamma^tr_t|s_0=s,a_0=a,\pi\right]$$

### Bellman Equation
观察Q Function我们可以发现，其实我们只需要知道最优状态下的Q Function，我们就能自然而然地制定最优的策略$\pi^*$，这是因为我们只需对按最优策略继续行动的所有可能行动对应的最优Q Function值进行比较，就可以通过寻找最大值得到当前状态下的最优行动，我们也将这最优的Q Function值叫做Optimal Q Function:
$$Q^*(s,a) = \max_\pi\mathbb{E}\left[\sum_t\gamma^tr_t|s_0=s,a_0=a,\pi\right]$$
于是我们就可以用最优的Q函数来描述最优的策略$\pi^*$:
$$\pi^*(s) = \mathrm{argmax}_aQ^*(s,a)$$
而Bellman Equation给出了如下的$Q^*$应当满足的条件：
$$\begin{aligned}
&Q^*(s,a) = \mathbb{E}_{r,s'}(r + \gamma\max_{a'}Q^*(s',a'))\\
&r\sim R(s,a),s'\sim P(s,a)
\end{aligned}$$
即$Q^*(s,a)$可以被视作是当前$s,a$期望收益与最优下一状态的$Q^*$函数之和。
事实上，若是一个函数满足Bellman Equation，那么我们可以认为它就是$Q^*$函数。

## Solving for the Optimal Policy
### Value Iteration
这种方式的核心在于让任意的$Q$函数收敛到满足Bellman Equation的形式。
我们首先从一个随机的Q函数出发，并直接使用Bellman Equation进行更新：
$$Q_{i+1}(s,a)\leftarrow \mathbb{E}_{r,s'}\left[r + \gamma\max_{a'}Q^*(s',a')\right]$$
我们如果假设Q可以收敛，那么显然就会有当$i\rightarrow +\infty$时，$Q_i\rightarrow Q^*$
但为了确定$Q$函数何时收敛，我们需要记录所有的函数值比较变化，这实际上是难以实现的。
解决方案也很简单，就是使用神经网络来估计函数值并且使用Bellman Equation来作为损失函数。
### Deep Q-Learning
我们使用$\theta$来代表神经网络的参数，我们的目标是训练$\theta$使其满足：
$$Q_{\theta}(s,a) = Q^*(s,a)$$
我们需要使用Bellman Equation来监督学习过程，即调整输入输出间的对应关系，期望输出应该是：
$$y_{s, a, \theta}=\mathbb{E}_{r, s^{\prime}}\left[r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta\right)\right]$$
使用均方误差来定义损失函数：
$$y_{s, a, \theta}=\mathbb{E}_{r, s^{\prime}}\left[r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta\right)\right]L(s, a)=\left(Q(s, a ; \theta)-y_{s, a, \theta}\right)^{2}$$
但这种方法有一个严重的问题：它不是stationary的。
即我们的目标应该是减少实际输出与期望输出之间的关系，在我们之前的学习中，期望输出一直都是一个恒定的值，但在这里，期望输出会随着$\theta$的更新而改变，这会导致训练不稳定甚至训练无法收敛。
并且在这种方式中如何选取训练数据的batch也是一个大问题。

### Case Study, Playing Atari Games
![](/assets/Pasted image 20241208174019.png)
![](/assets/Pasted image 20241208174038.png)

## Policy Gradients
在Q Learning中，我们训练网络来为所有$\left(state, action\right)$估计$Q^*(s,a)$。但在一些情况下，$Q^*$函数的学习可能是异常困难的。而这些时候我们可以转换策略的学习方式，转而直接去学习$\text{state}\rightarrow\text{action}$的映射是更为遍历的方式，这种学习方式也就是Policy Gradients。

在这种方法中，我们的目标是训练一个网络，接受$state$作为输入，输出一个关于当前状态应该采取什么$action$的分布$\pi_\theta(a|s)$。
目标函数被定义为在采取$\pi_\theta$作为策略时一次轨迹的期望收益：
$$J(\theta) = \mathbb{E}_{\pi_\theta}\left[\sum_{t}\gamma^tr_t\right]$$
我们的目标是最大化该函数的值，于是我们使用梯度上升得到：
$$\theta^* = \mathrm{argmax}_\theta J(\theta)$$
但问题在于环境是未知的，这就导致该函数不可微分，难以计算$\frac{\partial J}{\partial \theta}$。
但实际上微分的形式是可以计算出来的：
$$\begin{aligned}
\nabla_\theta J & = \nabla\mathbb{E}_{\pi_\theta}\left[\sum_{t}R(s_t,a_t)\right]\\
& = \nabla\mathbb{E}_{\pi_\theta}\left[R(\tau)\right]\\
& = \nabla_\theta\sum_{\tau}\Pr(\tau|\theta)R(\tau)\\
& = \sum_\tau \nabla_\theta \Pr(\tau|\theta)R(\tau)\\
& = \sum_\tau\Pr(\tau|\theta)\nabla_\theta\log\Pr(\tau|\theta)R(\tau)\\
& = \mathbb{E}_{\pi_\theta}\left[\nabla_\theta \log \Pr(\tau| \theta) R(\tau)\right]
\end{aligned}$$
而可以通过简单的计算知道：
$$\begin{aligned}
\nabla_\theta\log\Pr(\tau|\theta) = C + \sum_t\nabla_\theta\log\pi_\theta(a_t|s_t)
\end{aligned}$$
于是我们需要优化的梯度就变成了：
$$\cfrac{\partial J}{\partial \theta} = \mathbb{E}_{\pi_\theta}\left[\sum_t\log\pi_\theta(a_t|s_t)R_t\right]$$
并且我们使用REINFORCE Rule进行梯度更新。
即以$\pi_\theta$为策略，采样多次轨迹，并以这些轨迹中的数据作为训练数据对$\theta$进行更新。
然而由于我们使用了采样的方式来获得数据，这会导致数据的方差过高，我们可以在更新中加入基准(baseline)来改善这一状况：
$$\cfrac{\partial J}{\partial \theta} = \mathbb{E}_{\pi_\theta}\left[\sum_t\log\pi_\theta(a_t|s_t)(R_t - b(s_t))\right]$$
其中$b(s_t)$是基准函数。

## Other Approaches
以下方法仅做介绍
### Model-Based RL
以上我们介绍的都是Model-Free RL，即这些算法不依赖于对环境的建模，直接与环境进行交互学习策略，但这些方法存在一些问题：
![](/assets/Pasted image 20241208201855.png)
于是我们引入了基于模型的RL方法。
这种方法的重点是对环境进行建模和估计，实际上关键在于对MDP中介绍RL问题五大要素中的$P$也就是状态转移函数进行建模以及估计。
而在完成对环境的建模之后，我们只需要使用规划(Planning)的方法即可求解最优策略。
![](/assets/Pasted image 20241208202131.png)

### Actor-Critic
这种方法融合了Q-Learning和Policy Gradient。我们训练一个Actor来预测下一步的行动并且使用一个Critic来预测采取行动后的

### Imitation Learning
使用收集到的Expert的行为数据并且训练模型模仿这些行为数据

### Inverse Reinforcement Learning & Adversarial Learning
![](/assets/Pasted image 20241208202541.png)

## Active Research Problem
+ What tasks do we work on? 
+ How to get training data (sim)? 
+ How to get large-scale diverse data (real)? 
+ How to achieve successful sim2real transfer? 
+ How to interact with humans?